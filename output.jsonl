{"text": "\nSpeeding Up a Build\n*******************\nBuild time can be an issue. By default, the build system uses simple\ncontrols to try and maximize build efficiency. In general, the default\nsettings for all the following variables result in the most efficient\nbuild times when dealing with single socket systems (i.e. a single CPU).\nIf you have multiple CPUs, you might try increasing the default values\nto gain more speed. See the descriptions in the glossary for each\nvariable for more information:\n-  :term:`BB_NUMBER_THREADS`:\nThe maximum number of threads BitBake simultaneously executes.\n-  :term:`BB_NUMBER_PARSE_THREADS`:\nThe number of threads BitBake uses during parsing.\n-  :term:`PARALLEL_MAKE`: Extra\noptions passed to the ``make`` command during the\n:ref:`ref-tasks-compile` task in\norder to specify parallel compilation on the local build host.\n-  :term:`PARALLEL_MAKEINST`:\nExtra options passed to the ``make`` command during the\n:ref:`ref-tasks-install` task in\norder to specify parallel installation on the local build host.\nAs mentioned, these variables all scale to the number of processor cores\navailable on the build system. For single socket systems, this\nauto-scaling ensures that the build system fundamentally takes advantage\nof potential parallel operations during the build based on the build\nmachine's capabilities.\nFollowing are additional factors that can affect build speed:\n-  File system type: The file system type that the build is being\nperformed on can also influence performance. Using ``ext4`` is\nrecommended as compared to ``ext2`` and ``ext3`` due to ``ext4``\nimproved features such as extents.\n-  Disabling the updating of access time using ``noatime``: The\n``noatime`` mount option prevents the build system from updating file\nand directory access times.\n-  Setting a longer commit: Using the \"commit=\" mount option increases\nthe interval in seconds between disk cache writes. Changing this\ninterval from the five second default to something longer increases\nthe risk of data loss but decreases the need to write to the disk,\nthus increasing the build performance.\n-  Choosing the packaging backend: Of the available packaging backends,\nIPK is the fastest. Additionally, selecting a singular packaging\nbackend also helps.\n-  Using ``tmpfs`` for :term:`TMPDIR`\nas a temporary file system: While this can help speed up the build,\nthe benefits are limited due to the compiler using ``-pipe``. The\nbuild system goes to some lengths to avoid ``sync()`` calls into the\nfile system on the principle that if there was a significant failure,\nthe :term:`Build Directory` contents could easily be rebuilt.\n-  Inheriting the :ref:`ref-classes-rm-work` class:\nInheriting this class has shown to speed up builds due to\nsignificantly lower amounts of data stored in the data cache as well\nas on disk. Inheriting this class also makes cleanup of\n:term:`TMPDIR` faster, at the\nexpense of being easily able to dive into the source code. File\nsystem maintainers have recommended that the fastest way to clean up\nlarge numbers of files is to reformat partitions rather than delete\nfiles due to the linear nature of partitions. This, of course,\nassumes you structure the disk partitions and file systems in a way\nthat this is practical.\nAside from the previous list, you should keep some trade offs in mind\nthat can help you speed up the build:\n-  Remove items from\n:term:`DISTRO_FEATURES`\nthat you might not need.\n-  Exclude debug symbols and other debug information: If you do not need\nthese symbols and other debug information, disabling the ``*-dbg``\npackage generation can speed up the build. You can disable this\ngeneration by setting the\n:term:`INHIBIT_PACKAGE_DEBUG_SPLIT`\nvariable to \"1\".\n-  Disable static library generation for recipes derived from\n``autoconf`` or ``libtool``: Following is an example showing how to\ndisable static libraries and still provide an override to handle\nexceptions::\nSTATICLIBCONF = \"--disable-static\"\nSTATICLIBCONF:sqlite3-native = \"\"\nEXTRA_OECONF += \"${STATICLIBCONF}\"\n.. note::\n-  Some recipes need static libraries in order to work correctly\n(e.g. ``pseudo-native`` needs ``sqlite3-native``). Overrides,\nas in the previous example, account for these kinds of\nexceptions.\n-  Some packages have packaging code that assumes the presence of\nthe static libraries. If so, you might need to exclude them as\nwell."}
{"text": "\nUpgrading Recipes\n*****************\nOver time, upstream developers publish new versions for software built\nby layer recipes. It is recommended to keep recipes up-to-date with\nupstream version releases.\nWhile there are several methods to upgrade a recipe, you might\nconsider checking on the upgrade status of a recipe first. You can do so\nusing the ``devtool check-upgrade-status`` command. See the\n\":ref:`devtool-checking-on-the-upgrade-status-of-a-recipe`\"\nsection in the Yocto Project Reference Manual for more information.\nThe remainder of this section describes three ways you can upgrade a\nrecipe. You can use the Automated Upgrade Helper (AUH) to set up\nautomatic version upgrades. Alternatively, you can use\n``devtool upgrade`` to set up semi-automatic version upgrades. Finally,\nyou can manually upgrade a recipe by editing the recipe itself.\nUsing the Auto Upgrade Helper (AUH)\n===================================\nThe AUH utility works in conjunction with the OpenEmbedded build system\nin order to automatically generate upgrades for recipes based on new\nversions being published upstream. Use AUH when you want to create a\nservice that performs the upgrades automatically and optionally sends\nyou an email with the results.\nAUH allows you to update several recipes with a single use. You can also\noptionally perform build and integration tests using images with the\nresults saved to your hard drive and emails of results optionally sent\nto recipe maintainers. Finally, AUH creates Git commits with appropriate\ncommit messages in the layer's tree for the changes made to recipes.\n.. note::\nIn some conditions, you should not use AUH to upgrade recipes\nand should instead use either ``devtool upgrade`` or upgrade your\nrecipes manually:\n-  When AUH cannot complete the upgrade sequence. This situation\nusually results because custom patches carried by the recipe\ncannot be automatically rebased to the new version. In this case,\n``devtool upgrade`` allows you to manually resolve conflicts.\n-  When for any reason you want fuller control over the upgrade\nprocess. For example, when you want special arrangements for\ntesting.\nThe following steps describe how to set up the AUH utility:\n#. *Be Sure the Development Host is Set Up:* You need to be sure that\nyour development host is set up to use the Yocto Project. For\ninformation on how to set up your host, see the\n\":ref:`dev-manual/start:Preparing the Build Host`\" section.\n#. *Make Sure Git is Configured:* The AUH utility requires Git to be\nconfigured because AUH uses Git to save upgrades. Thus, you must have\nGit user and email configured. The following command shows your\nconfigurations::\n$ git config --list\nIf you do not have the user and\nemail configured, you can use the following commands to do so::\n$ git config --global user.name some_name\n$ git config --global user.email username@domain.com\n#. *Clone the AUH Repository:* To use AUH, you must clone the repository\nonto your development host. The following command uses Git to create\na local copy of the repository on your system::\n$ git clone git://git.yoctoproject.org/auto-upgrade-helper\nCloning into 'auto-upgrade-helper'... remote: Counting objects: 768, done.\nremote: Compressing objects: 100% (300/300), done.\nremote: Total 768 (delta 499), reused 703 (delta 434)\nReceiving objects: 100% (768/768), 191.47 KiB | 98.00 KiB/s, done.\nResolving deltas: 100% (499/499), done.\nChecking connectivity... done.\nAUH is not part of the :term:`OpenEmbedded-Core (OE-Core)` or\n:term:`Poky` repositories.\n#. *Create a Dedicated Build Directory:* Run the :ref:`structure-core-script`\nscript to create a fresh :term:`Build Directory` that you use exclusively\nfor running the AUH utility::\n$ cd poky\n$ source oe-init-build-env your_AUH_build_directory\nRe-using an existing :term:`Build Directory` and its configurations is not\nrecommended as existing settings could cause AUH to fail or behave\nundesirably.\n#. *Make Configurations in Your Local Configuration File:* Several\nsettings are needed in the ``local.conf`` file in the build\ndirectory you just created for AUH. Make these following\nconfigurations:\n-  If you want to enable :ref:`Build\nHistory <dev-manual/build-quality:maintaining build output quality>`,\nwhich is optional, you need the following lines in the\n``conf/local.conf`` file::\nINHERIT =+ \"buildhistory\"\nBUILDHISTORY_COMMIT = \"1\"\nWith this configuration and a successful\nupgrade, a build history \"diff\" file appears in the\n``upgrade-helper/work/recipe/buildhistory-diff.txt`` file found in\nyour :term:`Build Directory`.\n-  If you want to enable testing through the :ref:`ref-classes-testimage`\nclass, which is optional, you need to have the following set in\nyour ``conf/local.conf`` file::\nIMAGE_CLASSES += \"testimage\"\n.. note::\nIf your distro does not enable by default ptest, which Poky\ndoes, you need the following in your ``local.conf`` file::\nDISTRO_FEATURES:append = \" ptest\"\n#. *Optionally Start a vncserver:* If you are running in a server\nwithout an X11 session, you need to start a vncserver::\n$ vncserver :1\n$ export DISPLAY=:1\n#. *Create and Edit an AUH Configuration File:* You need to have the"}
{"text": "\n``upgrade-helper/upgrade-helper.conf`` configuration file in your\n:term:`Build Directory`. You can find a sample configuration file in the\n:yocto_git:`AUH source repository </auto-upgrade-helper/tree/>`.\nRead through the sample file and make configurations as needed. For\nexample, if you enabled build history in your ``local.conf`` as\ndescribed earlier, you must enable it in ``upgrade-helper.conf``.\nAlso, if you are using the default ``maintainers.inc`` file supplied\nwith Poky and located in ``meta-yocto`` and you do not set a\n\"maintainers_whitelist\" or \"global_maintainer_override\" in the\n``upgrade-helper.conf`` configuration, and you specify \"-e all\" on\nthe AUH command-line, the utility automatically sends out emails to\nall the default maintainers. Please avoid this.\nThis next set of examples describes how to use the AUH:\n-  *Upgrading a Specific Recipe:* To upgrade a specific recipe, use the\nfollowing form::\n$ upgrade-helper.py recipe_name\nFor example, this command upgrades the ``xmodmap`` recipe::\n$ upgrade-helper.py xmodmap\n-  *Upgrading a Specific Recipe to a Particular Version:* To upgrade a\nspecific recipe to a particular version, use the following form::\n$ upgrade-helper.py recipe_name -t version\nFor example, this command upgrades the ``xmodmap`` recipe to version 1.2.3::\n$ upgrade-helper.py xmodmap -t 1.2.3\n-  *Upgrading all Recipes to the Latest Versions and Suppressing Email\nNotifications:* To upgrade all recipes to their most recent versions\nand suppress the email notifications, use the following command::\n$ upgrade-helper.py all\n-  *Upgrading all Recipes to the Latest Versions and Send Email\nNotifications:* To upgrade all recipes to their most recent versions\nand send email messages to maintainers for each attempted recipe as\nwell as a status email, use the following command::\n$ upgrade-helper.py -e all\nOnce you have run the AUH utility, you can find the results in the AUH\n:term:`Build Directory`::\n${BUILDDIR}/upgrade-helper/timestamp\nThe AUH utility\nalso creates recipe update commits from successful upgrade attempts in\nthe layer tree.\nYou can easily set up to run the AUH utility on a regular basis by using\na cron job. See the\n:yocto_git:`weeklyjob.sh </auto-upgrade-helper/tree/weeklyjob.sh>`\nfile distributed with the utility for an example.\nUsing ``devtool upgrade``\n=========================\nAs mentioned earlier, an alternative method for upgrading recipes to\nnewer versions is to use\n:doc:`devtool upgrade </ref-manual/devtool-reference>`.\nYou can read about ``devtool upgrade`` in general in the\n\":ref:`sdk-manual/extensible:use \\`\\`devtool upgrade\\`\\` to create a version of the recipe that supports a newer version of the software`\"\nsection in the Yocto Project Application Development and the Extensible\nSoftware Development Kit (eSDK) Manual.\nTo see all the command-line options available with ``devtool upgrade``,\nuse the following help command::\n$ devtool upgrade -h\nIf you want to find out what version a recipe is currently at upstream\nwithout any attempt to upgrade your local version of the recipe, you can\nuse the following command::\n$ devtool latest-version recipe_name\nAs mentioned in the previous section describing AUH, ``devtool upgrade``\nworks in a less-automated manner than AUH. Specifically,\n``devtool upgrade`` only works on a single recipe that you name on the\ncommand line, cannot perform build and integration testing using images,\nand does not automatically generate commits for changes in the source\ntree. Despite all these \"limitations\", ``devtool upgrade`` updates the\nrecipe file to the new upstream version and attempts to rebase custom\npatches contained by the recipe as needed.\n.. note::\nAUH uses much of ``devtool upgrade`` behind the scenes making AUH somewhat\nof a \"wrapper\" application for ``devtool upgrade``.\nA typical scenario involves having used Git to clone an upstream\nrepository that you use during build operations. Because you have built the\nrecipe in the past, the layer is likely added to your\nconfiguration already. If for some reason, the layer is not added, you\ncould add it easily using the\n\":ref:`bitbake-layers <bsp-guide/bsp:creating a new bsp layer using the \\`\\`bitbake-layers\\`\\` script>`\"\nscript. For example, suppose you use the ``nano.bb`` recipe from the\n``meta-oe`` layer in the ``meta-openembedded`` repository. For this\nexample, assume that the layer has been cloned into following area::\n/home/scottrif/meta-openembedded\nThe following command from your :term:`Build Directory` adds the layer to\nyour build configuration (i.e. ``${BUILDDIR}/conf/bblayers.conf``)::\n$ bitbake-layers add-layer /home/scottrif/meta-openembedded/meta-oe\nNOTE: Starting bitbake server...\nParsing recipes: 100% |##########################################| Time: 0:00:55\nParsing of 1431 .bb files complete (0 cached, 1431 parsed). 2040 targets, 56 skipped, 0 masked, 0 errors.\nRemoving 12 recipes from the x86_64 sysroot: 100% |##############| Time: 0:00:00\nRemoving 1 recipes from the x86_64_i586 sysroot: 100% |##########| Time: 0:00:00\nRemoving 5 recipes from the i586 sysroot: 100% |#################| Time: 0:00:00\nRemoving 5 recipes from the qemux86 sysroot: 100% |##############| Time: 0:00:00\nFor this example, assume that the ``nano.bb`` recipe that\nis upstream has a 2.9.3 version number. However, the version in the\nlocal repository is 2.7.4. The following command from your build\ndirectory automatically upgrades the recipe for you::\n$ devtool upgrade nano -V 2.9.3\nNOTE: Starting bitbake server...\nNOTE: Creating workspace layer in /home/scottrif/poky/build/workspace\nParsing recipes: 100% |##########################################| Time: 0:00:46\nParsing of 1431 .bb files complete (0 cached, 1431 parsed). 2040 targets, 56 skipped, 0 masked, 0 errors.\nNOTE: Extracting current version source...\nNOTE: Resolving any missing task queue dependencies"}
{"text": "\n.\n.\n.\nNOTE: Executing SetScene Tasks\nNOTE: Executing RunQueue Tasks\nNOTE: Tasks Summary: Attempted 74 tasks of which 72 didn't need to be rerun and all succeeded.\nAdding changed files: 100% |#####################################| Time: 0:00:00\nNOTE: Upgraded source extracted to /home/scottrif/poky/build/workspace/sources/nano\nNOTE: New recipe is /home/scottrif/poky/build/workspace/recipes/nano/nano_2.9.3.bb\n.. note::\nUsing the ``-V`` option is not necessary. Omitting the version number causes\n``devtool upgrade`` to upgrade the recipe to the most recent version.\nContinuing with this example, you can use ``devtool build`` to build the\nnewly upgraded recipe::\n$ devtool build nano\nNOTE: Starting bitbake server...\nLoading cache: 100% |################################################################################################| Time: 0:00:01\nLoaded 2040 entries from dependency cache.\nParsing recipes: 100% |##############################################################################################| Time: 0:00:00\nParsing of 1432 .bb files complete (1431 cached, 1 parsed). 2041 targets, 56 skipped, 0 masked, 0 errors.\nNOTE: Resolving any missing task queue dependencies\n.\n.\n.\nNOTE: Executing SetScene Tasks\nNOTE: Executing RunQueue Tasks\nNOTE: nano: compiling from external source tree /home/scottrif/poky/build/workspace/sources/nano\nNOTE: Tasks Summary: Attempted 520 tasks of which 304 didn't need to be rerun and all succeeded.\nWithin the ``devtool upgrade`` workflow, you can\ndeploy and test your rebuilt software. For this example,\nhowever, running ``devtool finish`` cleans up the workspace once the\nsource in your workspace is clean. This usually means using Git to stage\nand submit commits for the changes generated by the upgrade process.\nOnce the tree is clean, you can clean things up in this example with the\nfollowing command from the ``${BUILDDIR}/workspace/sources/nano``\ndirectory::\n$ devtool finish nano meta-oe\nNOTE: Starting bitbake server...\nLoading cache: 100% |################################################################################################| Time: 0:00:00\nLoaded 2040 entries from dependency cache.\nParsing recipes: 100% |##############################################################################################| Time: 0:00:01\nParsing of 1432 .bb files complete (1431 cached, 1 parsed). 2041 targets, 56 skipped, 0 masked, 0 errors.\nNOTE: Adding new patch 0001-nano.bb-Stuff-I-changed-when-upgrading-nano.bb.patch\nNOTE: Updating recipe nano_2.9.3.bb\nNOTE: Removing file /home/scottrif/meta-openembedded/meta-oe/recipes-support/nano/nano_2.7.4.bb\nNOTE: Moving recipe file to /home/scottrif/meta-openembedded/meta-oe/recipes-support/nano\nNOTE: Leaving source tree /home/scottrif/poky/build/workspace/sources/nano as-is; if you no longer need it then please delete it manually\nUsing the ``devtool finish`` command cleans up the workspace and creates a patch\nfile based on your commits. The tool puts all patch files back into the\nsource directory in a sub-directory named ``nano`` in this case.\nManually Upgrading a Recipe\n===========================\nIf for some reason you choose not to upgrade recipes using\n:ref:`dev-manual/upgrading-recipes:Using the Auto Upgrade Helper (AUH)` or\nby :ref:`dev-manual/upgrading-recipes:Using \\`\\`devtool upgrade\\`\\``,\nyou can manually edit the recipe files to upgrade the versions.\n.. note::\nManually updating multiple recipes scales poorly and involves many\nsteps. The recommendation to upgrade recipe versions is through AUH\nor ``devtool upgrade``, both of which automate some steps and provide\nguidance for others needed for the manual process.\nTo manually upgrade recipe versions, follow these general steps:\n#. *Change the Version:* Rename the recipe such that the version (i.e.\nthe :term:`PV` part of the recipe name)\nchanges appropriately. If the version is not part of the recipe name,\nchange the value as it is set for :term:`PV` within the recipe itself.\n#. *Update* :term:`SRCREV` *if Needed*: If the source code your recipe builds\nis fetched from Git or some other version control system, update\n:term:`SRCREV` to point to the\ncommit hash that matches the new version.\n#. *Build the Software:* Try to build the recipe using BitBake. Typical\nbuild failures include the following:\n-  License statements were updated for the new version. For this\ncase, you need to review any changes to the license and update the\nvalues of :term:`LICENSE` and\n:term:`LIC_FILES_CHKSUM`\nas needed.\n.. note::\nLicense changes are often inconsequential. For example, the\nlicense text's copyright year might have changed.\n-  Custom patches carried by the older version of the recipe might\nfail to apply to the new version. For these cases, you need to\nreview the failures. Patches might not be necessary for the new\nversion of the software if the upgraded version has fixed those\nissues. If a patch is necessary and failing, you need to rebase it\ninto the new version.\n#. *Optionally Attempt to Build for Several Architectures:* Once you\nsuccessfully build the new software for a given architecture, you\ncould test the build for other architectures by changing the\n:term:`MACHINE` variable and\nrebuilding the software. This optional step is especially important\nif the recipe is to be released publicly.\n#. *Check the Upstream Change Log or Release Notes:* Checking both these\nreveals if there are new features that could break\nbackwards-compatibility. If so, you need to take steps to mitigate or\neliminate that situation.\n#. *Optionally Create a Bootable Image and Test:* If you want, you can\ntest the new software by booting it onto actual hardware.\n#. *Create a Commit with the Change in the Layer Repository:* After all\nbuilds work and any testing is successful, you can create commits for"}
{"text": "Upgrading Recipes\nany changes in the layer holding your upgraded recipe."}
{"text": "\n======================================\nYocto Project Development Tasks Manual\n======================================\n.. toctree::\n:caption: Table of Contents\n:numbered:\nintro\nstart\nlayers\ncustomizing-images\nnew-recipe\nnew-machine\nupgrading-recipes\ntemporary-source-code\nquilt.rst\ndevelopment-shell\npython-development-shell\nbuilding\nspeeding-up-build\nlibraries\nprebuilt-libraries\nx32-psabi\ngobject-introspection\nexternal-toolchain\nwic\nbmaptool\nsecuring-images\ncustom-distribution\ncustom-template-configuration-directory\ndisk-space\npackages\nefficiently-fetching-sources\ninit-manager\ndevice-manager\nexternal-scm\nread-only-rootfs\nbuild-quality\nruntime-testing\ndebugging\nlicenses\nsecurity-subjects\nvulnerabilities\nsbom\nerror-reporting-tool\nwayland\nqemu\n.. include:: /boilerplate.rst"}
{"text": "\nUsing the Error Reporting Tool\n******************************\nThe error reporting tool allows you to submit errors encountered during\nbuilds to a central database. Outside of the build environment, you can\nuse a web interface to browse errors, view statistics, and query for\nerrors. The tool works using a client-server system where the client\nportion is integrated with the installed Yocto Project\n:term:`Source Directory` (e.g. ``poky``).\nThe server receives the information collected and saves it in a\ndatabase.\nThere is a live instance of the error reporting server at\nhttps://errors.yoctoproject.org.\nWhen you want to get help with build failures, you can submit all of the\ninformation on the failure easily and then point to the URL in your bug\nreport or send an email to the mailing list.\n.. note::\nIf you send error reports to this server, the reports become publicly\nvisible.\nEnabling and Using the Tool\n===========================\nBy default, the error reporting tool is disabled. You can enable it by\ninheriting the :ref:`ref-classes-report-error` class by adding the\nfollowing statement to the end of your ``local.conf`` file in your\n:term:`Build Directory`::\nINHERIT += \"report-error\"\nBy default, the error reporting feature stores information in\n``${``\\ :term:`LOG_DIR`\\ ``}/error-report``.\nHowever, you can specify a directory to use by adding the following to\nyour ``local.conf`` file::\nERR_REPORT_DIR = \"path\"\nEnabling error\nreporting causes the build process to collect the errors and store them\nin a file as previously described. When the build system encounters an\nerror, it includes a command as part of the console output. You can run\nthe command to send the error file to the server. For example, the\nfollowing command sends the errors to an upstream server::\n$ send-error-report /home/brandusa/project/poky/build/tmp/log/error-report/error_report_201403141617.txt\nIn the previous example, the errors are sent to a public database\navailable at https://errors.yoctoproject.org, which is used by the\nentire community. If you specify a particular server, you can send the\nerrors to a different database. Use the following command for more\ninformation on available options::\n$ send-error-report --help\nWhen sending the error file, you are prompted to review the data being\nsent as well as to provide a name and optional email address. Once you\nsatisfy these prompts, the command returns a link from the server that\ncorresponds to your entry in the database. For example, here is a\ntypical link: https://errors.yoctoproject.org/Errors/Details/9522/\nFollowing the link takes you to a web interface where you can browse,\nquery the errors, and view statistics.\nDisabling the Tool\n==================\nTo disable the error reporting feature, simply remove or comment out the\nfollowing statement from the end of your ``local.conf`` file in your\n:term:`Build Directory`::\nINHERIT += \"report-error\"\nSetting Up Your Own Error Reporting Server\n==========================================\nIf you want to set up your own error reporting server, you can obtain\nthe code from the Git repository at :yocto_git:`/error-report-web/`.\nInstructions on how to set it up are in the README document."}
{"text": "\nFinding Temporary Source Code\n*****************************\nYou might find it helpful during development to modify the temporary\nsource code used by recipes to build packages. For example, suppose you\nare developing a patch and you need to experiment a bit to figure out\nyour solution. After you have initially built the package, you can\niteratively tweak the source code, which is located in the\n:term:`Build Directory`, and then you can force a re-compile and quickly\ntest your altered code. Once you settle on a solution, you can then preserve\nyour changes in the form of patches.\nDuring a build, the unpacked temporary source code used by recipes to\nbuild packages is available in the :term:`Build Directory` as defined by the\n:term:`S` variable. Below is the default value for the :term:`S` variable as\ndefined in the ``meta/conf/bitbake.conf`` configuration file in the\n:term:`Source Directory`::\nS = \"${WORKDIR}/${BP}\"\nYou should be aware that many recipes override the\n:term:`S` variable. For example, recipes that fetch their source from Git\nusually set :term:`S` to ``${WORKDIR}/git``.\n.. note::\nThe :term:`BP` represents the base recipe name, which consists of the name\nand version::\nBP = \"${BPN}-${PV}\"\nThe path to the work directory for the recipe\n(:term:`WORKDIR`) is defined as\nfollows::\n${TMPDIR}/work/${MULTIMACH_TARGET_SYS}/${PN}/${EXTENDPE}${PV}-${PR}\nThe actual directory depends on several things:\n-  :term:`TMPDIR`: The top-level build\noutput directory.\n-  :term:`MULTIMACH_TARGET_SYS`:\nThe target system identifier.\n-  :term:`PN`: The recipe name.\n-  :term:`EXTENDPE`: The epoch --- if\n:term:`PE` is not specified, which is\nusually the case for most recipes, then :term:`EXTENDPE` is blank.\n-  :term:`PV`: The recipe version.\n-  :term:`PR`: The recipe revision.\nAs an example, assume a Source Directory top-level folder named\n``poky``, a default :term:`Build Directory` at ``poky/build``, and a\n``qemux86-poky-linux`` machine target system. Furthermore, suppose your\nrecipe is named ``foo_1.3.0.bb``. In this case, the work directory the\nbuild system uses to build the package would be as follows::\npoky/build/tmp/work/qemux86-poky-linux/foo/1.3.0-r0"}
{"text": "\nUsing a Development Shell\n*************************\nWhen debugging certain commands or even when just editing packages,\n``devshell`` can be a useful tool. When you invoke ``devshell``, all\ntasks up to and including\n:ref:`ref-tasks-patch` are run for the\nspecified target. Then, a new terminal is opened and you are placed in\n``${``\\ :term:`S`\\ ``}``, the source\ndirectory. In the new terminal, all the OpenEmbedded build-related\nenvironment variables are still defined so you can use commands such as\n``configure`` and ``make``. The commands execute just as if the\nOpenEmbedded build system were executing them. Consequently, working\nthis way can be helpful when debugging a build or preparing software to\nbe used with the OpenEmbedded build system.\nFollowing is an example that uses ``devshell`` on a target named\n``matchbox-desktop``::\n$ bitbake matchbox-desktop -c devshell\nThis command spawns a terminal with a shell prompt within the\nOpenEmbedded build environment. The\n:term:`OE_TERMINAL` variable\ncontrols what type of shell is opened.\nFor spawned terminals, the following occurs:\n-  The ``PATH`` variable includes the cross-toolchain.\n-  The ``pkgconfig`` variables find the correct ``.pc`` files.\n-  The ``configure`` command finds the Yocto Project site files as well\nas any other necessary files.\nWithin this environment, you can run configure or compile commands as if\nthey were being run by the OpenEmbedded build system itself. As noted\nearlier, the working directory also automatically changes to the Source\nDirectory (:term:`S`).\nTo manually run a specific task using ``devshell``, run the\ncorresponding ``run.*`` script in the\n``${``\\ :term:`WORKDIR`\\ ``}/temp``\ndirectory (e.g., ``run.do_configure.``\\ `pid`). If a task's script does\nnot exist, which would be the case if the task was skipped by way of the\nsstate cache, you can create the task by first running it outside of the\n``devshell``::\n$ bitbake -c task\n.. note::\n-  Execution of a task's ``run.*`` script and BitBake's execution of\na task are identical. In other words, running the script re-runs\nthe task just as it would be run using the ``bitbake -c`` command.\n-  Any ``run.*`` file that does not have a ``.pid`` extension is a\nsymbolic link (symlink) to the most recent version of that file.\nRemember, that the ``devshell`` is a mechanism that allows you to get\ninto the BitBake task execution environment. And as such, all commands\nmust be called just as BitBake would call them. That means you need to\nprovide the appropriate options for cross-compilation and so forth as\napplicable.\nWhen you are finished using ``devshell``, exit the shell or close the\nterminal window.\n.. note::\n-  It is worth remembering that when using ``devshell`` you need to\nuse the full compiler name such as ``arm-poky-linux-gnueabi-gcc``\ninstead of just using ``gcc``. The same applies to other\napplications such as ``binutils``, ``libtool`` and so forth.\nBitBake sets up environment variables such as :term:`CC` to assist\napplications, such as ``make`` to find the correct tools.\n-  It is also worth noting that ``devshell`` still works over X11\nforwarding and similar situations."}
{"text": "\nCustomizing Images\n******************\nYou can customize images to satisfy particular requirements. This\nsection describes several methods and provides guidelines for each.\nCustomizing Images Using ``local.conf``\n=======================================\nProbably the easiest way to customize an image is to add a package by\nway of the ``local.conf`` configuration file. Because it is limited to\nlocal use, this method generally only allows you to add packages and is\nnot as flexible as creating your own customized image. When you add\npackages using local variables this way, you need to realize that these\nvariable changes are in effect for every build and consequently affect\nall images, which might not be what you require.\nTo add a package to your image using the local configuration file, use\nthe :term:`IMAGE_INSTALL` variable with the ``:append`` operator::\nIMAGE_INSTALL:append = \" strace\"\nUse of the syntax is important; specifically, the leading space\nafter the opening quote and before the package name, which is\n``strace`` in this example. This space is required since the ``:append``\noperator does not add the space.\nFurthermore, you must use ``:append`` instead of the ``+=`` operator if\nyou want to avoid ordering issues. The reason for this is because doing\nso unconditionally appends to the variable and avoids ordering problems\ndue to the variable being set in image recipes and ``.bbclass`` files\nwith operators like ``?=``. Using ``:append`` ensures the operation\ntakes effect.\nAs shown in its simplest use, ``IMAGE_INSTALL:append`` affects all\nimages. It is possible to extend the syntax so that the variable applies\nto a specific image only. Here is an example::\nIMAGE_INSTALL:append:pn-core-image-minimal = \" strace\"\nThis example adds ``strace`` to the ``core-image-minimal`` image only.\nYou can add packages using a similar approach through the\n:term:`CORE_IMAGE_EXTRA_INSTALL` variable. If you use this variable, only\n``core-image-*`` images are affected.\nCustomizing Images Using Custom ``IMAGE_FEATURES`` and ``EXTRA_IMAGE_FEATURES``\n===============================================================================\nAnother method for customizing your image is to enable or disable\nhigh-level image features by using the\n:term:`IMAGE_FEATURES` and\n:term:`EXTRA_IMAGE_FEATURES`\nvariables. Although the functions for both variables are nearly\nequivalent, best practices dictate using :term:`IMAGE_FEATURES` from within\na recipe and using :term:`EXTRA_IMAGE_FEATURES` from within your\n``local.conf`` file, which is found in the :term:`Build Directory`.\nTo understand how these features work, the best reference is\n:ref:`meta/classes-recipe/image.bbclass <ref-classes-image>`.\nThis class lists out the available\n:term:`IMAGE_FEATURES` of which most map to package groups while some, such\nas ``debug-tweaks`` and ``read-only-rootfs``, resolve as general\nconfiguration settings.\nIn summary, the file looks at the contents of the :term:`IMAGE_FEATURES`\nvariable and then maps or configures the feature accordingly. Based on\nthis information, the build system automatically adds the appropriate\npackages or configurations to the\n:term:`IMAGE_INSTALL` variable.\nEffectively, you are enabling extra features by extending the class or\ncreating a custom class for use with specialized image ``.bb`` files.\nUse the :term:`EXTRA_IMAGE_FEATURES` variable from within your local\nconfiguration file. Using a separate area from which to enable features\nwith this variable helps you avoid overwriting the features in the image\nrecipe that are enabled with :term:`IMAGE_FEATURES`. The value of\n:term:`EXTRA_IMAGE_FEATURES` is added to :term:`IMAGE_FEATURES` within\n``meta/conf/bitbake.conf``.\nTo illustrate how you can use these variables to modify your image,\nconsider an example that selects the SSH server. The Yocto Project ships\nwith two SSH servers you can use with your images: Dropbear and OpenSSH.\nDropbear is a minimal SSH server appropriate for resource-constrained\nenvironments, while OpenSSH is a well-known standard SSH server\nimplementation. By default, the ``core-image-sato`` image is configured\nto use Dropbear. The ``core-image-full-cmdline`` and ``core-image-lsb``\nimages both include OpenSSH. The ``core-image-minimal`` image does not\ncontain an SSH server.\nYou can customize your image and change these defaults. Edit the\n:term:`IMAGE_FEATURES` variable in your recipe or use the\n:term:`EXTRA_IMAGE_FEATURES` in your ``local.conf`` file so that it\nconfigures the image you are working with to include\n``ssh-server-dropbear`` or ``ssh-server-openssh``.\n.. note::\nSee the \":ref:`ref-manual/features:image features`\" section in the Yocto\nProject Reference Manual for a complete list of image features that ship\nwith the Yocto Project.\nCustomizing Images Using Custom .bb Files\n=========================================\nYou can also customize an image by creating a custom recipe that defines\nadditional software as part of the image. The following example shows\nthe form for the two lines you need::\nIMAGE_INSTALL = \"packagegroup-core-x11-base package1 package2\"\ninherit core-image\nDefining the software using a custom recipe gives you total control over\nthe contents of the image. It is important to use the correct names of\npackages in the :term:`IMAGE_INSTALL` variable. You must use the\nOpenEmbedded notation and not the Debian notation for the names (e.g.\n``glibc-dev`` instead of ``libc6-dev``).\nThe other method for creating a custom image is to base it on an\nexisting image. For example, if you want to create an image based on\n``core-image-sato`` but add the additional package ``strace`` to the\nimage, copy the ``meta/recipes-sato/images/core-image-sato.bb`` to a new\n``.bb`` and add the following line to the end of the copy::\nIMAGE_INSTALL += \"strace\""}
{"text": "\nCustomizing Images Using Custom Package Groups\n==============================================\nFor complex custom images, the best approach for customizing an image is\nto create a custom package group recipe that is used to build the image\nor images. A good example of a package group recipe is\n``meta/recipes-core/packagegroups/packagegroup-base.bb``.\nIf you examine that recipe, you see that the :term:`PACKAGES` variable lists\nthe package group packages to produce. The ``inherit packagegroup``\nstatement sets appropriate default values and automatically adds\n``-dev``, ``-dbg``, and ``-ptest`` complementary packages for each\npackage specified in the :term:`PACKAGES` statement.\n.. note::\nThe ``inherit packagegroup`` line should be located near the top of the\nrecipe, certainly before the :term:`PACKAGES` statement.\nFor each package you specify in :term:`PACKAGES`, you can use :term:`RDEPENDS`\nand :term:`RRECOMMENDS` entries to provide a list of packages the parent\ntask package should contain. You can see examples of these further down\nin the ``packagegroup-base.bb`` recipe.\nHere is a short, fabricated example showing the same basic pieces for a\nhypothetical packagegroup defined in ``packagegroup-custom.bb``, where\nthe variable :term:`PN` is the standard way to abbreviate the reference to\nthe full packagegroup name ``packagegroup-custom``::\nDESCRIPTION = \"My Custom Package Groups\"\ninherit packagegroup\nPACKAGES = \"\\\n${PN}-apps \\\n${PN}-tools \\\n\"\nRDEPENDS:${PN}-apps = \"\\\ndropbear \\\nportmap \\\npsplash\"\nRDEPENDS:${PN}-tools = \"\\\noprofile \\\noprofileui-server \\\nlttng-tools\"\nRRECOMMENDS:${PN}-tools = \"\\\nkernel-module-oprofile\"\nIn the previous example, two package group packages are created with\ntheir dependencies and their recommended package dependencies listed:\n``packagegroup-custom-apps``, and ``packagegroup-custom-tools``. To\nbuild an image using these package group packages, you need to add\n``packagegroup-custom-apps`` and/or ``packagegroup-custom-tools`` to\n:term:`IMAGE_INSTALL`. For other forms of image dependencies see the other\nareas of this section.\nCustomizing an Image Hostname\n=============================\nBy default, the configured hostname (i.e. ``/etc/hostname``) in an image\nis the same as the machine name. For example, if\n:term:`MACHINE` equals \"qemux86\", the\nconfigured hostname written to ``/etc/hostname`` is \"qemux86\".\nYou can customize this name by altering the value of the \"hostname\"\nvariable in the ``base-files`` recipe using either an append file or a\nconfiguration file. Use the following in an append file::\nhostname = \"myhostname\"\nUse the following in a configuration file::\nhostname:pn-base-files = \"myhostname\"\nChanging the default value of the variable \"hostname\" can be useful in\ncertain situations. For example, suppose you need to do extensive\ntesting on an image and you would like to easily identify the image\nunder test from existing images with typical default hostnames. In this\nsituation, you could change the default hostname to \"testme\", which\nresults in all the images using the name \"testme\". Once testing is\ncomplete and you do not need to rebuild the image for test any longer,\nyou can easily reset the default hostname.\nAnother point of interest is that if you unset the variable, the image\nwill have no default hostname in the filesystem. Here is an example that\nunsets the variable in a configuration file::\nhostname:pn-base-files = \"\"\nHaving no default hostname in the filesystem is suitable for\nenvironments that use dynamic hostnames such as virtual machines."}
{"text": "\n*******************************\nUsing the Quick EMUlator (QEMU)\n*******************************\nThe Yocto Project uses an implementation of the Quick EMUlator (QEMU)\nOpen Source project as part of the Yocto Project development \"tool set\".\nThis chapter provides both procedures that show you how to use the Quick\nEMUlator (QEMU) and other QEMU information helpful for development\npurposes.\nOverview\n========\nWithin the context of the Yocto Project, QEMU is an emulator and\nvirtualization machine that allows you to run a complete image you have\nbuilt using the Yocto Project as just another task on your build system.\nQEMU is useful for running and testing images and applications on\nsupported Yocto Project architectures without having actual hardware.\nAmong other things, the Yocto Project uses QEMU to run automated Quality\nAssurance (QA) tests on final images shipped with each release.\n.. note::\nThis implementation is not the same as QEMU in general.\nThis section provides a brief reference for the Yocto Project\nimplementation of QEMU.\nFor official information and documentation on QEMU in general, see the\nfollowing references:\n-  `QEMU Website <https://wiki.qemu.org/Main_Page>`__\\ *:* The official\nwebsite for the QEMU Open Source project.\n-  `Documentation <https://wiki.qemu.org/Manual>`__\\ *:* The QEMU user\nmanual.\nRunning QEMU\n============\nTo use QEMU, you need to have QEMU installed and initialized as well as\nhave the proper artifacts (i.e. image files and root filesystems)\navailable. Follow these general steps to run QEMU:\n#. *Install QEMU:* QEMU is made available with the Yocto Project a\nnumber of ways. One method is to install a Software Development Kit\n(SDK). See \":ref:`sdk-manual/intro:the qemu emulator`\" section in the\nYocto Project Application Development and the Extensible Software\nDevelopment Kit (eSDK) manual for information on how to install QEMU.\n#. *Setting Up the Environment:* How you set up the QEMU environment\ndepends on how you installed QEMU:\n-  If you cloned the ``poky`` repository or you downloaded and\nunpacked a Yocto Project release tarball, you can source the build\nenvironment script (i.e. :ref:`structure-core-script`)::\n$ cd poky\n$ source oe-init-build-env\n-  If you installed a cross-toolchain, you can run the script that\ninitializes the toolchain. For example, the following commands run\nthe initialization script from the default ``poky_sdk`` directory::\n. poky_sdk/environment-setup-core2-64-poky-linux\n#. *Ensure the Artifacts are in Place:* You need to be sure you have a\npre-built kernel that will boot in QEMU. You also need the target\nroot filesystem for your target machine's architecture:\n-  If you have previously built an image for QEMU (e.g. ``qemux86``,\n``qemuarm``, and so forth), then the artifacts are in place in\nyour :term:`Build Directory`.\n-  If you have not built an image, you can go to the\n:yocto_dl:`machines/qemu </releases/yocto/yocto-&DISTRO;/machines/qemu/>` area and download a\npre-built image that matches your architecture and can be run on\nQEMU.\nSee the \":ref:`sdk-manual/appendix-obtain:extracting the root filesystem`\"\nsection in the Yocto Project Application Development and the\nExtensible Software Development Kit (eSDK) manual for information on\nhow to extract a root filesystem.\n#. *Run QEMU:* The basic ``runqemu`` command syntax is as follows::\n$ runqemu [option ] [...]\nBased on what you provide on the command\nline, ``runqemu`` does a good job of figuring out what you are trying\nto do. For example, by default, QEMU looks for the most recently\nbuilt image according to the timestamp when it needs to look for an\nimage. Minimally, through the use of options, you must provide either\na machine name, a virtual machine image (``*wic.vmdk``), or a kernel\nimage (``*.bin``).\nHere are some additional examples to help illustrate further QEMU:\n-  This example starts QEMU with MACHINE set to \"qemux86-64\".\nAssuming a standard :term:`Build Directory`, ``runqemu``\nautomatically finds the ``bzImage-qemux86-64.bin`` image file and\nthe ``core-image-minimal-qemux86-64-20200218002850.rootfs.ext4``\n(assuming the current build created a ``core-image-minimal``\nimage)::\n$ runqemu qemux86-64\n.. note::\nWhen more than one image with the same name exists, QEMU finds\nand uses the most recently built image according to the\ntimestamp.\n-  This example produces the exact same results as the previous\nexample. This command, however, specifically provides the image\nand root filesystem type::\n$ runqemu qemux86-64 core-image-minimal ext4\n-  This example specifies to boot an :term:`Initramfs` image and to\nenable audio in QEMU. For this case, ``runqemu`` sets the internal\nvariable ``FSTYPE`` to ``cpio.gz``. Also, for audio to be enabled,\nan appropriate driver must be installed (see the ``audio`` option\nin :ref:`dev-manual/qemu:\\`\\`runqemu\\`\\` command-line options`\nfor more information)::\n$ runqemu qemux86-64 ramfs audio\n-  This example does not provide enough information for QEMU to\nlaunch. While the command does provide a root filesystem type, it\nmust also minimally provide a `MACHINE`, `KERNEL`, or `VM` option::\n$ runqemu ext4\n-  This example specifies to boot a virtual machine image"}
{"text": "\n\n(``.wic.vmdk`` file). From the ``.wic.vmdk``, ``runqemu``\ndetermines the QEMU architecture (`MACHINE`) to be \"qemux86-64\" and\nthe root filesystem type to be \"vmdk\"::\n$ runqemu /home/scott-lenovo/vm/core-image-minimal-qemux86-64.wic.vmdk\nSwitching Between Consoles\n==========================\nWhen booting or running QEMU, you can switch between supported consoles\nby using Ctrl+Alt+number. For example, Ctrl+Alt+3 switches you to the\nserial console as long as that console is enabled. Being able to switch\nconsoles is helpful, for example, if the main QEMU console breaks for\nsome reason.\n.. note::\nUsually, \"2\" gets you to the main console and \"3\" gets you to the\nserial console.\nRemoving the Splash Screen\n==========================\nYou can remove the splash screen when QEMU is booting by using Alt+left.\nRemoving the splash screen allows you to see what is happening in the\nbackground.\nDisabling the Cursor Grab\n=========================\nThe default QEMU integration captures the cursor within the main window.\nIt does this since standard mouse devices only provide relative input\nand not absolute coordinates. You then have to break out of the grab\nusing the \"Ctrl+Alt\" key combination. However, the Yocto Project's\nintegration of QEMU enables the wacom USB touch pad driver by default to\nallow input of absolute coordinates. This default means that the mouse\ncan enter and leave the main window without the grab taking effect\nleading to a better user experience.\nRunning Under a Network File System (NFS) Server\n================================================\nOne method for running QEMU is to run it on an NFS server. This is\nuseful when you need to access the same file system from both the build\nand the emulated system at the same time. It is also worth noting that\nthe system does not need root privileges to run. It uses a user space\nNFS server to avoid that. Follow these steps to set up for running QEMU\nusing an NFS server.\n#. *Extract a Root Filesystem:* Once you are able to run QEMU in your\nenvironment, you can use the ``runqemu-extract-sdk`` script, which is\nlocated in the ``scripts`` directory along with the ``runqemu``\nscript.\nThe ``runqemu-extract-sdk`` takes a root filesystem tarball and\nextracts it into a location that you specify. Here is an example that\ntakes a file system and extracts it to a directory named\n``test-nfs``:\n.. code-block:: none\nrunqemu-extract-sdk ./tmp/deploy/images/qemux86-64/core-image-sato-qemux86-64.tar.bz2 test-nfs\n#. *Start QEMU:* Once you have extracted the file system, you can run\n``runqemu`` normally with the additional location of the file system.\nYou can then also make changes to the files within ``./test-nfs`` and\nsee those changes appear in the image in real time. Here is an\nexample using the ``qemux86`` image:\n.. code-block:: none\nrunqemu qemux86-64 ./test-nfs\n.. note::\nShould you need to start, stop, or restart the NFS share, you can use\nthe following commands:\n-  To start the NFS share::\nrunqemu-export-rootfs start file-system-location\n-  To stop the NFS share::\nrunqemu-export-rootfs stop file-system-location\n-  To restart the NFS share::\nrunqemu-export-rootfs restart file-system-location\nQEMU CPU Compatibility Under KVM\n================================\nBy default, the QEMU build compiles for and targets 64-bit and x86 Intel\nCore2 Duo processors and 32-bit x86 Intel Pentium II processors. QEMU\nbuilds for and targets these CPU types because they display a broad\nrange of CPU feature compatibility with many commonly used CPUs.\nDespite this broad range of compatibility, the CPUs could support a\nfeature that your host CPU does not support. Although this situation is\nnot a problem when QEMU uses software emulation of the feature, it can\nbe a problem when QEMU is running with KVM enabled. Specifically,\nsoftware compiled with a certain CPU feature crashes when run on a CPU\nunder KVM that does not support that feature. To work around this\nproblem, you can override QEMU's runtime CPU setting by changing the\n``QB_CPU_KVM`` variable in ``qemuboot.conf`` in the :term:`Build Directory`\n``deploy/image`` directory. This setting specifies a ``-cpu`` option passed\ninto QEMU in the ``runqemu`` script. Running ``qemu -cpu help`` returns a\nlist of available supported CPU types.\nQEMU Performance\n================\nUsing QEMU to emulate your hardware can result in speed issues depending\non the target and host architecture mix. For example, using the\n``qemux86`` image in the emulator on an Intel-based 32-bit (x86) host\nmachine is fast because the target and host architectures match. On the\nother hand, using the ``qemuarm`` image on the same Intel-based host can\nbe slower. But, you still achieve faithful emulation of ARM-specific\nissues.\nTo speed things up, the QEMU images support using ``distcc`` to call a\ncross-compiler outside the emulated system. If you used ``runqemu`` to\nstart QEMU, and the ``distccd`` application is present on the host\nsystem, any BitBake cross-compiling toolchain available from the build\nsystem is automatically used from within QEMU simply by calling\n``distcc``. You can accomplish this by defining the cross-compiler\nvariable (e.g. ``export CC=\"distcc\"``). Alternatively, if you are using\na suitable SDK image or the appropriate stand-alone toolchain is\npresent, the toolchain is also automatically used.\n.. note::\nThere are several mechanisms to connect to the system running"}
{"text": "\n\non the QEMU emulator:\n-  QEMU provides a framebuffer interface that makes standard consoles\navailable.\n-  Generally, headless embedded devices have a serial port. If so,\nyou can configure the operating system of the running image to use\nthat port to run a console. The connection uses standard IP\nnetworking.\n-  SSH servers are available in some QEMU images. The ``core-image-sato``\nQEMU image has a Dropbear secure shell (SSH) server that runs with\nthe root password disabled. The ``core-image-full-cmdline`` and\n``core-image-lsb`` QEMU images have OpenSSH instead of Dropbear.\nIncluding these SSH servers allow you to use standard ``ssh`` and\n``scp`` commands. The ``core-image-minimal`` QEMU image, however,\ncontains no SSH server.\n-  You can use a provided, user-space NFS server to boot the QEMU\nsession using a local copy of the root filesystem on the host. In\norder to make this connection, you must extract a root filesystem\ntarball by using the ``runqemu-extract-sdk`` command. After\nrunning the command, you must then point the ``runqemu`` script to\nthe extracted directory instead of a root filesystem image file.\nSee the\n\":ref:`dev-manual/qemu:running under a network file system (nfs) server`\"\nsection for more information.\nQEMU Command-Line Syntax\n========================\nThe basic ``runqemu`` command syntax is as follows::\n$ runqemu [option ] [...]\nBased on what you provide on the command line, ``runqemu`` does a\ngood job of figuring out what you are trying to do. For example, by\ndefault, QEMU looks for the most recently built image according to the\ntimestamp when it needs to look for an image. Minimally, through the use\nof options, you must provide either a machine name, a virtual machine\nimage (``*wic.vmdk``), or a kernel image (``*.bin``).\nFollowing is the command-line help output for the ``runqemu`` command::\n$ runqemu --help\nUsage: you can run this script with any valid combination\nof the following environment variables (in any order):\nKERNEL - the kernel image file to use\nROOTFS - the rootfs image file or nfsroot directory to use\nMACHINE - the machine name (optional, autodetected from KERNEL filename if unspecified)\nSimplified QEMU command-line options can be passed with:\nnographic - disable video console\nserial - enable a serial console on /dev/ttyS0\nslirp - enable user networking, no root privileges required\nkvm - enable KVM when running x86/x86_64 (VT-capable CPU required)\nkvm-vhost - enable KVM with vhost when running x86/x86_64 (VT-capable CPU required)\npublicvnc - enable a VNC server open to all hosts\naudio - enable audio\n[*/]ovmf* - OVMF firmware file or base name for booting with UEFI\ntcpserial=<port> - specify tcp serial port number\nbiosdir=<dir> - specify custom bios dir\nbiosfilename=<filename> - specify bios filename\nqemuparams=<xyz> - specify custom parameters to QEMU\nbootparams=<xyz> - specify custom kernel parameters during boot\nhelp, -h, --help: print this text\nExamples:\nrunqemu\nrunqemu qemuarm\nrunqemu tmp/deploy/images/qemuarm\nrunqemu tmp/deploy/images/qemux86/<qemuboot.conf>\nrunqemu qemux86-64 core-image-sato ext4\nrunqemu qemux86-64 wic-image-minimal wic\nrunqemu path/to/bzImage-qemux86.bin path/to/nfsrootdir/ serial\nrunqemu qemux86 iso/hddimg/wic.vmdk/wic.qcow2/wic.vdi/ramfs/cpio.gz...\nrunqemu qemux86 qemuparams=\"-m 256\"\nrunqemu qemux86 bootparams=\"psplash=false\"\nrunqemu path/to/<image>-<machine>.wic\nrunqemu path/to/<image>-<machine>.wic.vmdk\n``runqemu`` Command-Line Options\n================================\nFollowing is a description of ``runqemu`` options you can provide on the\ncommand line:\n.. note::\nIf you do provide some \"illegal\" option combination or perhaps you do\nnot provide enough in the way of options, ``runqemu``\nprovides appropriate error messaging to help you correct the problem.\n-  `QEMUARCH`: The QEMU machine architecture, which must be \"qemuarm\",\n\"qemuarm64\", \"qemumips\", \"qemumips64\", \"qemuppc\", \"qemux86\", or\n\"qemux86-64\".\n-  `VM`: The virtual machine image, which must be a ``.wic.vmdk``\nfile. Use this option when you want to boot a ``.wic.vmdk`` image.\nThe image filename you provide must contain one of the following\nstrings: \"qemux86-64\", \"qemux86\", \"qemuarm\", \"qemumips64\",\n\"qemumips\", \"qemuppc\", or \"qemush4\".\n-  `ROOTFS`: A root filesystem that has one of the following filetype\nextensions: \"ext2\", \"ext3\", \"ext4\", \"jffs2\", \"nfs\", or \"btrfs\". If\nthe filename you provide for this option uses \"nfs\", it must provide\nan explicit root filesystem path.\n-  `KERNEL`: A kernel image, which is a ``.bin`` file. When you provide a\n``.bin`` file, ``runqemu`` detects it and assumes the file is a\nkernel image.\n-  `MACHINE`: The architecture of the QEMU machine, which must be one of\nthe following: \"qemux86\", \"qemux86-64\", \"qemuarm\", \"qemuarm64\",\n\"qemumips\", \"qemumips64\", or \"qemuppc\". The MACHINE and QEMUARCH\noptions are basically identical. If you do not provide a MACHINE\noption, ``runqemu`` tries to determine it based on other options.\n-  ``ramfs``: Indicates you are booting an :term:`Initramfs`\nimage, which means the ``FSTYPE`` is ``cpio.gz``.\n-  ``iso``: Indicates you are booting an ISO image, which means the\n``FSTYPE`` is ``.iso``."}
{"text": "\n\n-  ``nographic``: Disables the video console, which sets the console to\n\"ttys0\". This option is useful when you have logged into a server and\nyou do not want to disable forwarding from the X Window System (X11)\nto your workstation or laptop.\n-  ``serial``: Enables a serial console on ``/dev/ttyS0``.\n-  ``biosdir``: Establishes a custom directory for BIOS, VGA BIOS and\nkeymaps.\n-  ``biosfilename``: Establishes a custom BIOS name.\n-  ``qemuparams=\\\"xyz\\\"``: Specifies custom QEMU parameters. Use this\noption to pass options other than the simple \"kvm\" and \"serial\"\noptions.\n-  ``bootparams=\\\"xyz\\\"``: Specifies custom boot parameters for the\nkernel.\n-  ``audio``: Enables audio in QEMU. The MACHINE option must be either\n\"qemux86\" or \"qemux86-64\" in order for audio to be enabled.\nAdditionally, the ``snd_intel8x0`` or ``snd_ens1370`` driver must be\ninstalled in linux guest.\n-  ``slirp``: Enables \"slirp\" networking, which is a different way of\nnetworking that does not need root access but also is not as easy to\nuse or comprehensive as the default.\nUsing ``slirp`` by default will forward the guest machine's\n22 and 23 TCP ports to host machine's 2222 and 2323 ports\n(or the next free ports). Specific forwarding rules can be configured\nby setting ``QB_SLIRP_OPT`` as environment variable or in ``qemuboot.conf``\nin the :term:`Build Directory` ``deploy/image`` directory.\nExamples::\nQB_SLIRP_OPT=\"-netdev user,id=net0,hostfwd=tcp::8080-:80\"\nQB_SLIRP_OPT=\"-netdev user,id=net0,hostfwd=tcp::8080-:80,hostfwd=tcp::2222-:22\"\nThe first example forwards TCP port 80 from the emulated system to\nport 8080 (or the next free port) on the host system,\nallowing access to an http server running in QEMU from\n``http://<host ip>:8080/``.\nThe second example does the same, but also forwards TCP port 22 on the\nguest system to 2222 (or the next free port) on the host system,\nallowing ssh access to the emulated system using\n``ssh -P 2222 <user>@<host ip>``.\nKeep in mind that proper configuration of firewall software is required.\n-  ``kvm``: Enables KVM when running \"qemux86\" or \"qemux86-64\" QEMU\narchitectures. For KVM to work, all the following conditions must be\nmet:\n-  Your MACHINE must be either qemux86\" or \"qemux86-64\".\n-  Your build host has to have the KVM modules installed, which are\n``/dev/kvm``.\n-  The build host ``/dev/kvm`` directory has to be both writable and\nreadable.\n-  ``kvm-vhost``: Enables KVM with VHOST support when running \"qemux86\"\nor \"qemux86-64\" QEMU architectures. For KVM with VHOST to work, the\nfollowing conditions must be met:\n-  ``kvm`` option conditions defined above must be met.\n-  Your build host has to have virtio net device, which are\n``/dev/vhost-net``.\n-  The build host ``/dev/vhost-net`` directory has to be either\nreadable or writable and \"slirp-enabled\".\n-  ``publicvnc``: Enables a VNC server open to all hosts."}
{"text": "\nCreating a Read-Only Root Filesystem\n************************************\nSuppose, for security reasons, you need to disable your target device's\nroot filesystem's write permissions (i.e. you need a read-only root\nfilesystem). Or, perhaps you are running the device's operating system\nfrom a read-only storage device. For either case, you can customize your\nimage for that behavior.\n.. note::\nSupporting a read-only root filesystem requires that the system and\napplications do not try to write to the root filesystem. You must\nconfigure all parts of the target system to write elsewhere, or to\ngracefully fail in the event of attempting to write to the root\nfilesystem.\nCreating the Root Filesystem\n============================\nTo create the read-only root filesystem, simply add the\n\"read-only-rootfs\" feature to your image, normally in one of two ways.\nThe first way is to add the \"read-only-rootfs\" image feature in the\nimage's recipe file via the :term:`IMAGE_FEATURES` variable::\nIMAGE_FEATURES += \"read-only-rootfs\"\nAs an alternative, you can add the same feature\nfrom within your :term:`Build Directory`'s ``local.conf`` file with the\nassociated :term:`EXTRA_IMAGE_FEATURES` variable, as in::\nEXTRA_IMAGE_FEATURES = \"read-only-rootfs\"\nFor more information on how to use these variables, see the\n\":ref:`dev-manual/customizing-images:Customizing Images Using Custom \\`\\`IMAGE_FEATURES\\`\\` and \\`\\`EXTRA_IMAGE_FEATURES\\`\\``\"\nsection. For information on the variables, see\n:term:`IMAGE_FEATURES` and\n:term:`EXTRA_IMAGE_FEATURES`.\nPost-Installation Scripts and Read-Only Root Filesystem\n=======================================================\nIt is very important that you make sure all post-Installation\n(``pkg_postinst``) scripts for packages that are installed into the\nimage can be run at the time when the root filesystem is created during\nthe build on the host system. These scripts cannot attempt to run during\nthe first boot on the target device. With the \"read-only-rootfs\" feature\nenabled, the build system makes sure that all post-installation scripts\nsucceed at file system creation time. If any of these scripts\nstill need to be run after the root filesystem is created, the build\nimmediately fails. These build-time checks ensure that the build fails\nrather than the target device fails later during its initial boot\noperation.\nMost of the common post-installation scripts generated by the build\nsystem for the out-of-the-box Yocto Project are engineered so that they\ncan run during root filesystem creation (e.g. post-installation scripts\nfor caching fonts). However, if you create and add custom scripts, you\nneed to be sure they can be run during this file system creation.\nHere are some common problems that prevent post-installation scripts\nfrom running during root filesystem creation:\n-  *Not using $D in front of absolute paths:* The build system defines\n``$``\\ :term:`D` when the root\nfilesystem is created. Furthermore, ``$D`` is blank when the script\nis run on the target device. This implies two purposes for ``$D``:\nensuring paths are valid in both the host and target environments,\nand checking to determine which environment is being used as a method\nfor taking appropriate actions.\n-  *Attempting to run processes that are specific to or dependent on the\ntarget architecture:* You can work around these attempts by using\nnative tools, which run on the host system, to accomplish the same\ntasks, or by alternatively running the processes under QEMU, which\nhas the ``qemu_run_binary`` function. For more information, see the\n:ref:`ref-classes-qemu` class.\nAreas With Write Access\n=======================\nWith the \"read-only-rootfs\" feature enabled, any attempt by the target\nto write to the root filesystem at runtime fails. Consequently, you must\nmake sure that you configure processes and applications that attempt\nthese types of writes do so to directories with write access (e.g.\n``/tmp`` or ``/var/run``)."}
{"text": "\nWriting a New Recipe\n********************\nRecipes (``.bb`` files) are fundamental components in the Yocto Project\nenvironment. Each software component built by the OpenEmbedded build\nsystem requires a recipe to define the component. This section describes\nhow to create, write, and test a new recipe.\n.. note::\nFor information on variables that are useful for recipes and for\ninformation about recipe naming issues, see the\n\":ref:`ref-manual/varlocality:recipes`\" section of the Yocto Project\nReference Manual.\nOverview\n========\nThe following figure shows the basic process for creating a new recipe.\nThe remainder of the section provides details for the steps.\n.. image:: figures/recipe-workflow.png\n:align: center\n:width: 50%\nLocate or Automatically Create a Base Recipe\n============================================\nYou can always write a recipe from scratch. However, there are three choices\nthat can help you quickly get started with a new recipe:\n-  ``devtool add``: A command that assists in creating a recipe and an\nenvironment conducive to development.\n-  ``recipetool create``: A command provided by the Yocto Project that\nautomates creation of a base recipe based on the source files.\n-  *Existing Recipes:* Location and modification of an existing recipe\nthat is similar in function to the recipe you need.\n.. note::\nFor information on recipe syntax, see the\n\":ref:`dev-manual/new-recipe:recipe syntax`\" section.\nCreating the Base Recipe Using ``devtool add``\n----------------------------------------------\nThe ``devtool add`` command uses the same logic for auto-creating the\nrecipe as ``recipetool create``, which is listed below. Additionally,\nhowever, ``devtool add`` sets up an environment that makes it easy for\nyou to patch the source and to make changes to the recipe as is often\nnecessary when adding a recipe to build a new piece of software to be\nincluded in a build.\nYou can find a complete description of the ``devtool add`` command in\nthe \":ref:`sdk-manual/extensible:a closer look at \\`\\`devtool add\\`\\``\" section\nin the Yocto Project Application Development and the Extensible Software\nDevelopment Kit (eSDK) manual.\nCreating the Base Recipe Using ``recipetool create``\n----------------------------------------------------\n``recipetool create`` automates creation of a base recipe given a set of\nsource code files. As long as you can extract or point to the source\nfiles, the tool will construct a recipe and automatically configure all\npre-build information into the recipe. For example, suppose you have an\napplication that builds using Autotools. Creating the base recipe using\n``recipetool`` results in a recipe that has the pre-build dependencies,\nlicense requirements, and checksums configured.\nTo run the tool, you just need to be in your :term:`Build Directory` and\nhave sourced the build environment setup script (i.e.\n:ref:`structure-core-script`). To get help on the tool, use the following\ncommand::\n$ recipetool -h\nNOTE: Starting bitbake server...\nusage: recipetool [-d] [-q] [--color COLOR] [-h] <subcommand> ...\nOpenEmbedded recipe tool\noptions:\n-d, --debug     Enable debug output\n-q, --quiet     Print only errors\n--color COLOR   Colorize output (where COLOR is auto, always, never)\n-h, --help      show this help message and exit\nsubcommands:\ncreate          Create a new recipe\nnewappend       Create a bbappend for the specified target in the specified\nlayer\nsetvar          Set a variable within a recipe\nappendfile      Create/update a bbappend to replace a target file\nappendsrcfiles  Create/update a bbappend to add or replace source files\nappendsrcfile   Create/update a bbappend to add or replace a source file\nUse recipetool <subcommand> --help to get help on a specific command\nRunning ``recipetool create -o OUTFILE`` creates the base recipe and\nlocates it properly in the layer that contains your source files.\nFollowing are some syntax examples:\n- Use this syntax to generate a recipe based on source. Once generated,\nthe recipe resides in the existing source code layer::\nrecipetool create -o OUTFILE source\n- Use this syntax to generate a recipe using code that\nyou extract from source. The extracted code is placed in its own layer\ndefined by :term:`EXTERNALSRC`::\nrecipetool create -o OUTFILE -x EXTERNALSRC source\n- Use this syntax to generate a recipe based on source. The options\ndirect ``recipetool`` to generate debugging information. Once generated,\nthe recipe resides in the existing source code layer::\nrecipetool create -d -o OUTFILE source\nLocating and Using a Similar Recipe\n-----------------------------------\nBefore writing a recipe from scratch, it is often useful to discover\nwhether someone else has already written one that meets (or comes close\nto meeting) your needs. The Yocto Project and OpenEmbedded communities\nmaintain many recipes that might be candidates for what you are doing.\nYou can find a good central index of these recipes in the\n:oe_layerindex:`OpenEmbedded Layer Index <>`.\nWorking from an existing recipe or a skeleton recipe is the best way to\nget started. Here are some points on both methods:\n-  *Locate and modify a recipe that is close to what you want to do:*"}
{"text": "\nThis method works when you are familiar with the current recipe\nspace. The method does not work so well for those new to the Yocto\nProject or writing recipes.\nSome risks associated with this method are using a recipe that has\nareas totally unrelated to what you are trying to accomplish with\nyour recipe, not recognizing areas of the recipe that you might have\nto add from scratch, and so forth. All these risks stem from\nunfamiliarity with the existing recipe space.\n-  *Use and modify the following skeleton recipe:* If for some reason\nyou do not want to use ``recipetool`` and you cannot find an existing\nrecipe that is close to meeting your needs, you can use the following\nstructure to provide the fundamental areas of a new recipe::\nDESCRIPTION = \"\"\nHOMEPAGE = \"\"\nLICENSE = \"\"\nSECTION = \"\"\nDEPENDS = \"\"\nLIC_FILES_CHKSUM = \"\"\nSRC_URI = \"\"\nStoring and Naming the Recipe\n=============================\nOnce you have your base recipe, you should put it in your own layer and\nname it appropriately. Locating it correctly ensures that the\nOpenEmbedded build system can find it when you use BitBake to process\nthe recipe.\n-  *Storing Your Recipe:* The OpenEmbedded build system locates your\nrecipe through the layer's ``conf/layer.conf`` file and the\n:term:`BBFILES` variable. This\nvariable sets up a path from which the build system can locate\nrecipes. Here is the typical use::\nBBFILES += \"${LAYERDIR}/recipes-*/*/*.bb \\\n${LAYERDIR}/recipes-*/*/*.bbappend\"\nConsequently, you need to be sure you locate your new recipe inside\nyour layer such that it can be found.\nYou can find more information on how layers are structured in the\n\":ref:`dev-manual/layers:understanding and creating layers`\" section.\n-  *Naming Your Recipe:* When you name your recipe, you need to follow\nthis naming convention::\nbasename_version.bb\nUse lower-cased characters and do not include the reserved suffixes\n``-native``, ``-cross``, ``-initial``, or ``-dev`` casually (i.e. do not use\nthem as part of your recipe name unless the string applies). Here are some\nexamples:\n.. code-block:: none\ncups_1.7.0.bb\ngawk_4.0.2.bb\nirssi_0.8.16-rc1.bb\nRunning a Build on the Recipe\n=============================\nCreating a new recipe is usually an iterative process that requires\nusing BitBake to process the recipe multiple times in order to\nprogressively discover and add information to the recipe file.\nAssuming you have sourced the build environment setup script (i.e.\n:ref:`structure-core-script`) and you are in the :term:`Build Directory`, use\nBitBake to process your recipe. All you need to provide is the\n``basename`` of the recipe as described in the previous section::\n$ bitbake basename\nDuring the build, the OpenEmbedded build system creates a temporary work\ndirectory for each recipe\n(``${``\\ :term:`WORKDIR`\\ ``}``)\nwhere it keeps extracted source files, log files, intermediate\ncompilation and packaging files, and so forth.\nThe path to the per-recipe temporary work directory depends on the\ncontext in which it is being built. The quickest way to find this path\nis to have BitBake return it by running the following::\n$ bitbake -e basename | grep ^WORKDIR=\nAs an example, assume a Source Directory\ntop-level folder named ``poky``, a default :term:`Build Directory` at\n``poky/build``, and a ``qemux86-poky-linux`` machine target system.\nFurthermore, suppose your recipe is named ``foo_1.3.0.bb``. In this\ncase, the work directory the build system uses to build the package\nwould be as follows::\npoky/build/tmp/work/qemux86-poky-linux/foo/1.3.0-r0\nInside this directory you can find sub-directories such as ``image``,\n``packages-split``, and ``temp``. After the build, you can examine these\nto determine how well the build went.\n.. note::\nYou can find log files for each task in the recipe's ``temp``\ndirectory (e.g. ``poky/build/tmp/work/qemux86-poky-linux/foo/1.3.0-r0/temp``).\nLog files are named ``log.taskname`` (e.g. ``log.do_configure``,\n``log.do_fetch``, and ``log.do_compile``).\nYou can find more information about the build process in\n\":doc:`/overview-manual/development-environment`\"\nchapter of the Yocto Project Overview and Concepts Manual.\nFetching Code\n=============\nThe first thing your recipe must do is specify how to fetch the source\nfiles. Fetching is controlled mainly through the\n:term:`SRC_URI` variable. Your recipe\nmust have a :term:`SRC_URI` variable that points to where the source is\nlocated. For a graphical representation of source locations, see the\n\":ref:`overview-manual/concepts:sources`\" section in\nthe Yocto Project Overview and Concepts Manual.\nThe :ref:`ref-tasks-fetch` task uses the prefix of each entry in the\n:term:`SRC_URI` variable value to determine which\n:ref:`fetcher <bitbake-user-manual/bitbake-user-manual-fetching:fetchers>`\nto use to get your source files. It is the :term:`SRC_URI` variable that triggers\nthe fetcher. The :ref:`ref-tasks-patch` task uses the variable after source is\nfetched to apply patches. The OpenEmbedded build system uses\n:term:`FILESOVERRIDES` for scanning directory locations for local files in"}
{"text": "\n:term:`SRC_URI`.\nThe :term:`SRC_URI` variable in your recipe must define each unique location\nfor your source files. It is good practice to not hard-code version\nnumbers in a URL used in :term:`SRC_URI`. Rather than hard-code these\nvalues, use ``${``\\ :term:`PV`\\ ``}``,\nwhich causes the fetch process to use the version specified in the\nrecipe filename. Specifying the version in this manner means that\nupgrading the recipe to a future version is as simple as renaming the\nrecipe to match the new version.\nHere is a simple example from the\n``meta/recipes-devtools/strace/strace_5.5.bb`` recipe where the source\ncomes from a single tarball. Notice the use of the\n:term:`PV` variable::\nSRC_URI = \"https://strace.io/files/${PV}/strace-${PV}.tar.xz \\\nFiles mentioned in :term:`SRC_URI` whose names end in a typical archive\nextension (e.g. ``.tar``, ``.tar.gz``, ``.tar.bz2``, ``.zip``, and so\nforth), are automatically extracted during the\n:ref:`ref-tasks-unpack` task. For\nanother example that specifies these types of files, see the\n\":ref:`dev-manual/new-recipe:building an autotooled package`\" section.\nAnother way of specifying source is from an SCM. For Git repositories,\nyou must specify :term:`SRCREV` and you should specify :term:`PV` to include\nthe revision with :term:`SRCPV`. Here is an example from the recipe\n``meta/recipes-core/musl/gcompat_git.bb``::\nSRC_URI = \"git://git.adelielinux.org/adelie/gcompat.git;protocol=https;branch=current\"\nPV = \"1.0.0+1.1+git${SRCPV}\"\nSRCREV = \"af5a49e489fdc04b9cf02547650d7aeaccd43793\"\nIf your :term:`SRC_URI` statement includes URLs pointing to individual files\nfetched from a remote server other than a version control system,\nBitBake attempts to verify the files against checksums defined in your\nrecipe to ensure they have not been tampered with or otherwise modified\nsince the recipe was written. Multiple checksums are supported:\n``SRC_URI[md5sum]``, ``SRC_URI[sha1sum]``, ``SRC_URI[sha256sum]``.\n``SRC_URI[sha384sum]`` and ``SRC_URI[sha512sum]``, but only\n``SRC_URI[sha256sum]`` is commonly used.\n.. note::\n``SRC_URI[md5sum]`` used to also be commonly used, but it is deprecated\nand should be replaced by ``SRC_URI[sha256sum]`` when updating existing\nrecipes.\nIf your :term:`SRC_URI` variable points to more than a single URL (excluding\nSCM URLs), you need to provide the ``sha256`` checksum for each URL. For these\ncases, you provide a name for each URL as part of the :term:`SRC_URI` and then\nreference that name in the subsequent checksum statements. Here is an example\ncombining lines from the files ``git.inc`` and ``git_2.24.1.bb``::\nSRC_URI = \"${KERNELORG_MIRROR}/software/scm/git/git-${PV}.tar.gz;name=tarball \\\n${KERNELORG_MIRROR}/software/scm/git/git-manpages-${PV}.tar.gz;name=manpages\"\nSRC_URI[tarball.sha256sum] = \"ad5334956301c86841eb1e5b1bb20884a6bad89a10a6762c958220c7cf64da02\"\nSRC_URI[manpages.sha256sum] = \"9a7ae3a093bea39770eb96ca3e5b40bff7af0b9f6123f089d7821d0e5b8e1230\"\nThe proper value for the ``sha256`` checksum might be available together\nwith other signatures on the download page for the upstream source (e.g.\n``md5``, ``sha1``, ``sha256``, ``GPG``, and so forth). Because the\nOpenEmbedded build system typically only deals with ``sha256sum``,\nyou should verify all the signatures you find by hand.\nIf no :term:`SRC_URI` checksums are specified when you attempt to build the\nrecipe, or you provide an incorrect checksum, the build will produce an\nerror for each missing or incorrect checksum. As part of the error\nmessage, the build system provides the checksum string corresponding to\nthe fetched file. Once you have the correct checksums, you can copy and\npaste them into your recipe and then run the build again to continue.\n.. note::\nAs mentioned, if the upstream source provides signatures for\nverifying the downloaded source code, you should verify those\nmanually before setting the checksum values in the recipe and\ncontinuing with the build.\nThis final example is a bit more complicated and is from the\n``meta/recipes-sato/rxvt-unicode/rxvt-unicode_9.20.bb`` recipe. The\nexample's :term:`SRC_URI` statement identifies multiple files as the source\nfiles for the recipe: a tarball, a patch file, a desktop file, and an icon::\nSRC_URI = \"http://dist.schmorp.de/rxvt-unicode/Attic/rxvt-unicode-${PV}.tar.bz2 \\\nfile://xwc.patch \\\nfile://rxvt.desktop \\\nfile://rxvt.png\"\nWhen you specify local files using the ``file://`` URI protocol, the\nbuild system fetches files from the local machine. The path is relative\nto the :term:`FILESPATH` variable\nand searches specific directories in a certain order:\n``${``\\ :term:`BP`\\ ``}``,\n``${``\\ :term:`BPN`\\ ``}``, and\n``files``. The directories are assumed to be subdirectories of the\ndirectory in which the recipe or append file resides. For another\nexample that specifies these types of files, see the\n\"`building a single .c file package`_\" section.\nThe previous example also specifies a patch file. Patch files are files\nwhose names usually end in ``.patch`` or ``.diff`` but can end with\ncompressed suffixes such as ``diff.gz`` and ``patch.bz2``, for example.\nThe build system automatically applies patches as described in the\n\":ref:`dev-manual/new-recipe:patching code`\" section.\nFetching Code Through Firewalls\n-------------------------------\nSome users are behind firewalls and need to fetch code through a proxy.\nSee the \":doc:`/ref-manual/faq`\" chapter for advice.\nLimiting the Number of Parallel Connections\n-------------------------------------------\nSome users are behind firewalls or use servers where the number of parallel\nconnections is limited. In such cases, you can limit the number of fetch\ntasks being run in parallel by adding the following to your ``local.conf``\nfile::\ndo_fetch[number_threads] = \"4\"\nUnpacking Code\n=============="}
{"text": "\nDuring the build, the\n:ref:`ref-tasks-unpack` task unpacks\nthe source with ``${``\\ :term:`S`\\ ``}``\npointing to where it is unpacked.\nIf you are fetching your source files from an upstream source archived\ntarball and the tarball's internal structure matches the common\nconvention of a top-level subdirectory named\n``${``\\ :term:`BPN`\\ ``}-${``\\ :term:`PV`\\ ``}``,\nthen you do not need to set :term:`S`. However, if :term:`SRC_URI` specifies to\nfetch source from an archive that does not use this convention, or from\nan SCM like Git or Subversion, your recipe needs to define :term:`S`.\nIf processing your recipe using BitBake successfully unpacks the source\nfiles, you need to be sure that the directory pointed to by ``${S}``\nmatches the structure of the source.\nPatching Code\n=============\nSometimes it is necessary to patch code after it has been fetched. Any\nfiles mentioned in :term:`SRC_URI` whose names end in ``.patch`` or\n``.diff`` or compressed versions of these suffixes (e.g. ``diff.gz``,\n``patch.bz2``, etc.) are treated as patches. The\n:ref:`ref-tasks-patch` task\nautomatically applies these patches.\nThe build system should be able to apply patches with the \"-p1\" option\n(i.e. one directory level in the path will be stripped off). If your\npatch needs to have more directory levels stripped off, specify the\nnumber of levels using the \"striplevel\" option in the :term:`SRC_URI` entry\nfor the patch. Alternatively, if your patch needs to be applied in a\nspecific subdirectory that is not specified in the patch file, use the\n\"patchdir\" option in the entry.\nAs with all local files referenced in\n:term:`SRC_URI` using ``file://``,\nyou should place patch files in a directory next to the recipe either\nnamed the same as the base name of the recipe\n(:term:`BP` and\n:term:`BPN`) or \"files\".\nLicensing\n=========\nYour recipe needs to define variables related to the license\nunder whith the software is distributed. See the\n:ref:`contributor-guide/recipe-style-guide:recipe license fields`\nsection in the Contributor Guide for details.\nDependencies\n============\nMost software packages have a short list of other packages that they\nrequire, which are called dependencies. These dependencies fall into two\nmain categories: build-time dependencies, which are required when the\nsoftware is built; and runtime dependencies, which are required to be\ninstalled on the target in order for the software to run.\nWithin a recipe, you specify build-time dependencies using the\n:term:`DEPENDS` variable. Although there are nuances,\nitems specified in :term:`DEPENDS` should be names of other\nrecipes. It is important that you specify all build-time dependencies\nexplicitly.\nAnother consideration is that configure scripts might automatically\ncheck for optional dependencies and enable corresponding functionality\nif those dependencies are found. If you wish to make a recipe that is\nmore generally useful (e.g. publish the recipe in a layer for others to\nuse), instead of hard-disabling the functionality, you can use the\n:term:`PACKAGECONFIG` variable to allow functionality and the\ncorresponding dependencies to be enabled and disabled easily by other\nusers of the recipe.\nSimilar to build-time dependencies, you specify runtime dependencies\nthrough a variable -\n:term:`RDEPENDS`, which is\npackage-specific. All variables that are package-specific need to have\nthe name of the package added to the end as an override. Since the main\npackage for a recipe has the same name as the recipe, and the recipe's\nname can be found through the\n``${``\\ :term:`PN`\\ ``}`` variable, then\nyou specify the dependencies for the main package by setting\n``RDEPENDS:${PN}``. If the package were named ``${PN}-tools``, then you\nwould set ``RDEPENDS:${PN}-tools``, and so forth.\nSome runtime dependencies will be set automatically at packaging time.\nThese dependencies include any shared library dependencies (i.e. if a\npackage \"example\" contains \"libexample\" and another package \"mypackage\"\ncontains a binary that links to \"libexample\" then the OpenEmbedded build\nsystem will automatically add a runtime dependency to \"mypackage\" on\n\"example\"). See the\n\":ref:`overview-manual/concepts:automatically added runtime dependencies`\"\nsection in the Yocto Project Overview and Concepts Manual for further\ndetails.\nConfiguring the Recipe\n======================\nMost software provides some means of setting build-time configuration\noptions before compilation. Typically, setting these options is\naccomplished by running a configure script with options, or by modifying\na build configuration file.\n.. note::\nAs of Yocto Project Release 1.7, some of the core recipes that\npackage binary configuration scripts now disable the scripts due to\nthe scripts previously requiring error-prone path substitution. The\nOpenEmbedded build system uses ``pkg-config`` now, which is much more\nrobust. You can find a list of the ``*-config`` scripts that are disabled\nin the \":ref:`migration-1.7-binary-configuration-scripts-disabled`\" section\nin the Yocto Project Reference Manual.\nA major part of build-time configuration is about checking for\nbuild-time dependencies and possibly enabling optional functionality as\na result. You need to specify any build-time dependencies for the\nsoftware you are building in your recipe's\n:term:`DEPENDS` value, in terms of"}
{"text": "\nother recipes that satisfy those dependencies. You can often find\nbuild-time or runtime dependencies described in the software's\ndocumentation.\nThe following list provides configuration items of note based on how\nyour software is built:\n-  *Autotools:* If your source files have a ``configure.ac`` file, then\nyour software is built using Autotools. If this is the case, you just\nneed to modify the configuration.\nWhen using Autotools, your recipe needs to inherit the\n:ref:`ref-classes-autotools` class and it does not have to\ncontain a :ref:`ref-tasks-configure` task. However, you might still want to\nmake some adjustments. For example, you can set :term:`EXTRA_OECONF` or\n:term:`PACKAGECONFIG_CONFARGS` to pass any needed configure options that\nare specific to the recipe.\n-  *CMake:* If your source files have a ``CMakeLists.txt`` file, then\nyour software is built using CMake. If this is the case, you just\nneed to modify the configuration.\nWhen you use CMake, your recipe needs to inherit the\n:ref:`ref-classes-cmake` class and it does not have to contain a\n:ref:`ref-tasks-configure` task. You can make some adjustments by setting\n:term:`EXTRA_OECMAKE` to pass any needed configure options that are\nspecific to the recipe.\n.. note::\nIf you need to install one or more custom CMake toolchain files\nthat are supplied by the application you are building, install the\nfiles to ``${D}${datadir}/cmake/Modules`` during :ref:`ref-tasks-install`.\n-  *Other:* If your source files do not have a ``configure.ac`` or\n``CMakeLists.txt`` file, then your software is built using some\nmethod other than Autotools or CMake. If this is the case, you\nnormally need to provide a\n:ref:`ref-tasks-configure` task\nin your recipe unless, of course, there is nothing to configure.\nEven if your software is not being built by Autotools or CMake, you\nstill might not need to deal with any configuration issues. You need\nto determine if configuration is even a required step. You might need\nto modify a Makefile or some configuration file used for the build to\nspecify necessary build options. Or, perhaps you might need to run a\nprovided, custom configure script with the appropriate options.\nFor the case involving a custom configure script, you would run\n``./configure --help`` and look for the options you need to set.\nOnce configuration succeeds, it is always good practice to look at the\n``log.do_configure`` file to ensure that the appropriate options have\nbeen enabled and no additional build-time dependencies need to be added\nto :term:`DEPENDS`. For example, if the configure script reports that it\nfound something not mentioned in :term:`DEPENDS`, or that it did not find\nsomething that it needed for some desired optional functionality, then\nyou would need to add those to :term:`DEPENDS`. Looking at the log might\nalso reveal items being checked for, enabled, or both that you do not\nwant, or items not being found that are in :term:`DEPENDS`, in which case\nyou would need to look at passing extra options to the configure script\nas needed. For reference information on configure options specific to\nthe software you are building, you can consult the output of the\n``./configure --help`` command within ``${S}`` or consult the software's\nupstream documentation.\nUsing Headers to Interface with Devices\n=======================================\nIf your recipe builds an application that needs to communicate with some\ndevice or needs an API into a custom kernel, you will need to provide\nappropriate header files. Under no circumstances should you ever modify\nthe existing\n``meta/recipes-kernel/linux-libc-headers/linux-libc-headers.inc`` file.\nThese headers are used to build ``libc`` and must not be compromised\nwith custom or machine-specific header information. If you customize\n``libc`` through modified headers all other applications that use\n``libc`` thus become affected.\n.. note::\nNever copy and customize the ``libc`` header file (i.e.\n``meta/recipes-kernel/linux-libc-headers/linux-libc-headers.inc``).\nThe correct way to interface to a device or custom kernel is to use a\nseparate package that provides the additional headers for the driver or\nother unique interfaces. When doing so, your application also becomes\nresponsible for creating a dependency on that specific provider.\nConsider the following:\n-  Never modify ``linux-libc-headers.inc``. Consider that file to be\npart of the ``libc`` system, and not something you use to access the\nkernel directly. You should access ``libc`` through specific ``libc``\ncalls.\n-  Applications that must talk directly to devices should either provide\nnecessary headers themselves, or establish a dependency on a special\nheaders package that is specific to that driver.\nFor example, suppose you want to modify an existing header that adds I/O\ncontrol or network support. If the modifications are used by a small\nnumber programs, providing a unique version of a header is easy and has\nlittle impact. When doing so, bear in mind the guidelines in the\nprevious list.\n.. note::\nIf for some reason your changes need to modify the behavior of the ``libc``,\nand subsequently all other applications on the system, use a ``.bbappend``\nto modify the ``linux-kernel-headers.inc`` file. However, take care to not\nmake the changes machine specific.\nConsider a case where your kernel is older and you need an older\n``libc`` ABI. The headers installed by your recipe should still be a\nstandard mainline kernel, not your own custom one.\nWhen you use custom kernel headers you need to get them from\n:term:`STAGING_KERNEL_DIR`,\nwhich is the directory with kernel headers that are required to build\nout-of-tree modules. Your recipe will also need the following::\ndo_configure[depends] += \"virtual/kernel:do_shared_workdir\"\nCompilation\n==========="}
{"text": "\nDuring a build, the :ref:`ref-tasks-compile` task happens after source is fetched,\nunpacked, and configured. If the recipe passes through :ref:`ref-tasks-compile`\nsuccessfully, nothing needs to be done.\nHowever, if the compile step fails, you need to diagnose the failure.\nHere are some common issues that cause failures.\n.. note::\nFor cases where improper paths are detected for configuration files\nor for when libraries/headers cannot be found, be sure you are using\nthe more robust ``pkg-config``. See the note in section\n\":ref:`dev-manual/new-recipe:Configuring the Recipe`\" for additional information.\n-  *Parallel build failures:* These failures manifest themselves as\nintermittent errors, or errors reporting that a file or directory\nthat should be created by some other part of the build process could\nnot be found. This type of failure can occur even if, upon\ninspection, the file or directory does exist after the build has\nfailed, because that part of the build process happened in the wrong\norder.\nTo fix the problem, you need to either satisfy the missing dependency\nin the Makefile or whatever script produced the Makefile, or (as a\nworkaround) set :term:`PARALLEL_MAKE` to an empty string::\nPARALLEL_MAKE = \"\"\nFor information on parallel Makefile issues, see the\n\":ref:`dev-manual/debugging:debugging parallel make races`\" section.\n-  *Improper host path usage:* This failure applies to recipes building\nfor the target or \":ref:`ref-classes-nativesdk`\" only. The\nfailure occurs when the compilation process uses improper headers,\nlibraries, or other files from the host system when cross-compiling for\nthe target.\nTo fix the problem, examine the ``log.do_compile`` file to identify\nthe host paths being used (e.g. ``/usr/include``, ``/usr/lib``, and\nso forth) and then either add configure options, apply a patch, or do\nboth.\n-  *Failure to find required libraries/headers:* If a build-time\ndependency is missing because it has not been declared in\n:term:`DEPENDS`, or because the\ndependency exists but the path used by the build process to find the\nfile is incorrect and the configure step did not detect it, the\ncompilation process could fail. For either of these failures, the\ncompilation process notes that files could not be found. In these\ncases, you need to go back and add additional options to the\nconfigure script as well as possibly add additional build-time\ndependencies to :term:`DEPENDS`.\nOccasionally, it is necessary to apply a patch to the source to\nensure the correct paths are used. If you need to specify paths to\nfind files staged into the sysroot from other recipes, use the\nvariables that the OpenEmbedded build system provides (e.g.\n:term:`STAGING_BINDIR`, :term:`STAGING_INCDIR`, :term:`STAGING_DATADIR`, and so\nforth).\nInstalling\n==========\nDuring :ref:`ref-tasks-install`, the task copies the built files along with their\nhierarchy to locations that would mirror their locations on the target\ndevice. The installation process copies files from the\n``${``\\ :term:`S`\\ ``}``,\n``${``\\ :term:`B`\\ ``}``, and\n``${``\\ :term:`WORKDIR`\\ ``}``\ndirectories to the ``${``\\ :term:`D`\\ ``}``\ndirectory to create the structure as it should appear on the target\nsystem.\nHow your software is built affects what you must do to be sure your\nsoftware is installed correctly. The following list describes what you\nmust do for installation depending on the type of build system used by\nthe software being built:\n-  *Autotools and CMake:* If the software your recipe is building uses\nAutotools or CMake, the OpenEmbedded build system understands how to\ninstall the software. Consequently, you do not have to have a\n:ref:`ref-tasks-install` task as part of your recipe. You just need to make\nsure the install portion of the build completes with no issues.\nHowever, if you wish to install additional files not already being\ninstalled by ``make install``, you should do this using a\n``do_install:append`` function using the install command as described\nin the \"Manual\" bulleted item later in this list.\n-  *Other (using* ``make install``\\ *)*: You need to define a :ref:`ref-tasks-install`\nfunction in your recipe. The function should call\n``oe_runmake install`` and will likely need to pass in the\ndestination directory as well. How you pass that path is dependent on\nhow the ``Makefile`` being run is written (e.g. ``DESTDIR=${D}``,\n``PREFIX=${D}``, ``INSTALLROOT=${D}``, and so forth).\nFor an example recipe using ``make install``, see the\n\":ref:`dev-manual/new-recipe:building a makefile-based package`\" section.\n-  *Manual:* You need to define a :ref:`ref-tasks-install` function in your\nrecipe. The function must first use ``install -d`` to create the\ndirectories under\n``${``\\ :term:`D`\\ ``}``. Once the\ndirectories exist, your function can use ``install`` to manually\ninstall the built software into the directories.\nYou can find more information on ``install`` at\nhttps://www.gnu.org/software/coreutils/manual/html_node/install-invocation.html.\nFor the scenarios that do not use Autotools or CMake, you need to track\nthe installation and diagnose and fix any issues until everything\ninstalls correctly. You need to look in the default location of\n``${D}``, which is ``${WORKDIR}/image``, to be sure your files have been\ninstalled correctly.\n.. note::\n-  During the installation process, you might need to modify some of\nthe installed files to suit the target layout. For example, you\nmight need to replace hard-coded paths in an initscript with\nvalues of variables provided by the build system, such as\nreplacing ``/usr/bin/`` with ``${bindir}``. If you do perform such\nmodifications during :ref:`ref-tasks-install`, be sure to modify the"}
{"text": "\ndestination file after copying rather than before copying.\nModifying after copying ensures that the build system can\nre-execute :ref:`ref-tasks-install` if needed.\n-  ``oe_runmake install``, which can be run directly or can be run\nindirectly by the :ref:`ref-classes-autotools` and\n:ref:`ref-classes-cmake` classes, runs ``make install`` in parallel.\nSometimes, a Makefile can have missing dependencies between targets that\ncan result in race conditions. If you experience intermittent failures\nduring :ref:`ref-tasks-install`, you might be able to work around them by\ndisabling parallel Makefile installs by adding the following to the\nrecipe::\nPARALLEL_MAKEINST = \"\"\nSee :term:`PARALLEL_MAKEINST` for additional information.\n-  If you need to install one or more custom CMake toolchain files\nthat are supplied by the application you are building, install the\nfiles to ``${D}${datadir}/cmake/Modules`` during\n:ref:`ref-tasks-install`.\nEnabling System Services\n========================\nIf you want to install a service, which is a process that usually starts\non boot and runs in the background, then you must include some\nadditional definitions in your recipe.\nIf you are adding services and the service initialization script or the\nservice file itself is not installed, you must provide for that\ninstallation in your recipe using a ``do_install:append`` function. If\nyour recipe already has a :ref:`ref-tasks-install` function, update the function\nnear its end rather than adding an additional ``do_install:append``\nfunction.\nWhen you create the installation for your services, you need to\naccomplish what is normally done by ``make install``. In other words,\nmake sure your installation arranges the output similar to how it is\narranged on the target system.\nThe OpenEmbedded build system provides support for starting services two\ndifferent ways:\n-  *SysVinit:* SysVinit is a system and service manager that manages the\ninit system used to control the very basic functions of your system.\nThe init program is the first program started by the Linux kernel\nwhen the system boots. Init then controls the startup, running and\nshutdown of all other programs.\nTo enable a service using SysVinit, your recipe needs to inherit the\n:ref:`ref-classes-update-rc.d` class. The class helps\nfacilitate safely installing the package on the target.\nYou will need to set the\n:term:`INITSCRIPT_PACKAGES`,\n:term:`INITSCRIPT_NAME`,\nand\n:term:`INITSCRIPT_PARAMS`\nvariables within your recipe.\n-  *systemd:* System Management Daemon (systemd) was designed to replace\nSysVinit and to provide enhanced management of services. For more\ninformation on systemd, see the systemd homepage at\nhttps://freedesktop.org/wiki/Software/systemd/.\nTo enable a service using systemd, your recipe needs to inherit the\n:ref:`ref-classes-systemd` class. See the ``systemd.bbclass`` file\nlocated in your :term:`Source Directory` section for more information.\nPackaging\n=========\nSuccessful packaging is a combination of automated processes performed\nby the OpenEmbedded build system and some specific steps you need to\ntake. The following list describes the process:\n-  *Splitting Files*: The :ref:`ref-tasks-package` task splits the files produced\nby the recipe into logical components. Even software that produces a\nsingle binary might still have debug symbols, documentation, and\nother logical components that should be split out. The :ref:`ref-tasks-package`\ntask ensures that files are split up and packaged correctly.\n-  *Running QA Checks*: The :ref:`ref-classes-insane` class adds a\nstep to the package generation process so that output quality\nassurance checks are generated by the OpenEmbedded build system. This\nstep performs a range of checks to be sure the build's output is free\nof common problems that show up during runtime. For information on\nthese checks, see the :ref:`ref-classes-insane` class and\nthe \":ref:`ref-manual/qa-checks:qa error and warning messages`\"\nchapter in the Yocto Project Reference Manual.\n-  *Hand-Checking Your Packages*: After you build your software, you\nneed to be sure your packages are correct. Examine the\n``${``\\ :term:`WORKDIR`\\ ``}/packages-split``\ndirectory and make sure files are where you expect them to be. If you\ndiscover problems, you can set\n:term:`PACKAGES`,\n:term:`FILES`,\n``do_install(:append)``, and so forth as needed.\n-  *Splitting an Application into Multiple Packages*: If you need to\nsplit an application into several packages, see the\n\":ref:`dev-manual/new-recipe:splitting an application into multiple packages`\"\nsection for an example.\n-  *Installing a Post-Installation Script*: For an example showing how\nto install a post-installation script, see the\n\":ref:`dev-manual/new-recipe:post-installation scripts`\" section.\n-  *Marking Package Architecture*: Depending on what your recipe is\nbuilding and how it is configured, it might be important to mark the\npackages produced as being specific to a particular machine, or to\nmark them as not being specific to a particular machine or\narchitecture at all.\nBy default, packages apply to any machine with the same architecture\nas the target machine. When a recipe produces packages that are\nmachine-specific (e.g. the\n:term:`MACHINE` value is passed\ninto the configure script or a patch is applied only for a particular\nmachine), you should mark them as such by adding the following to the\nrecipe::"}
{"text": "\nPACKAGE_ARCH = \"${MACHINE_ARCH}\"\nOn the other hand, if the recipe produces packages that do not\ncontain anything specific to the target machine or architecture at\nall (e.g. recipes that simply package script files or configuration\nfiles), you should use the :ref:`ref-classes-allarch` class to\ndo this for you by adding this to your recipe::\ninherit allarch\nEnsuring that the package architecture is correct is not critical\nwhile you are doing the first few builds of your recipe. However, it\nis important in order to ensure that your recipe rebuilds (or does\nnot rebuild) appropriately in response to changes in configuration,\nand to ensure that you get the appropriate packages installed on the\ntarget machine, particularly if you run separate builds for more than\none target machine.\nSharing Files Between Recipes\n=============================\nRecipes often need to use files provided by other recipes on the build\nhost. For example, an application linking to a common library needs\naccess to the library itself and its associated headers. The way this\naccess is accomplished is by populating a sysroot with files. Each\nrecipe has two sysroots in its work directory, one for target files\n(``recipe-sysroot``) and one for files that are native to the build host\n(``recipe-sysroot-native``).\n.. note::\nYou could find the term \"staging\" used within the Yocto project\nregarding files populating sysroots (e.g. the :term:`STAGING_DIR`\nvariable).\nRecipes should never populate the sysroot directly (i.e. write files\ninto sysroot). Instead, files should be installed into standard\nlocations during the\n:ref:`ref-tasks-install` task within\nthe ``${``\\ :term:`D`\\ ``}`` directory. The\nreason for this limitation is that almost all files that populate the\nsysroot are cataloged in manifests in order to ensure the files can be\nremoved later when a recipe is either modified or removed. Thus, the\nsysroot is able to remain free from stale files.\nA subset of the files installed by the :ref:`ref-tasks-install` task are\nused by the :ref:`ref-tasks-populate_sysroot` task as defined by the\n:term:`SYSROOT_DIRS` variable to automatically populate the sysroot. It\nis possible to modify the list of directories that populate the sysroot.\nThe following example shows how you could add the ``/opt`` directory to\nthe list of directories within a recipe::\nSYSROOT_DIRS += \"/opt\"\n.. note::\nThe `/sysroot-only` is to be used by recipes that generate artifacts\nthat are not included in the target filesystem, allowing them to share\nthese artifacts without needing to use the :term:`DEPLOY_DIR`.\nFor a more complete description of the :ref:`ref-tasks-populate_sysroot`\ntask and its associated functions, see the\n:ref:`staging <ref-classes-staging>` class.\nUsing Virtual Providers\n=======================\nPrior to a build, if you know that several different recipes provide the\nsame functionality, you can use a virtual provider (i.e. ``virtual/*``)\nas a placeholder for the actual provider. The actual provider is\ndetermined at build-time.\nA common scenario where a virtual provider is used would be for the kernel\nrecipe. Suppose you have three kernel recipes whose :term:`PN` values map to\n``kernel-big``, ``kernel-mid``, and ``kernel-small``. Furthermore, each of\nthese recipes in some way uses a :term:`PROVIDES` statement that essentially\nidentifies itself as being able to provide ``virtual/kernel``. Here is one way\nthrough the :ref:`ref-classes-kernel` class::\nPROVIDES += \"virtual/kernel\"\nAny recipe that inherits the :ref:`ref-classes-kernel` class is\ngoing to utilize a :term:`PROVIDES` statement that identifies that recipe as\nbeing able to provide the ``virtual/kernel`` item.\nNow comes the time to actually build an image and you need a kernel\nrecipe, but which one? You can configure your build to call out the\nkernel recipe you want by using the :term:`PREFERRED_PROVIDER` variable. As\nan example, consider the :yocto_git:`x86-base.inc\n</poky/tree/meta/conf/machine/include/x86/x86-base.inc>` include file, which is a\nmachine (i.e. :term:`MACHINE`) configuration file. This include file is the\nreason all x86-based machines use the ``linux-yocto`` kernel. Here are the\nrelevant lines from the include file::\nPREFERRED_PROVIDER_virtual/kernel ??= \"linux-yocto\"\nPREFERRED_VERSION_linux-yocto ??= \"4.15%\"\nWhen you use a virtual provider, you do not have to \"hard code\" a recipe\nname as a build dependency. You can use the\n:term:`DEPENDS` variable to state the\nbuild is dependent on ``virtual/kernel`` for example::\nDEPENDS = \"virtual/kernel\"\nDuring the build, the OpenEmbedded build system picks\nthe correct recipe needed for the ``virtual/kernel`` dependency based on\nthe :term:`PREFERRED_PROVIDER` variable. If you want to use the small kernel\nmentioned at the beginning of this section, configure your build as\nfollows::\nPREFERRED_PROVIDER_virtual/kernel ??= \"kernel-small\"\n.. note::\nAny recipe that :term:`PROVIDES` a ``virtual/*`` item that is ultimately not\nselected through :term:`PREFERRED_PROVIDER` does not get built. Preventing these\nrecipes from building is usually the desired behavior since this mechanism's\npurpose is to select between mutually exclusive alternative providers.\nThe following lists specific examples of virtual providers:\n-  ``virtual/kernel``: Provides the name of the kernel recipe to use\nwhen building a kernel image.\n-  ``virtual/bootloader``: Provides the name of the bootloader to use\nwhen building an image.\n-  ``virtual/libgbm``: Provides ``gbm.pc``.\n-  ``virtual/egl``: Provides ``egl.pc`` and possibly ``wayland-egl.pc``.\n-  ``virtual/libgl``: Provides ``gl.pc`` (i.e. libGL)."}
{"text": "\n-  ``virtual/libgles1``: Provides ``glesv1_cm.pc`` (i.e. libGLESv1_CM).\n-  ``virtual/libgles2``: Provides ``glesv2.pc`` (i.e. libGLESv2).\n.. note::\nVirtual providers only apply to build time dependencies specified with\n:term:`PROVIDES` and :term:`DEPENDS`. They do not apply to runtime\ndependencies specified with :term:`RPROVIDES` and :term:`RDEPENDS`.\nProperly Versioning Pre-Release Recipes\n=======================================\nSometimes the name of a recipe can lead to versioning problems when the\nrecipe is upgraded to a final release. For example, consider the\n``irssi_0.8.16-rc1.bb`` recipe file in the list of example recipes in\nthe \":ref:`dev-manual/new-recipe:storing and naming the recipe`\" section.\nThis recipe is at a release candidate stage (i.e. \"rc1\"). When the recipe is\nreleased, the recipe filename becomes ``irssi_0.8.16.bb``. The version\nchange from ``0.8.16-rc1`` to ``0.8.16`` is seen as a decrease by the\nbuild system and package managers, so the resulting packages will not\ncorrectly trigger an upgrade.\nIn order to ensure the versions compare properly, the recommended\nconvention is to use a tilde (``~``) character as follows::\nPV = 0.8.16~rc1\nThis way ``0.8.16~rc1`` sorts before ``0.8.16``. See the\n\":ref:`contributor-guide/recipe-style-guide:version policy`\" section in the\nYocto Project and OpenEmbedded Contributor Guide for more details about\nversioning code corresponding to a pre-release or to a specific Git commit.\nPost-Installation Scripts\n=========================\nPost-installation scripts run immediately after installing a package on\nthe target or during image creation when a package is included in an\nimage. To add a post-installation script to a package, add a\n``pkg_postinst:``\\ `PACKAGENAME`\\ ``()`` function to the recipe file\n(``.bb``) and replace `PACKAGENAME` with the name of the package you want\nto attach to the ``postinst`` script. To apply the post-installation\nscript to the main package for the recipe, which is usually what is\nrequired, specify\n``${``\\ :term:`PN`\\ ``}`` in place of\nPACKAGENAME.\nA post-installation function has the following structure::\npkg_postinst:PACKAGENAME() {\n# Commands to carry out\n}\nThe script defined in the post-installation function is called when the\nroot filesystem is created. If the script succeeds, the package is\nmarked as installed.\n.. note::\nAny RPM post-installation script that runs on the target should\nreturn a 0 exit code. RPM does not allow non-zero exit codes for\nthese scripts, and the RPM package manager will cause the package to\nfail installation on the target.\nSometimes it is necessary for the execution of a post-installation\nscript to be delayed until the first boot. For example, the script might\nneed to be executed on the device itself. To delay script execution\nuntil boot time, you must explicitly mark post installs to defer to the\ntarget. You can use ``pkg_postinst_ontarget()`` or call\n``postinst_intercept delay_to_first_boot`` from ``pkg_postinst()``. Any\nfailure of a ``pkg_postinst()`` script (including exit 1) triggers an\nerror during the\n:ref:`ref-tasks-rootfs` task.\nIf you have recipes that use ``pkg_postinst`` function and they require\nthe use of non-standard native tools that have dependencies during\nroot filesystem construction, you need to use the\n:term:`PACKAGE_WRITE_DEPS`\nvariable in your recipe to list these tools. If you do not use this\nvariable, the tools might be missing and execution of the\npost-installation script is deferred until first boot. Deferring the\nscript to the first boot is undesirable and impossible for read-only\nroot filesystems.\n.. note::\nThere is equivalent support for pre-install, pre-uninstall, and post-uninstall\nscripts by way of ``pkg_preinst``, ``pkg_prerm``, and ``pkg_postrm``,\nrespectively. These scrips work in exactly the same way as does\n``pkg_postinst`` with the exception that they run at different times. Also,\nbecause of when they run, they are not applicable to being run at image\ncreation time like ``pkg_postinst``.\nTesting\n=======\nThe final step for completing your recipe is to be sure that the\nsoftware you built runs correctly. To accomplish runtime testing, add\nthe build's output packages to your image and test them on the target.\nFor information on how to customize your image by adding specific\npackages, see \":ref:`dev-manual/customizing-images:customizing images`\" section.\nExamples\n========\nTo help summarize how to write a recipe, this section provides some\nrecipe examples given various scenarios:\n-  `Building a single .c file package`_\n-  `Building a Makefile-based package`_\n-  `Building an Autotooled package`_\n-  `Building a Meson package`_\n-  `Splitting an application into multiple packages`_\n-  `Packaging externally produced binaries`_\nBuilding a Single .c File Package\n---------------------------------\nBuilding an application from a single file that is stored locally (e.g. under\n``files``) requires a recipe that has the file listed in the :term:`SRC_URI`\nvariable. Additionally, you need to manually write the :ref:`ref-tasks-compile`\nand :ref:`ref-tasks-install` tasks. The :term:`S` variable defines the\ndirectory containing the source code, which is set to :term:`WORKDIR` in this\ncase --- the directory BitBake uses for the build::\nSUMMARY = \"Simple helloworld application\"\nSECTION = \"examples\""}
{"text": "\nLICENSE = \"MIT\"\nLIC_FILES_CHKSUM = \"file://${COMMON_LICENSE_DIR}/MIT;md5=0835ade698e0bcf8506ecda2f7b4f302\"\nSRC_URI = \"file://helloworld.c\"\nS = \"${WORKDIR}\"\ndo_compile() {\n${CC} ${LDFLAGS} helloworld.c -o helloworld\n}\ndo_install() {\ninstall -d ${D}${bindir}\ninstall -m 0755 helloworld ${D}${bindir}\n}\nBy default, the ``helloworld``, ``helloworld-dbg``, and ``helloworld-dev`` packages\nare built. For information on how to customize the packaging process, see the\n\":ref:`dev-manual/new-recipe:splitting an application into multiple packages`\"\nsection.\nBuilding a Makefile-Based Package\n---------------------------------\nApplications built with GNU ``make`` require a recipe that has the source archive\nlisted in :term:`SRC_URI`. You do not need to add a :ref:`ref-tasks-compile`\nstep since by default BitBake starts the ``make`` command to compile the\napplication. If you need additional ``make`` options, you should store them in\nthe :term:`EXTRA_OEMAKE` or :term:`PACKAGECONFIG_CONFARGS` variables. BitBake\npasses these options into the GNU ``make`` invocation. Note that a\n:ref:`ref-tasks-install` task is still required. Otherwise, BitBake runs an\nempty :ref:`ref-tasks-install` task by default.\nSome applications might require extra parameters to be passed to the\ncompiler. For example, the application might need an additional header\npath. You can accomplish this by adding to the :term:`CFLAGS` variable. The\nfollowing example shows this::\nCFLAGS:prepend = \"-I ${S}/include \"\nIn the following example, ``lz4`` is a makefile-based package::\nSUMMARY = \"Extremely Fast Compression algorithm\"\nDESCRIPTION = \"LZ4 is a very fast lossless compression algorithm, providing compression speed at 400 MB/s per core, scalable with multi-cores CPU. It also features an extremely fast decoder, with speed in multiple GB/s per core, typically reaching RAM speed limits on multi-core systems.\"\nHOMEPAGE = \"https://github.com/lz4/lz4\"\nLICENSE = \"BSD-2-Clause | GPL-2.0-only\"\nLIC_FILES_CHKSUM = \"file://lib/LICENSE;md5=ebc2ea4814a64de7708f1571904b32cc \\\nfile://programs/COPYING;md5=b234ee4d69f5fce4486a80fdaf4a4263 \\\nfile://LICENSE;md5=d57c0d21cb917fb4e0af2454aa48b956 \\\n\"\nPE = \"1\"\nSRCREV = \"d44371841a2f1728a3f36839fd4b7e872d0927d3\"\nSRC_URI = \"git://github.com/lz4/lz4.git;branch=release;protocol=https \\\nfile://CVE-2021-3520.patch \\\n\"\nUPSTREAM_CHECK_GITTAGREGEX = \"v(?P<pver>.*)\"\nS = \"${WORKDIR}/git\"\nCVE_STATUS[CVE-2014-4715] = \"fixed-version: Fixed in r118, which is larger than the current version\"\nEXTRA_OEMAKE = \"PREFIX=${prefix} CC='${CC}' CFLAGS='${CFLAGS}' DESTDIR=${D} LIBDIR=${libdir} INCLUDEDIR=${includedir} BUILD_STATIC=no\"\ndo_install() {\noe_runmake install\n}\nBBCLASSEXTEND = \"native nativesdk\"\nBuilding an Autotooled Package\n------------------------------\nApplications built with the Autotools such as ``autoconf`` and ``automake``\nrequire a recipe that has a source archive listed in :term:`SRC_URI` and also\ninherit the :ref:`ref-classes-autotools` class, which contains the definitions\nof all the steps needed to build an Autotool-based application. The result of\nthe build is automatically packaged. And, if the application uses NLS for\nlocalization, packages with local information are generated (one package per\nlanguage). Following is one example: (``hello_2.3.bb``)::\nSUMMARY = \"GNU Helloworld application\"\nSECTION = \"examples\"\nLICENSE = \"GPL-2.0-or-later\"\nLIC_FILES_CHKSUM = \"file://COPYING;md5=751419260aa954499f7abaabaa882bbe\"\nSRC_URI = \"${GNU_MIRROR}/hello/hello-${PV}.tar.gz\"\ninherit autotools gettext\nThe variable :term:`LIC_FILES_CHKSUM` is used to track source license changes\nas described in the \":ref:`dev-manual/licenses:tracking license changes`\"\nsection in the Yocto Project Overview and Concepts Manual. You can quickly\ncreate Autotool-based recipes in a manner similar to the previous example.\n.. _ref-building-meson-package:\nBuilding a Meson Package\n------------------------\nApplications built with the `Meson build system <https://mesonbuild.com/>`__\njust need a recipe that has sources described in :term:`SRC_URI` and inherits\nthe :ref:`ref-classes-meson` class.\nThe :oe_git:`ipcalc recipe </meta-openembedded/tree/meta-networking/recipes-support/ipcalc>`\nis a simple example of an application without dependencies::\nSUMMARY = \"Tool to assist in network address calculations for IPv4 and IPv6.\"\nHOMEPAGE = \"https://gitlab.com/ipcalc/ipcalc\"\nSECTION = \"net\"\nLICENSE = \"GPL-2.0-only\"\nLIC_FILES_CHKSUM = \"file://COPYING;md5=b234ee4d69f5fce4486a80fdaf4a4263\"\nSRC_URI = \"git://gitlab.com/ipcalc/ipcalc.git;protocol=https;branch=master\"\nSRCREV = \"4c4261a47f355946ee74013d4f5d0494487cc2d6\"\nS = \"${WORKDIR}/git\"\ninherit meson\nApplications with dependencies are likely to inherit the\n:ref:`ref-classes-pkgconfig` class, as ``pkg-config`` is the default method\nused by Meson to find dependencies and compile applications against them.\nSplitting an Application into Multiple Packages\n-----------------------------------------------\nYou can use the variables :term:`PACKAGES` and :term:`FILES` to split an\napplication into multiple packages.\nFollowing is an example that uses the ``libxpm`` recipe. By default,\nthis recipe generates a single package that contains the library along\nwith a few binaries. You can modify the recipe to split the binaries\ninto separate packages::\nrequire xorg-lib-common.inc"}
{"text": "\nSUMMARY = \"Xpm: X Pixmap extension library\"\nLICENSE = \"MIT\"\nLIC_FILES_CHKSUM = \"file://COPYING;md5=51f4270b012ecd4ab1a164f5f4ed6cf7\"\nDEPENDS += \"libxext libsm libxt\"\nPE = \"1\"\nXORG_PN = \"libXpm\"\nPACKAGES =+ \"sxpm cxpm\"\nFILES:cxpm = \"${bindir}/cxpm\"\nFILES:sxpm = \"${bindir}/sxpm\"\nIn the previous example, we want to ship the ``sxpm`` and ``cxpm``\nbinaries in separate packages. Since ``bindir`` would be packaged into\nthe main :term:`PN` package by default, we prepend the :term:`PACKAGES` variable\nso additional package names are added to the start of list. This results\nin the extra ``FILES:*`` variables then containing information that\ndefine which files and directories go into which packages. Files\nincluded by earlier packages are skipped by latter packages. Thus, the\nmain :term:`PN` package does not include the above listed files.\nPackaging Externally Produced Binaries\n--------------------------------------\nSometimes, you need to add pre-compiled binaries to an image. For\nexample, suppose that there are binaries for proprietary code,\ncreated by a particular division of a company. Your part of the company\nneeds to use those binaries as part of an image that you are building\nusing the OpenEmbedded build system. Since you only have the binaries\nand not the source code, you cannot use a typical recipe that expects to\nfetch the source specified in\n:term:`SRC_URI` and then compile it.\nOne method is to package the binaries and then install them as part of\nthe image. Generally, it is not a good idea to package binaries since,\namong other things, it can hinder the ability to reproduce builds and\ncould lead to compatibility problems with ABI in the future. However,\nsometimes you have no choice.\nThe easiest solution is to create a recipe that uses the\n:ref:`ref-classes-bin-package` class and to be sure that you are using default\nlocations for build artifacts.  In most cases, the\n:ref:`ref-classes-bin-package` class handles \"skipping\" the configure and\ncompile steps as well as sets things up to grab packages from the appropriate\narea. In particular, this class sets ``noexec`` on both the\n:ref:`ref-tasks-configure` and :ref:`ref-tasks-compile` tasks, sets\n``FILES:${PN}`` to \"/\" so that it picks up all files, and sets up a\n:ref:`ref-tasks-install` task, which effectively copies all files from ``${S}``\nto ``${D}``. The :ref:`ref-classes-bin-package` class works well when the files\nextracted into ``${S}`` are already laid out in the way they should be laid out\non the target. For more information on these variables, see the :term:`FILES`,\n:term:`PN`, :term:`S`, and :term:`D` variables in the Yocto Project Reference\nManual's variable glossary.\n.. note::\n-  Using :term:`DEPENDS` is a good\nidea even for components distributed in binary form, and is often\nnecessary for shared libraries. For a shared library, listing the\nlibrary dependencies in :term:`DEPENDS` makes sure that the libraries\nare available in the staging sysroot when other recipes link\nagainst the library, which might be necessary for successful\nlinking.\n-  Using :term:`DEPENDS` also allows runtime dependencies between\npackages to be added automatically. See the\n\":ref:`overview-manual/concepts:automatically added runtime dependencies`\"\nsection in the Yocto Project Overview and Concepts Manual for more\ninformation.\nIf you cannot use the :ref:`ref-classes-bin-package` class, you need to be sure you are\ndoing the following:\n-  Create a recipe where the\n:ref:`ref-tasks-configure` and\n:ref:`ref-tasks-compile` tasks do\nnothing: It is usually sufficient to just not define these tasks in\nthe recipe, because the default implementations do nothing unless a\nMakefile is found in\n``${``\\ :term:`S`\\ ``}``.\nIf ``${S}`` might contain a Makefile, or if you inherit some class\nthat replaces :ref:`ref-tasks-configure` and :ref:`ref-tasks-compile` with custom\nversions, then you can use the\n``[``\\ :ref:`noexec <bitbake-user-manual/bitbake-user-manual-metadata:variable flags>`\\ ``]``\nflag to turn the tasks into no-ops, as follows::\ndo_configure[noexec] = \"1\"\ndo_compile[noexec] = \"1\"\nUnlike :ref:`bitbake-user-manual/bitbake-user-manual-metadata:deleting a task`,\nusing the flag preserves the dependency chain from the :ref:`ref-tasks-fetch`,\n:ref:`ref-tasks-unpack`, and :ref:`ref-tasks-patch` tasks to the\n:ref:`ref-tasks-install` task.\n-  Make sure your :ref:`ref-tasks-install` task installs the binaries\nappropriately.\n-  Ensure that you set up :term:`FILES`\n(usually\n``FILES:${``\\ :term:`PN`\\ ``}``) to\npoint to the files you have installed, which of course depends on\nwhere you have installed them and whether those files are in\ndifferent locations than the defaults.\nFollowing Recipe Style Guidelines\n=================================\nWhen writing recipes, it is good to conform to existing style guidelines.\nSee the \":doc:`../contributor-guide/recipe-style-guide`\" in the Yocto Project\nand OpenEmbedded Contributor Guide for reference.\nIt is common for existing recipes to deviate a bit from this style.\nHowever, aiming for at least a consistent style is a good idea. Some\npractices, such as omitting spaces around ``=`` operators in assignments\nor ordering recipe components in an erratic way, are widely seen as poor\nstyle.\nRecipe Syntax\n=============\nUnderstanding recipe file syntax is important for writing recipes. The"}
{"text": "\nfollowing list overviews the basic items that make up a BitBake recipe\nfile. For more complete BitBake syntax descriptions, see the\n\":doc:`bitbake:bitbake-user-manual/bitbake-user-manual-metadata`\"\nchapter of the BitBake User Manual.\n-  *Variable Assignments and Manipulations:* Variable assignments allow\na value to be assigned to a variable. The assignment can be static\ntext or might include the contents of other variables. In addition to\nthe assignment, appending and prepending operations are also\nsupported.\nThe following example shows some of the ways you can use variables in\nrecipes::\nS = \"${WORKDIR}/postfix-${PV}\"\nCFLAGS += \"-DNO_ASM\"\nCFLAGS:append = \" --enable-important-feature\"\n-  *Functions:* Functions provide a series of actions to be performed.\nYou usually use functions to override the default implementation of a\ntask function or to complement a default function (i.e. append or\nprepend to an existing function). Standard functions use ``sh`` shell\nsyntax, although access to OpenEmbedded variables and internal\nmethods are also available.\nHere is an example function from the ``sed`` recipe::\ndo_install () {\nautotools_do_install\ninstall -d ${D}${base_bindir}\nmv ${D}${bindir}/sed ${D}${base_bindir}/sed\nrmdir ${D}${bindir}/\n}\nIt is\nalso possible to implement new functions that are called between\nexisting tasks as long as the new functions are not replacing or\ncomplementing the default functions. You can implement functions in\nPython instead of shell. Both of these options are not seen in the\nmajority of recipes.\n-  *Keywords:* BitBake recipes use only a few keywords. You use keywords\nto include common functions (``inherit``), load parts of a recipe\nfrom other files (``include`` and ``require``) and export variables\nto the environment (``export``).\nThe following example shows the use of some of these keywords::\nexport POSTCONF = \"${STAGING_BINDIR}/postconf\"\ninherit autoconf\nrequire otherfile.inc\n-  *Comments (#):* Any lines that begin with the hash character (``#``)\nare treated as comment lines and are ignored::\n# This is a comment\nThis next list summarizes the most important and most commonly used\nparts of the recipe syntax. For more information on these parts of the\nsyntax, you can reference the\n\":doc:`bitbake:bitbake-user-manual/bitbake-user-manual-metadata`\" chapter\nin the BitBake User Manual.\n-  *Line Continuation (\\\\):* Use the backward slash (``\\``) character to\nsplit a statement over multiple lines. Place the slash character at\nthe end of the line that is to be continued on the next line::\nVAR = \"A really long \\\nline\"\n.. note::\nYou cannot have any characters including spaces or tabs after the\nslash character.\n-  *Using Variables (${VARNAME}):* Use the ``${VARNAME}`` syntax to\naccess the contents of a variable::\nSRC_URI = \"${SOURCEFORGE_MIRROR}/libpng/zlib-${PV}.tar.gz\"\n.. note::\nIt is important to understand that the value of a variable\nexpressed in this form does not get substituted automatically. The\nexpansion of these expressions happens on-demand later (e.g.\nusually when a function that makes reference to the variable\nexecutes). This behavior ensures that the values are most\nappropriate for the context in which they are finally used. On the\nrare occasion that you do need the variable expression to be\nexpanded immediately, you can use the\n:=\noperator instead of\n=\nwhen you make the assignment, but this is not generally needed.\n-  *Quote All Assignments (\"value\"):* Use double quotes around values in\nall variable assignments (e.g. ``\"value\"``). Following is an example::\nVAR1 = \"${OTHERVAR}\"\nVAR2 = \"The version is ${PV}\"\n-  *Conditional Assignment (?=):* Conditional assignment is used to\nassign a value to a variable, but only when the variable is currently\nunset. Use the question mark followed by the equal sign (``?=``) to\nmake a \"soft\" assignment used for conditional assignment. Typically,\n\"soft\" assignments are used in the ``local.conf`` file for variables\nthat are allowed to come through from the external environment.\nHere is an example where ``VAR1`` is set to \"New value\" if it is\ncurrently empty. However, if ``VAR1`` has already been set, it\nremains unchanged::\nVAR1 ?= \"New value\"\nIn this next example, ``VAR1`` is left with the value \"Original value\"::\nVAR1 = \"Original value\"\nVAR1 ?= \"New value\"\n-  *Appending (+=):* Use the plus character followed by the equals sign\n(``+=``) to append values to existing variables.\n.. note::\nThis operator adds a space between the existing content of the\nvariable and the new content.\nHere is an example::\nSRC_URI += \"file://fix-makefile.patch\"\n-  *Prepending (=+):* Use the equals sign followed by the plus character\n(``=+``) to prepend values to existing variables.\n.. note::"}
{"text": "\nThis operator adds a space between the new content and the\nexisting content of the variable.\nHere is an example::\nVAR =+ \"Starts\"\n-  *Appending (:append):* Use the ``:append`` operator to append values\nto existing variables. This operator does not add any additional\nspace. Also, the operator is applied after all the ``+=``, and ``=+``\noperators have been applied and after all ``=`` assignments have\noccurred. This means that if ``:append`` is used in a recipe, it can\nonly be overridden by another layer using the special ``:remove``\noperator, which in turn will prevent further layers from adding it back.\nThe following example shows the space being explicitly added to the\nstart to ensure the appended value is not merged with the existing\nvalue::\nCFLAGS:append = \" --enable-important-feature\"\nYou can also use\nthe ``:append`` operator with overrides, which results in the actions\nonly being performed for the specified target or machine::\nCFLAGS:append:sh4 = \" --enable-important-sh4-specific-feature\"\n-  *Prepending (:prepend):* Use the ``:prepend`` operator to prepend\nvalues to existing variables. This operator does not add any\nadditional space. Also, the operator is applied after all the ``+=``,\nand ``=+`` operators have been applied and after all ``=``\nassignments have occurred.\nThe following example shows the space being explicitly added to the\nend to ensure the prepended value is not merged with the existing\nvalue::\nCFLAGS:prepend = \"-I${S}/myincludes \"\nYou can also use the\n``:prepend`` operator with overrides, which results in the actions\nonly being performed for the specified target or machine::\nCFLAGS:prepend:sh4 = \"-I${S}/myincludes \"\n-  *Overrides:* You can use overrides to set a value conditionally,\ntypically based on how the recipe is being built. For example, to set\nthe :term:`KBRANCH` variable's\nvalue to \"standard/base\" for any target\n:term:`MACHINE`, except for\nqemuarm where it should be set to \"standard/arm-versatile-926ejs\",\nyou would do the following::\nKBRANCH = \"standard/base\"\nKBRANCH:qemuarm = \"standard/arm-versatile-926ejs\"\nOverrides are also used to separate\nalternate values of a variable in other situations. For example, when\nsetting variables such as\n:term:`FILES` and\n:term:`RDEPENDS` that are\nspecific to individual packages produced by a recipe, you should\nalways use an override that specifies the name of the package.\n-  *Indentation:* Use spaces for indentation rather than tabs. For\nshell functions, both currently work. However, it is a policy\ndecision of the Yocto Project to use tabs in shell functions. Realize\nthat some layers have a policy to use spaces for all indentation.\n-  *Using Python for Complex Operations:* For more advanced processing,\nit is possible to use Python code during variable assignments (e.g.\nsearch and replacement on a variable).\nYou indicate Python code using the ``${@python_code}`` syntax for the\nvariable assignment::\nSRC_URI = \"ftp://ftp.info-zip.org/pub/infozip/src/zip${@d.getVar('PV',1).replace('.', '')}.tgz\n-  *Shell Function Syntax:* Write shell functions as if you were writing\na shell script when you describe a list of actions to take. You\nshould ensure that your script works with a generic ``sh`` and that\nit does not require any ``bash`` or other shell-specific\nfunctionality. The same considerations apply to various system\nutilities (e.g. ``sed``, ``grep``, ``awk``, and so forth) that you\nmight wish to use. If in doubt, you should check with multiple\nimplementations --- including those from BusyBox."}
{"text": "\nUnderstanding and Creating Layers\n*********************************\nThe OpenEmbedded build system supports organizing\n:term:`Metadata` into multiple layers.\nLayers allow you to isolate different types of customizations from each\nother. For introductory information on the Yocto Project Layer Model,\nsee the\n\":ref:`overview-manual/yp-intro:the yocto project layer model`\"\nsection in the Yocto Project Overview and Concepts Manual.\nCreating Your Own Layer\n=======================\n.. note::\nIt is very easy to create your own layers to use with the OpenEmbedded\nbuild system, as the Yocto Project ships with tools that speed up creating\nlayers. This section describes the steps you perform by hand to create\nlayers so that you can better understand them. For information about the\nlayer-creation tools, see the\n\":ref:`bsp-guide/bsp:creating a new bsp layer using the \\`\\`bitbake-layers\\`\\` script`\"\nsection in the Yocto Project Board Support Package (BSP) Developer's\nGuide and the \":ref:`dev-manual/layers:creating a general layer using the \\`\\`bitbake-layers\\`\\` script`\"\nsection further down in this manual.\nFollow these general steps to create your layer without using tools:\n#. *Check Existing Layers:* Before creating a new layer, you should be\nsure someone has not already created a layer containing the Metadata\nyou need. You can see the :oe_layerindex:`OpenEmbedded Metadata Index <>`\nfor a list of layers from the OpenEmbedded community that can be used in\nthe Yocto Project. You could find a layer that is identical or close\nto what you need.\n#. *Create a Directory:* Create the directory for your layer. When you\ncreate the layer, be sure to create the directory in an area not\nassociated with the Yocto Project :term:`Source Directory`\n(e.g. the cloned ``poky`` repository).\nWhile not strictly required, prepend the name of the directory with\nthe string \"meta-\". For example::\nmeta-mylayer\nmeta-GUI_xyz\nmeta-mymachine\nWith rare exceptions, a layer's name follows this form::\nmeta-root_name\nFollowing this layer naming convention can save\nyou trouble later when tools, components, or variables \"assume\" your\nlayer name begins with \"meta-\". A notable example is in configuration\nfiles as shown in the following step where layer names without the\n\"meta-\" string are appended to several variables used in the\nconfiguration.\n#. *Create a Layer Configuration File:* Inside your new layer folder,\nyou need to create a ``conf/layer.conf`` file. It is easiest to take\nan existing layer configuration file and copy that to your layer's\n``conf`` directory and then modify the file as needed.\nThe ``meta-yocto-bsp/conf/layer.conf`` file in the Yocto Project\n:yocto_git:`Source Repositories </poky/tree/meta-yocto-bsp/conf>`\ndemonstrates the required syntax. For your layer, you need to replace\n\"yoctobsp\" with a unique identifier for your layer (e.g. \"machinexyz\"\nfor a layer named \"meta-machinexyz\")::\n# We have a conf and classes directory, add to BBPATH\nBBPATH .= \":${LAYERDIR}\"\n# We have recipes-* directories, add to BBFILES\nBBFILES += \"${LAYERDIR}/recipes-*/*/*.bb \\\n${LAYERDIR}/recipes-*/*/*.bbappend\"\nBBFILE_COLLECTIONS += \"yoctobsp\"\nBBFILE_PATTERN_yoctobsp = \"^${LAYERDIR}/\"\nBBFILE_PRIORITY_yoctobsp = \"5\"\nLAYERVERSION_yoctobsp = \"4\"\nLAYERSERIES_COMPAT_yoctobsp = \"dunfell\"\nFollowing is an explanation of the layer configuration file:\n-  :term:`BBPATH`: Adds the layer's\nroot directory to BitBake's search path. Through the use of the\n:term:`BBPATH` variable, BitBake locates class files (``.bbclass``),\nconfiguration files, and files that are included with ``include``\nand ``require`` statements. For these cases, BitBake uses the\nfirst file that matches the name found in :term:`BBPATH`. This is\nsimilar to the way the ``PATH`` variable is used for binaries. It\nis recommended, therefore, that you use unique class and\nconfiguration filenames in your custom layer.\n-  :term:`BBFILES`: Defines the\nlocation for all recipes in the layer.\n-  :term:`BBFILE_COLLECTIONS`:\nEstablishes the current layer through a unique identifier that is\nused throughout the OpenEmbedded build system to refer to the\nlayer. In this example, the identifier \"yoctobsp\" is the\nrepresentation for the container layer named \"meta-yocto-bsp\".\n-  :term:`BBFILE_PATTERN`:\nExpands immediately during parsing to provide the directory of the\nlayer.\n-  :term:`BBFILE_PRIORITY`:\nEstablishes a priority to use for recipes in the layer when the\nOpenEmbedded build finds recipes of the same name in different\nlayers.\n-  :term:`LAYERVERSION`:\nEstablishes a version number for the layer. You can use this\nversion number to specify this exact version of the layer as a\ndependency when using the\n:term:`LAYERDEPENDS`\nvariable.\n-  :term:`LAYERDEPENDS`:\nLists all layers on which this layer depends (if any).\n-  :term:`LAYERSERIES_COMPAT`:\nLists the :yocto_wiki:`Yocto Project </Releases>`\nreleases for which the current version is compatible. This"}
{"text": "\nvariable is a good way to indicate if your particular layer is\ncurrent.\n.. note::\nA layer does not have to contain only recipes ``.bb`` or append files\n``.bbappend``. Generally, developers create layers using\n``bitbake-layers create-layer``.\nSee \":ref:`dev-manual/layers:creating a general layer using the \\`\\`bitbake-layers\\`\\` script`\",\nexplaining how the ``layer.conf`` file is created from a template located in\n``meta/lib/bblayers/templates/layer.conf``.\nIn fact, none of the variables set in ``layer.conf`` are mandatory,\nexcept when :term:`BBFILE_COLLECTIONS` is present. In this case\n:term:`LAYERSERIES_COMPAT` and :term:`BBFILE_PATTERN` have to be\ndefined too.\n#. *Add Content:* Depending on the type of layer, add the content. If\nthe layer adds support for a machine, add the machine configuration\nin a ``conf/machine/`` file within the layer. If the layer adds\ndistro policy, add the distro configuration in a ``conf/distro/``\nfile within the layer. If the layer introduces new recipes, put the\nrecipes you need in ``recipes-*`` subdirectories within the layer.\n.. note::\nFor an explanation of layer hierarchy that is compliant with the\nYocto Project, see the \":ref:`bsp-guide/bsp:example filesystem layout`\"\nsection in the Yocto Project Board Support Package (BSP) Developer's Guide.\n#. *Optionally Test for Compatibility:* If you want permission to use\nthe Yocto Project Compatibility logo with your layer or application\nthat uses your layer, perform the steps to apply for compatibility.\nSee the\n\":ref:`dev-manual/layers:making sure your layer is compatible with yocto project`\"\nsection for more information.\nFollowing Best Practices When Creating Layers\n=============================================\nTo create layers that are easier to maintain and that will not impact\nbuilds for other machines, you should consider the information in the\nfollowing list:\n-  *Avoid \"Overlaying\" Entire Recipes from Other Layers in Your\nConfiguration:* In other words, do not copy an entire recipe into\nyour layer and then modify it. Rather, use an append file\n(``.bbappend``) to override only those parts of the original recipe\nyou need to modify.\n-  *Avoid Duplicating Include Files:* Use append files (``.bbappend``)\nfor each recipe that uses an include file. Or, if you are introducing\na new recipe that requires the included file, use the path relative\nto the original layer directory to refer to the file. For example,\nuse ``require recipes-core/``\\ `package`\\ ``/``\\ `file`\\ ``.inc`` instead\nof ``require`` `file`\\ ``.inc``. If you're finding you have to overlay\nthe include file, it could indicate a deficiency in the include file\nin the layer to which it originally belongs. If this is the case, you\nshould try to address that deficiency instead of overlaying the\ninclude file. For example, you could address this by getting the\nmaintainer of the include file to add a variable or variables to make\nit easy to override the parts needing to be overridden.\n-  *Structure Your Layers:* Proper use of overrides within append files\nand placement of machine-specific files within your layer can ensure\nthat a build is not using the wrong Metadata and negatively impacting\na build for a different machine. Following are some examples:\n-  *Modify Variables to Support a Different Machine:* Suppose you\nhave a layer named ``meta-one`` that adds support for building\nmachine \"one\". To do so, you use an append file named\n``base-files.bbappend`` and create a dependency on \"foo\" by\naltering the :term:`DEPENDS`\nvariable::\nDEPENDS = \"foo\"\nThe dependency is created during any\nbuild that includes the layer ``meta-one``. However, you might not\nwant this dependency for all machines. For example, suppose you\nare building for machine \"two\" but your ``bblayers.conf`` file has\nthe ``meta-one`` layer included. During the build, the\n``base-files`` for machine \"two\" will also have the dependency on\n``foo``.\nTo make sure your changes apply only when building machine \"one\",\nuse a machine override with the :term:`DEPENDS` statement::\nDEPENDS:one = \"foo\"\nYou should follow the same strategy when using ``:append``\nand ``:prepend`` operations::\nDEPENDS:append:one = \" foo\"\nDEPENDS:prepend:one = \"foo \"\nAs an actual example, here's a\nsnippet from the generic kernel include file ``linux-yocto.inc``,\nwherein the kernel compile and link options are adjusted in the\ncase of a subset of the supported architectures::\nDEPENDS:append:aarch64 = \" libgcc\"\nKERNEL_CC:append:aarch64 = \" ${TOOLCHAIN_OPTIONS}\"\nKERNEL_LD:append:aarch64 = \" ${TOOLCHAIN_OPTIONS}\"\nDEPENDS:append:nios2 = \" libgcc\"\nKERNEL_CC:append:nios2 = \" ${TOOLCHAIN_OPTIONS}\"\nKERNEL_LD:append:nios2 = \" ${TOOLCHAIN_OPTIONS}\"\nDEPENDS:append:arc = \" libgcc\"\nKERNEL_CC:append:arc = \" ${TOOLCHAIN_OPTIONS}\"\nKERNEL_LD:append:arc = \" ${TOOLCHAIN_OPTIONS}\"\nKERNEL_FEATURES:append:qemuall=\" features/debug/printk.scc\"\n-  *Place Machine-Specific Files in Machine-Specific Locations:* When\nyou have a base recipe, such as ``base-files.bb``, that contains a\n:term:`SRC_URI` statement to a\nfile, you can use an append file to cause the build to use your\nown version of the file. For example, an append file in your layer\nat ``meta-one/recipes-core/base-files/base-files.bbappend`` could\nextend :term:`FILESPATH` using :term:`FILESEXTRAPATHS` as follows::\nFILESEXTRAPATHS:prepend := \"${THISDIR}/${BPN}:\"\nThe build for machine \"one\" will pick up your machine-specific file as\nlong as you have the file in"}
{"text": "\n``meta-one/recipes-core/base-files/base-files/``. However, if you\nare building for a different machine and the ``bblayers.conf``\nfile includes the ``meta-one`` layer and the location of your\nmachine-specific file is the first location where that file is\nfound according to :term:`FILESPATH`, builds for all machines will\nalso use that machine-specific file.\nYou can make sure that a machine-specific file is used for a\nparticular machine by putting the file in a subdirectory specific\nto the machine. For example, rather than placing the file in\n``meta-one/recipes-core/base-files/base-files/`` as shown above,\nput it in ``meta-one/recipes-core/base-files/base-files/one/``.\nNot only does this make sure the file is used only when building\nfor machine \"one\", but the build process locates the file more\nquickly.\nIn summary, you need to place all files referenced from\n:term:`SRC_URI` in a machine-specific subdirectory within the layer in\norder to restrict those files to machine-specific builds.\n-  *Perform Steps to Apply for Yocto Project Compatibility:* If you want\npermission to use the Yocto Project Compatibility logo with your\nlayer or application that uses your layer, perform the steps to apply\nfor compatibility. See the\n\":ref:`dev-manual/layers:making sure your layer is compatible with yocto project`\"\nsection for more information.\n-  *Follow the Layer Naming Convention:* Store custom layers in a Git\nrepository that use the ``meta-layer_name`` format.\n-  *Group Your Layers Locally:* Clone your repository alongside other\ncloned ``meta`` directories from the :term:`Source Directory`.\nMaking Sure Your Layer is Compatible With Yocto Project\n=======================================================\nWhen you create a layer used with the Yocto Project, it is advantageous\nto make sure that the layer interacts well with existing Yocto Project\nlayers (i.e. the layer is compatible with the Yocto Project). Ensuring\ncompatibility makes the layer easy to be consumed by others in the Yocto\nProject community and could allow you permission to use the Yocto\nProject Compatible Logo.\n.. note::\nOnly Yocto Project member organizations are permitted to use the\nYocto Project Compatible Logo. The logo is not available for general\nuse. For information on how to become a Yocto Project member\norganization, see the :yocto_home:`Yocto Project Website <>`.\nThe Yocto Project Compatibility Program consists of a layer application\nprocess that requests permission to use the Yocto Project Compatibility\nLogo for your layer and application. The process consists of two parts:\n#. Successfully passing a script (``yocto-check-layer``) that when run\nagainst your layer, tests it against constraints based on experiences\nof how layers have worked in the real world and where pitfalls have\nbeen found. Getting a \"PASS\" result from the script is required for\nsuccessful compatibility registration.\n#. Completion of an application acceptance form, which you can find at\n:yocto_home:`/compatible-registration/`.\nTo be granted permission to use the logo, you need to satisfy the\nfollowing:\n-  Be able to check the box indicating that you got a \"PASS\" when\nrunning the script against your layer.\n-  Answer \"Yes\" to the questions on the form or have an acceptable\nexplanation for any questions answered \"No\".\n-  Be a Yocto Project Member Organization.\nThe remainder of this section presents information on the registration\nform and on the ``yocto-check-layer`` script.\nYocto Project Compatible Program Application\n--------------------------------------------\nUse the form to apply for your layer's approval. Upon successful\napplication, you can use the Yocto Project Compatibility Logo with your\nlayer and the application that uses your layer.\nTo access the form, use this link:\n:yocto_home:`/compatible-registration`.\nFollow the instructions on the form to complete your application.\nThe application consists of the following sections:\n-  *Contact Information:* Provide your contact information as the fields\nrequire. Along with your information, provide the released versions\nof the Yocto Project for which your layer is compatible.\n-  *Acceptance Criteria:* Provide \"Yes\" or \"No\" answers for each of the\nitems in the checklist. There is space at the bottom of the form for\nany explanations for items for which you answered \"No\".\n-  *Recommendations:* Provide answers for the questions regarding Linux\nkernel use and build success.\n``yocto-check-layer`` Script\n----------------------------\nThe ``yocto-check-layer`` script provides you a way to assess how\ncompatible your layer is with the Yocto Project. You should run this\nscript prior to using the form to apply for compatibility as described\nin the previous section. You need to achieve a \"PASS\" result in order to\nhave your application form successfully processed.\nThe script divides tests into three areas: COMMON, BSP, and DISTRO. For\nexample, given a distribution layer (DISTRO), the layer must pass both\nthe COMMON and DISTRO related tests. Furthermore, if your layer is a BSP\nlayer, the layer must pass the COMMON and BSP set of tests.\nTo execute the script, enter the following commands from your build\ndirectory::\n$ source oe-init-build-env\n$ yocto-check-layer your_layer_directory\nBe sure to provide the actual directory for your\nlayer as part of the command.\nEntering the command causes the script to determine the type of layer\nand then to execute a set of specific tests against the layer. The\nfollowing list overviews the test:\n-  ``common.test_readme``: Tests if a ``README`` file exists in the\nlayer and the file is not empty.\n-  ``common.test_parse``: Tests to make sure that BitBake can parse the\nfiles without error (i.e. ``bitbake -p``)."}
{"text": "\n-  ``common.test_show_environment``: Tests that the global or per-recipe\nenvironment is in order without errors (i.e. ``bitbake -e``).\n-  ``common.test_world``: Verifies that ``bitbake world`` works.\n-  ``common.test_signatures``: Tests to be sure that BSP and DISTRO\nlayers do not come with recipes that change signatures.\n-  ``common.test_layerseries_compat``: Verifies layer compatibility is\nset properly.\n-  ``bsp.test_bsp_defines_machines``: Tests if a BSP layer has machine\nconfigurations.\n-  ``bsp.test_bsp_no_set_machine``: Tests to ensure a BSP layer does not\nset the machine when the layer is added.\n-  ``bsp.test_machine_world``: Verifies that ``bitbake world`` works\nregardless of which machine is selected.\n-  ``bsp.test_machine_signatures``: Verifies that building for a\nparticular machine affects only the signature of tasks specific to\nthat machine.\n-  ``distro.test_distro_defines_distros``: Tests if a DISTRO layer has\ndistro configurations.\n-  ``distro.test_distro_no_set_distros``: Tests to ensure a DISTRO layer\ndoes not set the distribution when the layer is added.\nEnabling Your Layer\n===================\nBefore the OpenEmbedded build system can use your new layer, you need to\nenable it. To enable your layer, simply add your layer's path to the\n:term:`BBLAYERS` variable in your ``conf/bblayers.conf`` file, which is\nfound in the :term:`Build Directory`. The following example shows how to\nenable your new ``meta-mylayer`` layer (note how your new layer exists\noutside of the official ``poky`` repository which you would have checked\nout earlier)::\n# POKY_BBLAYERS_CONF_VERSION is increased each time build/conf/bblayers.conf\n# changes incompatibly\nPOKY_BBLAYERS_CONF_VERSION = \"2\"\nBBPATH = \"${TOPDIR}\"\nBBFILES ?= \"\"\nBBLAYERS ?= \" \\\n/home/user/poky/meta \\\n/home/user/poky/meta-poky \\\n/home/user/poky/meta-yocto-bsp \\\n/home/user/mystuff/meta-mylayer \\\n\"\nBitBake parses each ``conf/layer.conf`` file from the top down as\nspecified in the :term:`BBLAYERS` variable within the ``conf/bblayers.conf``\nfile. During the processing of each ``conf/layer.conf`` file, BitBake\nadds the recipes, classes and configurations contained within the\nparticular layer to the source directory.\nAppending Other Layers Metadata With Your Layer\n===============================================\nA recipe that appends Metadata to another recipe is called a BitBake\nappend file. A BitBake append file uses the ``.bbappend`` file type\nsuffix, while the corresponding recipe to which Metadata is being\nappended uses the ``.bb`` file type suffix.\nYou can use a ``.bbappend`` file in your layer to make additions or\nchanges to the content of another layer's recipe without having to copy\nthe other layer's recipe into your layer. Your ``.bbappend`` file\nresides in your layer, while the main ``.bb`` recipe file to which you\nare appending Metadata resides in a different layer.\nBeing able to append information to an existing recipe not only avoids\nduplication, but also automatically applies recipe changes from a\ndifferent layer into your layer. If you were copying recipes, you would\nhave to manually merge changes as they occur.\nWhen you create an append file, you must use the same root name as the\ncorresponding recipe file. For example, the append file\n``someapp_3.1.bbappend`` must apply to ``someapp_3.1.bb``. This\nmeans the original recipe and append filenames are version\nnumber-specific. If the corresponding recipe is renamed to update to a\nnewer version, you must also rename and possibly update the\ncorresponding ``.bbappend`` as well. During the build process, BitBake\ndisplays an error on starting if it detects a ``.bbappend`` file that\ndoes not have a corresponding recipe with a matching name. See the\n:term:`BB_DANGLINGAPPENDS_WARNONLY`\nvariable for information on how to handle this error.\nOverlaying a File Using Your Layer\n----------------------------------\nAs an example, consider the main formfactor recipe and a corresponding\nformfactor append file both from the :term:`Source Directory`.\nHere is the main\nformfactor recipe, which is named ``formfactor_0.0.bb`` and located in\nthe \"meta\" layer at ``meta/recipes-bsp/formfactor``::\nSUMMARY = \"Device formfactor information\"\nDESCRIPTION = \"A formfactor configuration file provides information about the \\\ntarget hardware for which the image is being built and information that the \\\nbuild system cannot obtain from other sources such as the kernel.\"\nSECTION = \"base\"\nLICENSE = \"MIT\"\nLIC_FILES_CHKSUM = \"file://${COREBASE}/meta/COPYING.MIT;md5=3da9cfbcb788c80a0384361b4de20420\"\nPR = \"r45\"\nSRC_URI = \"file://config file://machconfig\"\nS = \"${WORKDIR}\"\nPACKAGE_ARCH = \"${MACHINE_ARCH}\"\nINHIBIT_DEFAULT_DEPS = \"1\"\ndo_install() {\n# Install file only if it has contents\ninstall -d ${D}${sysconfdir}/formfactor/\ninstall -m 0644 ${S}/config ${D}${sysconfdir}/formfactor/\nif [ -s \"${S}/machconfig\" ]; then\ninstall -m 0644 ${S}/machconfig ${D}${sysconfdir}/formfactor/\nfi\n}\nIn the main recipe, note the :term:`SRC_URI`\nvariable, which tells the OpenEmbedded build system where to find files"}
{"text": "\nduring the build.\nFollowing is the append file, which is named ``formfactor_0.0.bbappend``\nand is from the Raspberry Pi BSP Layer named ``meta-raspberrypi``. The\nfile is in the layer at ``recipes-bsp/formfactor``::\nFILESEXTRAPATHS:prepend := \"${THISDIR}/${PN}:\"\nBy default, the build system uses the\n:term:`FILESPATH` variable to\nlocate files. This append file extends the locations by setting the\n:term:`FILESEXTRAPATHS`\nvariable. Setting this variable in the ``.bbappend`` file is the most\nreliable and recommended method for adding directories to the search\npath used by the build system to find files.\nThe statement in this example extends the directories to include\n``${``\\ :term:`THISDIR`\\ ``}/${``\\ :term:`PN`\\ ``}``,\nwhich resolves to a directory named ``formfactor`` in the same directory\nin which the append file resides (i.e.\n``meta-raspberrypi/recipes-bsp/formfactor``. This implies that you must\nhave the supporting directory structure set up that will contain any\nfiles or patches you will be including from the layer.\nUsing the immediate expansion assignment operator ``:=`` is important\nbecause of the reference to :term:`THISDIR`. The trailing colon character is\nimportant as it ensures that items in the list remain colon-separated.\n.. note::\nBitBake automatically defines the :term:`THISDIR` variable. You should\nnever set this variable yourself. Using \":prepend\" as part of the\n:term:`FILESEXTRAPATHS` ensures your path will be searched prior to other\npaths in the final list.\nAlso, not all append files add extra files. Many append files simply\nallow to add build options (e.g. ``systemd``). For these cases, your\nappend file would not even use the :term:`FILESEXTRAPATHS` statement.\nThe end result of this ``.bbappend`` file is that on a Raspberry Pi, where\n``rpi`` will exist in the list of :term:`OVERRIDES`, the file\n``meta-raspberrypi/recipes-bsp/formfactor/formfactor/rpi/machconfig`` will be\nused during :ref:`ref-tasks-fetch` and the test for a non-zero file size in\n:ref:`ref-tasks-install` will return true, and the file will be installed.\nInstalling Additional Files Using Your Layer\n--------------------------------------------\nAs another example, consider the main ``xserver-xf86-config`` recipe and a\ncorresponding ``xserver-xf86-config`` append file both from the :term:`Source\nDirectory`.  Here is the main ``xserver-xf86-config`` recipe, which is named\n``xserver-xf86-config_0.1.bb`` and located in the \"meta\" layer at\n``meta/recipes-graphics/xorg-xserver``::\nSUMMARY = \"X.Org X server configuration file\"\nHOMEPAGE = \"http://www.x.org\"\nSECTION = \"x11/base\"\nLICENSE = \"MIT\"\nLIC_FILES_CHKSUM = \"file://${COREBASE}/meta/COPYING.MIT;md5=3da9cfbcb788c80a0384361b4de20420\"\nPR = \"r33\"\nSRC_URI = \"file://xorg.conf\"\nS = \"${WORKDIR}\"\nCONFFILES:${PN} = \"${sysconfdir}/X11/xorg.conf\"\nPACKAGE_ARCH = \"${MACHINE_ARCH}\"\nALLOW_EMPTY:${PN} = \"1\"\ndo_install () {\nif test -s ${WORKDIR}/xorg.conf; then\ninstall -d ${D}/${sysconfdir}/X11\ninstall -m 0644 ${WORKDIR}/xorg.conf ${D}/${sysconfdir}/X11/\nfi\n}\nFollowing is the append file, which is named ``xserver-xf86-config_%.bbappend``\nand is from the Raspberry Pi BSP Layer named ``meta-raspberrypi``. The\nfile is in the layer at ``recipes-graphics/xorg-xserver``::\nFILESEXTRAPATHS:prepend := \"${THISDIR}/${PN}:\"\nSRC_URI:append:rpi = \" \\\nfile://xorg.conf.d/98-pitft.conf \\\nfile://xorg.conf.d/99-calibration.conf \\\n\"\ndo_install:append:rpi () {\nPITFT=\"${@bb.utils.contains(\"MACHINE_FEATURES\", \"pitft\", \"1\", \"0\", d)}\"\nif [ \"${PITFT}\" = \"1\" ]; then\ninstall -d ${D}/${sysconfdir}/X11/xorg.conf.d/\ninstall -m 0644 ${WORKDIR}/xorg.conf.d/98-pitft.conf ${D}/${sysconfdir}/X11/xorg.conf.d/\ninstall -m 0644 ${WORKDIR}/xorg.conf.d/99-calibration.conf ${D}/${sysconfdir}/X11/xorg.conf.d/\nfi\n}\nFILES:${PN}:append:rpi = \" ${sysconfdir}/X11/xorg.conf.d/*\"\nBuilding off of the previous example, we once again are setting the\n:term:`FILESEXTRAPATHS` variable.  In this case we are also using\n:term:`SRC_URI` to list additional source files to use when ``rpi`` is found in\nthe list of :term:`OVERRIDES`.  The :ref:`ref-tasks-install` task will then perform a\ncheck for an additional :term:`MACHINE_FEATURES` that if set will cause these\nadditional files to be installed.  These additional files are listed in\n:term:`FILES` so that they will be packaged.\nPrioritizing Your Layer\n=======================\nEach layer is assigned a priority value. Priority values control which\nlayer takes precedence if there are recipe files with the same name in\nmultiple layers. For these cases, the recipe file from the layer with a\nhigher priority number takes precedence. Priority values also affect the\norder in which multiple ``.bbappend`` files for the same recipe are\napplied. You can either specify the priority manually, or allow the\nbuild system to calculate it based on the layer's dependencies.\nTo specify the layer's priority manually, use the\n:term:`BBFILE_PRIORITY`\nvariable and append the layer's root name::\nBBFILE_PRIORITY_mylayer = \"1\"\n.. note::\nIt is possible for a recipe with a lower version number\n:term:`PV` in a layer that has a higher\npriority to take precedence."}
{"text": "\nAlso, the layer priority does not currently affect the precedence\norder of ``.conf`` or ``.bbclass`` files. Future versions of BitBake\nmight address this.\nManaging Layers\n===============\nYou can use the BitBake layer management tool ``bitbake-layers`` to\nprovide a view into the structure of recipes across a multi-layer\nproject. Being able to generate output that reports on configured layers\nwith their paths and priorities and on ``.bbappend`` files and their\napplicable recipes can help to reveal potential problems.\nFor help on the BitBake layer management tool, use the following\ncommand::\n$ bitbake-layers --help\nThe following list describes the available commands:\n-  ``help:`` Displays general help or help on a specified command.\n-  ``show-layers:`` Shows the current configured layers.\n-  ``show-overlayed:`` Lists overlayed recipes. A recipe is overlayed\nwhen a recipe with the same name exists in another layer that has a\nhigher layer priority.\n-  ``show-recipes:`` Lists available recipes and the layers that\nprovide them.\n-  ``show-appends:`` Lists ``.bbappend`` files and the recipe files to\nwhich they apply.\n-  ``show-cross-depends:`` Lists dependency relationships between\nrecipes that cross layer boundaries.\n-  ``add-layer:`` Adds a layer to ``bblayers.conf``.\n-  ``remove-layer:`` Removes a layer from ``bblayers.conf``\n-  ``flatten:`` Flattens the layer configuration into a separate\noutput directory. Flattening your layer configuration builds a\n\"flattened\" directory that contains the contents of all layers, with\nany overlayed recipes removed and any ``.bbappend`` files appended to\nthe corresponding recipes. You might have to perform some manual\ncleanup of the flattened layer as follows:\n-  Non-recipe files (such as patches) are overwritten. The flatten\ncommand shows a warning for these files.\n-  Anything beyond the normal layer setup has been added to the\n``layer.conf`` file. Only the lowest priority layer's\n``layer.conf`` is used.\n-  Overridden and appended items from ``.bbappend`` files need to be\ncleaned up. The contents of each ``.bbappend`` end up in the\nflattened recipe. However, if there are appended or changed\nvariable values, you need to tidy these up yourself. Consider the\nfollowing example. Here, the ``bitbake-layers`` command adds the\nline ``#### bbappended ...`` so that you know where the following\nlines originate::\n...\nDESCRIPTION = \"A useful utility\"\n...\nEXTRA_OECONF = \"--enable-something\"\n...\n#### bbappended from meta-anotherlayer ####\nDESCRIPTION = \"Customized utility\"\nEXTRA_OECONF += \"--enable-somethingelse\"\nIdeally, you would tidy up these utilities as follows::\n...\nDESCRIPTION = \"Customized utility\"\n...\nEXTRA_OECONF = \"--enable-something --enable-somethingelse\"\n...\n-  ``layerindex-fetch``: Fetches a layer from a layer index, along\nwith its dependent layers, and adds the layers to the\n``conf/bblayers.conf`` file.\n-  ``layerindex-show-depends``: Finds layer dependencies from the\nlayer index.\n-  ``save-build-conf``: Saves the currently active build configuration\n(``conf/local.conf``, ``conf/bblayers.conf``) as a template into a layer.\nThis template can later be used for setting up builds via :term:``TEMPLATECONF``.\nFor information about saving and using configuration templates, see\n\":ref:`dev-manual/custom-template-configuration-directory:creating a custom template configuration directory`\".\n-  ``create-layer``: Creates a basic layer.\n-  ``create-layers-setup``: Writes out a configuration file and/or a script that\ncan replicate the directory structure and revisions of the layers in a current build.\nFor more information, see \":ref:`dev-manual/layers:saving and restoring the layers setup`\".\nCreating a General Layer Using the ``bitbake-layers`` Script\n============================================================\nThe ``bitbake-layers`` script with the ``create-layer`` subcommand\nsimplifies creating a new general layer.\n.. note::\n-  For information on BSP layers, see the \":ref:`bsp-guide/bsp:bsp layers`\"\nsection in the Yocto\nProject Board Specific (BSP) Developer's Guide.\n-  In order to use a layer with the OpenEmbedded build system, you\nneed to add the layer to your ``bblayers.conf`` configuration\nfile. See the \":ref:`dev-manual/layers:adding a layer using the \\`\\`bitbake-layers\\`\\` script`\"\nsection for more information.\nThe default mode of the script's operation with this subcommand is to\ncreate a layer with the following:\n-  A layer priority of 6.\n-  A ``conf`` subdirectory that contains a ``layer.conf`` file.\n-  A ``recipes-example`` subdirectory that contains a further\nsubdirectory named ``example``, which contains an ``example.bb``\nrecipe file.\n-  A ``COPYING.MIT``, which is the license statement for the layer. The\nscript assumes you want to use the MIT license, which is typical for\nmost layers, for the contents of the layer itself.\n-  A ``README`` file, which is a file describing the contents of your\nnew layer.\nIn its simplest form, you can use the following command form to create a\nlayer. The command creates a layer whose name corresponds to\n\"your_layer_name\" in the current directory::"}
{"text": "\n$ bitbake-layers create-layer your_layer_name\nAs an example, the following command creates a layer named ``meta-scottrif``\nin your home directory::\n$ cd /usr/home\n$ bitbake-layers create-layer meta-scottrif\nNOTE: Starting bitbake server...\nAdd your new layer with 'bitbake-layers add-layer meta-scottrif'\nIf you want to set the priority of the layer to other than the default\nvalue of \"6\", you can either use the ``--priority`` option or you\ncan edit the\n:term:`BBFILE_PRIORITY` value\nin the ``conf/layer.conf`` after the script creates it. Furthermore, if\nyou want to give the example recipe file some name other than the\ndefault, you can use the ``--example-recipe-name`` option.\nThe easiest way to see how the ``bitbake-layers create-layer`` command\nworks is to experiment with the script. You can also read the usage\ninformation by entering the following::\n$ bitbake-layers create-layer --help\nNOTE: Starting bitbake server...\nusage: bitbake-layers create-layer [-h] [--priority PRIORITY]\n[--example-recipe-name EXAMPLERECIPE]\nlayerdir\nCreate a basic layer\npositional arguments:\nlayerdir              Layer directory to create\noptional arguments:\n-h, --help            show this help message and exit\n--priority PRIORITY, -p PRIORITY\nLayer directory to create\n--example-recipe-name EXAMPLERECIPE, -e EXAMPLERECIPE\nFilename of the example recipe\nAdding a Layer Using the ``bitbake-layers`` Script\n==================================================\nOnce you create your general layer, you must add it to your\n``bblayers.conf`` file. Adding the layer to this configuration file\nmakes the OpenEmbedded build system aware of your layer so that it can\nsearch it for metadata.\nAdd your layer by using the ``bitbake-layers add-layer`` command::\n$ bitbake-layers add-layer your_layer_name\nHere is an example that adds a\nlayer named ``meta-scottrif`` to the configuration file. Following the\ncommand that adds the layer is another ``bitbake-layers`` command that\nshows the layers that are in your ``bblayers.conf`` file::\n$ bitbake-layers add-layer meta-scottrif\nNOTE: Starting bitbake server...\nParsing recipes: 100% |##########################################################| Time: 0:00:49\nParsing of 1441 .bb files complete (0 cached, 1441 parsed). 2055 targets, 56 skipped, 0 masked, 0 errors.\n$ bitbake-layers show-layers\nNOTE: Starting bitbake server...\nlayer                 path                                      priority\n==========================================================================\nmeta                  /home/scottrif/poky/meta                  5\nmeta-poky             /home/scottrif/poky/meta-poky             5\nmeta-yocto-bsp        /home/scottrif/poky/meta-yocto-bsp        5\nworkspace             /home/scottrif/poky/build/workspace       99\nmeta-scottrif         /home/scottrif/poky/build/meta-scottrif   6\nAdding the layer to this file\nenables the build system to locate the layer during the build.\n.. note::\nDuring a build, the OpenEmbedded build system looks in the layers\nfrom the top of the list down to the bottom in that order.\nSaving and restoring the layers setup\n=====================================\nOnce you have a working build with the correct set of layers, it is beneficial\nto capture the layer setup --- what they are, which repositories they come from\nand which SCM revisions they're at --- into a configuration file, so that this\nsetup can be easily replicated later, perhaps on a different machine. Here's\nhow to do this::\n$ bitbake-layers create-layers-setup /srv/work/alex/meta-alex/\nNOTE: Starting bitbake server...\nNOTE: Created /srv/work/alex/meta-alex/setup-layers.json\nNOTE: Created /srv/work/alex/meta-alex/setup-layers\nThe tool needs a single argument which tells where to place the output, consisting\nof a json formatted layer configuration, and a ``setup-layers`` script that can use that configuration\nto restore the layers in a different location, or on a different host machine. The argument\ncan point to a custom layer (which is then deemed a \"bootstrap\" layer that needs to be\nchecked out first), or into a completely independent location.\nThe replication of the layers is performed by running the ``setup-layers`` script provided\nabove:\n#. Clone the bootstrap layer or some other repository to obtain\nthe json config and the setup script that can use it.\n#. Run the script directly with no options::\nalex@Zen2:/srv/work/alex/my-build$ meta-alex/setup-layers\nNote: not checking out source meta-alex, use --force-bootstraplayer-checkout to override.\nSetting up source meta-intel, revision 15.0-hardknott-3.3-310-g0a96edae, branch master\nRunning 'git init -q /srv/work/alex/my-build/meta-intel'\nRunning 'git remote remove origin > /dev/null 2>&1; git remote add origin git://git.yoctoproject.org/meta-intel' in /srv/work/alex/my-build/meta-intel\nRunning 'git fetch -q origin || true' in /srv/work/alex/my-build/meta-intel\nRunning 'git checkout -q 0a96edae609a3f48befac36af82cf1eed6786b4a' in /srv/work/alex/my-build/meta-intel\nSetting up source poky, revision 4.1_M1-372-g55483d28f2, branch akanavin/setup-layers\nRunning 'git init -q /srv/work/alex/my-build/poky'\nRunning 'git remote remove origin > /dev/null 2>&1; git remote add origin git://git.yoctoproject.org/poky' in /srv/work/alex/my-build/poky\nRunning 'git fetch -q origin || true' in /srv/work/alex/my-build/poky\nRunning 'git remote remove poky-contrib > /dev/null 2>&1; git remote add poky-contrib ssh://git@push.yoctoproject.org/poky-contrib' in /srv/work/alex/my-build/poky\nRunning 'git fetch -q poky-contrib || true' in /srv/work/alex/my-build/poky\nRunning 'git checkout -q 11db0390b02acac1324e0f827beb0e2e3d0d1d63' in /srv/work/alex/my-build/poky\n.. note::\nThis will work to update an existing checkout as well.\n.. note::\nThe script is self-sufficient and requires only python3"}
{"text": "Understanding and Creating Layers\nand git on the build machine.\n.. note::\nBoth the ``create-layers-setup`` and the ``setup-layers`` provided several additional options\nthat customize their behavior - you are welcome to study them via ``--help`` command line parameter."}
{"text": "\n******************************************\nThe Yocto Project Development Tasks Manual\n******************************************\nWelcome\n=======\nWelcome to the Yocto Project Development Tasks Manual. This manual\nprovides relevant procedures necessary for developing in the Yocto\nProject environment (i.e. developing embedded Linux images and\nuser-space applications that run on targeted devices). This manual groups\nrelated procedures into higher-level sections. Procedures can consist of\nhigh-level steps or low-level steps depending on the topic.\nThis manual provides the following:\n-  Procedures that help you get going with the Yocto Project; for\nexample, procedures that show you how to set up a build host and work\nwith the Yocto Project source repositories.\n-  Procedures that show you how to submit changes to the Yocto Project.\nChanges can be improvements, new features, or bug fixes.\n-  Procedures related to \"everyday\" tasks you perform while developing\nimages and applications using the Yocto Project, such as\ncreating a new layer, customizing an image, writing a new recipe,\nand so forth.\nThis manual does not provide the following:\n-  Redundant step-by-step instructions: For example, the\n:doc:`/sdk-manual/index` manual contains detailed\ninstructions on how to install an SDK, which is used to develop\napplications for target hardware.\n-  Reference or conceptual material: This type of material resides in an\nappropriate reference manual. As an example, system variables are\ndocumented in the :doc:`/ref-manual/index`.\n-  Detailed public information not specific to the Yocto Project: For\nexample, exhaustive information on how to use the Git version\ncontrol system is better covered with Internet searches and official Git\ndocumentation than through the Yocto Project documentation.\nOther Information\n=================\nBecause this manual presents information for many different topics,\nsupplemental information is recommended for full comprehension. For\nintroductory information on the Yocto Project, see the\n:yocto_home:`Yocto Project Website <>`. If you want to build an image with no\nknowledge of Yocto Project as a way of quickly testing it out, see the\n:doc:`/brief-yoctoprojectqs/index` document.\nFor a comprehensive list of links and other documentation, see the\n\":ref:`ref-manual/resources:links and related documentation`\"\nsection in the Yocto Project Reference Manual."}
{"text": "\nMaintaining Build Output Quality\n********************************\nMany factors can influence the quality of a build. For example, if you\nupgrade a recipe to use a new version of an upstream software package or\nyou experiment with some new configuration options, subtle changes can\noccur that you might not detect until later. Consider the case where\nyour recipe is using a newer version of an upstream package. In this\ncase, a new version of a piece of software might introduce an optional\ndependency on another library, which is auto-detected. If that library\nhas already been built when the software is building, the software will\nlink to the built library and that library will be pulled into your\nimage along with the new software even if you did not want the library.\nThe :ref:`ref-classes-buildhistory` class helps you maintain the quality of\nyour build output. You can use the class to highlight unexpected and possibly\nunwanted changes in the build output. When you enable build history, it records\ninformation about the contents of each package and image and then commits that\ninformation to a local Git repository where you can examine the information.\nThe remainder of this section describes the following:\n-  :ref:`How you can enable and disable build history <dev-manual/build-quality:enabling and disabling build history>`\n-  :ref:`How to understand what the build history contains <dev-manual/build-quality:understanding what the build history contains>`\n-  :ref:`How to limit the information used for build history <dev-manual/build-quality:using build history to gather image information only>`\n-  :ref:`How to examine the build history from both a command-line and web interface <dev-manual/build-quality:examining build history information>`\nEnabling and Disabling Build History\n====================================\nBuild history is disabled by default. To enable it, add the following\n:term:`INHERIT` statement and set the :term:`BUILDHISTORY_COMMIT` variable to\n\"1\" at the end of your ``conf/local.conf`` file found in the\n:term:`Build Directory`::\nINHERIT += \"buildhistory\"\nBUILDHISTORY_COMMIT = \"1\"\nEnabling build history as\npreviously described causes the OpenEmbedded build system to collect\nbuild output information and commit it as a single commit to a local\n:ref:`overview-manual/development-environment:git` repository.\n.. note::\nEnabling build history increases your build times slightly,\nparticularly for images, and increases the amount of disk space used\nduring the build.\nYou can disable build history by removing the previous statements from\nyour ``conf/local.conf`` file.\nUnderstanding What the Build History Contains\n=============================================\nBuild history information is kept in ``${``\\ :term:`TOPDIR`\\ ``}/buildhistory``\nin the :term:`Build Directory` as defined by the :term:`BUILDHISTORY_DIR`\nvariable. Here is an example abbreviated listing:\n.. image:: figures/buildhistory.png\n:align: center\n:width: 50%\nAt the top level, there is a ``metadata-revs`` file that lists the\nrevisions of the repositories for the enabled layers when the build was\nproduced. The rest of the data splits into separate ``packages``,\n``images`` and ``sdk`` directories, the contents of which are described\nas follows.\nBuild History Package Information\n---------------------------------\nThe history for each package contains a text file that has name-value\npairs with information about the package. For example,\n``buildhistory/packages/i586-poky-linux/busybox/busybox/latest``\ncontains the following:\n.. code-block:: none\nPV = 1.22.1\nPR = r32\nRPROVIDES =\nRDEPENDS = glibc (>= 2.20) update-alternatives-opkg\nRRECOMMENDS = busybox-syslog busybox-udhcpc update-rc.d\nPKGSIZE = 540168\nFILES = /usr/bin/* /usr/sbin/* /usr/lib/busybox/* /usr/lib/lib*.so.* \\\n/etc /com /var /bin/* /sbin/* /lib/*.so.* /lib/udev/rules.d \\\n/usr/lib/udev/rules.d /usr/share/busybox /usr/lib/busybox/* \\\n/usr/share/pixmaps /usr/share/applications /usr/share/idl \\\n/usr/share/omf /usr/share/sounds /usr/lib/bonobo/servers\nFILELIST = /bin/busybox /bin/busybox.nosuid /bin/busybox.suid /bin/sh \\\n/etc/busybox.links.nosuid /etc/busybox.links.suid\nMost of these\nname-value pairs correspond to variables used to produce the package.\nThe exceptions are ``FILELIST``, which is the actual list of files in\nthe package, and ``PKGSIZE``, which is the total size of files in the\npackage in bytes.\nThere is also a file that corresponds to the recipe from which the package\ncame (e.g. ``buildhistory/packages/i586-poky-linux/busybox/latest``):\n.. code-block:: none\nPV = 1.22.1\nPR = r32\nDEPENDS = initscripts kern-tools-native update-rc.d-native \\\nvirtual/i586-poky-linux-compilerlibs virtual/i586-poky-linux-gcc \\\nvirtual/libc virtual/update-alternatives\nPACKAGES = busybox-ptest busybox-httpd busybox-udhcpd busybox-udhcpc \\\nbusybox-syslog busybox-mdev busybox-hwclock busybox-dbg \\\nbusybox-staticdev busybox-dev busybox-doc busybox-locale busybox\nFinally, for those recipes fetched from a version control system (e.g.,\nGit), there is a file that lists source revisions that are specified in\nthe recipe and the actual revisions used during the build. Listed\nand actual revisions might differ when\n:term:`SRCREV` is set to\n${:term:`AUTOREV`}. Here is an\nexample assuming\n``buildhistory/packages/qemux86-poky-linux/linux-yocto/latest_srcrev``)::\n# SRCREV_machine = \"38cd560d5022ed2dbd1ab0dca9642e47c98a0aa1\"\nSRCREV_machine = \"38cd560d5022ed2dbd1ab0dca9642e47c98a0aa1\""}
{"text": "\n# SRCREV_meta = \"a227f20eff056e511d504b2e490f3774ab260d6f\"\nSRCREV_meta =\"a227f20eff056e511d504b2e490f3774ab260d6f\"\nYou can use the\n``buildhistory-collect-srcrevs`` command with the ``-a`` option to\ncollect the stored :term:`SRCREV` values from build history and report them\nin a format suitable for use in global configuration (e.g.,\n``local.conf`` or a distro include file) to override floating\n:term:`AUTOREV` values to a fixed set of revisions. Here is some example\noutput from this command::\n$ buildhistory-collect-srcrevs -a\n# all-poky-linux\nSRCREV:pn-ca-certificates = \"07de54fdcc5806bde549e1edf60738c6bccf50e8\"\nSRCREV:pn-update-rc.d = \"8636cf478d426b568c1be11dbd9346f67e03adac\"\n# core2-64-poky-linux\nSRCREV:pn-binutils = \"87d4632d36323091e731eb07b8aa65f90293da66\"\nSRCREV:pn-btrfs-tools = \"8ad326b2f28c044cb6ed9016d7c3285e23b673c8\"\nSRCREV_bzip2-tests:pn-bzip2 = \"f9061c030a25de5b6829e1abf373057309c734c0\"\nSRCREV:pn-e2fsprogs = \"02540dedd3ddc52c6ae8aaa8a95ce75c3f8be1c0\"\nSRCREV:pn-file = \"504206e53a89fd6eed71aeaf878aa3512418eab1\"\nSRCREV_glibc:pn-glibc = \"24962427071fa532c3c48c918e9d64d719cc8a6c\"\nSRCREV:pn-gnome-desktop-testing = \"e346cd4ed2e2102c9b195b614f3c642d23f5f6e7\"\nSRCREV:pn-init-system-helpers = \"dbd9197569c0935029acd5c9b02b84c68fd937ee\"\nSRCREV:pn-kmod = \"b6ecfc916a17eab8f93be5b09f4e4f845aabd3d1\"\nSRCREV:pn-libnsl2 = \"82245c0c58add79a8e34ab0917358217a70e5100\"\nSRCREV:pn-libseccomp = \"57357d2741a3b3d3e8425889a6b79a130e0fa2f3\"\nSRCREV:pn-libxcrypt = \"50cf2b6dd4fdf04309445f2eec8de7051d953abf\"\nSRCREV:pn-ncurses = \"51d0fd9cc3edb975f04224f29f777f8f448e8ced\"\nSRCREV:pn-procps = \"19a508ea121c0c4ac6d0224575a036de745eaaf8\"\nSRCREV:pn-psmisc = \"5fab6b7ab385080f1db725d6803136ec1841a15f\"\nSRCREV:pn-ptest-runner = \"bcb82804daa8f725b6add259dcef2067e61a75aa\"\nSRCREV:pn-shared-mime-info = \"18e558fa1c8b90b86757ade09a4ba4d6a6cf8f70\"\nSRCREV:pn-zstd = \"e47e674cd09583ff0503f0f6defd6d23d8b718d3\"\n# qemux86_64-poky-linux\nSRCREV_machine:pn-linux-yocto = \"20301aeb1a64164b72bc72af58802b315e025c9c\"\nSRCREV_meta:pn-linux-yocto = \"2d38a472b21ae343707c8bd64ac68a9eaca066a0\"\n# x86_64-linux\nSRCREV:pn-binutils-cross-x86_64 = \"87d4632d36323091e731eb07b8aa65f90293da66\"\nSRCREV_glibc:pn-cross-localedef-native = \"24962427071fa532c3c48c918e9d64d719cc8a6c\"\nSRCREV_localedef:pn-cross-localedef-native = \"794da69788cbf9bf57b59a852f9f11307663fa87\"\nSRCREV:pn-debianutils-native = \"de14223e5bffe15e374a441302c528ffc1cbed57\"\nSRCREV:pn-libmodulemd-native = \"ee80309bc766d781a144e6879419b29f444d94eb\"\nSRCREV:pn-virglrenderer-native = \"363915595e05fb252e70d6514be2f0c0b5ca312b\"\nSRCREV:pn-zstd-native = \"e47e674cd09583ff0503f0f6defd6d23d8b718d3\"\n.. note::\nHere are some notes on using the ``buildhistory-collect-srcrevs`` command:\n-  By default, only values where the :term:`SRCREV` was not hardcoded\n(usually when :term:`AUTOREV` is used) are reported. Use the ``-a``\noption to see all :term:`SRCREV` values.\n-  The output statements might not have any effect if overrides are\napplied elsewhere in the build system configuration. Use the\n``-f`` option to add the ``forcevariable`` override to each output\nline if you need to work around this restriction.\n-  The script does apply special handling when building for multiple\nmachines. However, the script does place a comment before each set\nof values that specifies which triplet to which they belong as\npreviously shown (e.g., ``i586-poky-linux``).\nBuild History Image Information\n-------------------------------\nThe files produced for each image are as follows:\n-  ``image-files:`` A directory containing selected files from the root\nfilesystem. The files are defined by\n:term:`BUILDHISTORY_IMAGE_FILES`.\n-  ``build-id.txt:`` Human-readable information about the build\nconfiguration and metadata source revisions. This file contains the\nfull build header as printed by BitBake.\n-  ``*.dot:`` Dependency graphs for the image that are compatible with\n``graphviz``.\n-  ``files-in-image.txt:`` A list of files in the image with\npermissions, owner, group, size, and symlink information.\n-  ``image-info.txt:`` A text file containing name-value pairs with\ninformation about the image. See the following listing example for\nmore information.\n-  ``installed-package-names.txt:`` A list of installed packages by name\nonly.\n-  ``installed-package-sizes.txt:`` A list of installed packages ordered\nby size.\n-  ``installed-packages.txt:`` A list of installed packages with full\npackage filenames.\n.. note::\nInstalled package information is able to be gathered and produced\neven if package management is disabled for the final image.\nHere is an example of ``image-info.txt``:\n.. code-block:: none\nDISTRO = poky\nDISTRO_VERSION = 3.4+snapshot-a0245d7be08f3d24ea1875e9f8872aa6bbff93be\nUSER_CLASSES = buildstats\nIMAGE_CLASSES = qemuboot qemuboot license_image\nIMAGE_FEATURES = debug-tweaks\nIMAGE_LINGUAS =\nIMAGE_INSTALL = packagegroup-core-boot speex speexdsp\nBAD_RECOMMENDATIONS =\nNO_RECOMMENDATIONS =\nPACKAGE_EXCLUDE =\nROOTFS_POSTPROCESS_COMMAND = write_package_manifest; license_create_manifest; cve_check_write_rootfs_manifest;   ssh_allow_empty_password;  ssh_allow_root_login;  postinst_enable_logging;  rootfs_update_timestamp;   write_image_test_data;   empty_var_volatile;   sort_passwd; rootfs_reproducible;\nIMAGE_POSTPROCESS_COMMAND =  buildhistory_get_imageinfo ;\nIMAGESIZE = 9265\nOther than ``IMAGESIZE``,\nwhich is the total size of the files in the image in Kbytes, the\nname-value pairs are variables that may have influenced the content of\nthe image. This information is often useful when you are trying to"}
{"text": "\ndetermine why a change in the package or file listings has occurred.\nUsing Build History to Gather Image Information Only\n----------------------------------------------------\nAs you can see, build history produces image information, including\ndependency graphs, so you can see why something was pulled into the\nimage. If you are just interested in this information and not interested\nin collecting specific package or SDK information, you can enable\nwriting only image information without any history by adding the\nfollowing to your ``conf/local.conf`` file found in the\n:term:`Build Directory`::\nINHERIT += \"buildhistory\"\nBUILDHISTORY_COMMIT = \"0\"\nBUILDHISTORY_FEATURES = \"image\"\nHere, you set the\n:term:`BUILDHISTORY_FEATURES`\nvariable to use the image feature only.\nBuild History SDK Information\n-----------------------------\nBuild history collects similar information on the contents of SDKs (e.g.\n``bitbake -c populate_sdk imagename``) as compared to information it\ncollects for images. Furthermore, this information differs depending on\nwhether an extensible or standard SDK is being produced.\nThe following list shows the files produced for SDKs:\n-  ``files-in-sdk.txt:`` A list of files in the SDK with permissions,\nowner, group, size, and symlink information. This list includes both\nthe host and target parts of the SDK.\n-  ``sdk-info.txt:`` A text file containing name-value pairs with\ninformation about the SDK. See the following listing example for more\ninformation.\n-  ``sstate-task-sizes.txt:`` A text file containing name-value pairs\nwith information about task group sizes (e.g. :ref:`ref-tasks-populate_sysroot`\ntasks have a total size). The ``sstate-task-sizes.txt`` file exists\nonly when an extensible SDK is created.\n-  ``sstate-package-sizes.txt:`` A text file containing name-value pairs\nwith information for the shared-state packages and sizes in the SDK.\nThe ``sstate-package-sizes.txt`` file exists only when an extensible\nSDK is created.\n-  ``sdk-files:`` A folder that contains copies of the files mentioned\nin ``BUILDHISTORY_SDK_FILES`` if the files are present in the output.\nAdditionally, the default value of ``BUILDHISTORY_SDK_FILES`` is\nspecific to the extensible SDK although you can set it differently if\nyou would like to pull in specific files from the standard SDK.\nThe default files are ``conf/local.conf``, ``conf/bblayers.conf``,\n``conf/auto.conf``, ``conf/locked-sigs.inc``, and\n``conf/devtool.conf``. Thus, for an extensible SDK, these files get\ncopied into the ``sdk-files`` directory.\n-  The following information appears under each of the ``host`` and\n``target`` directories for the portions of the SDK that run on the\nhost and on the target, respectively:\n.. note::\nThe following files for the most part are empty when producing an\nextensible SDK because this type of SDK is not constructed from\npackages as is the standard SDK.\n-  ``depends.dot:`` Dependency graph for the SDK that is compatible\nwith ``graphviz``.\n-  ``installed-package-names.txt:`` A list of installed packages by\nname only.\n-  ``installed-package-sizes.txt:`` A list of installed packages\nordered by size.\n-  ``installed-packages.txt:`` A list of installed packages with full\npackage filenames.\nHere is an example of ``sdk-info.txt``:\n.. code-block:: none\nDISTRO = poky\nDISTRO_VERSION = 1.3+snapshot-20130327\nSDK_NAME = poky-glibc-i686-arm\nSDK_VERSION = 1.3+snapshot\nSDKMACHINE =\nSDKIMAGE_FEATURES = dev-pkgs dbg-pkgs\nBAD_RECOMMENDATIONS =\nSDKSIZE = 352712\nOther than ``SDKSIZE``, which is\nthe total size of the files in the SDK in Kbytes, the name-value pairs\nare variables that might have influenced the content of the SDK. This\ninformation is often useful when you are trying to determine why a\nchange in the package or file listings has occurred.\nExamining Build History Information\n-----------------------------------\nYou can examine build history output from the command line or from a web\ninterface.\nTo see any changes that have occurred (assuming you have\n:term:`BUILDHISTORY_COMMIT` = \"1\"),\nyou can simply use any Git command that allows you to view the history\nof a repository. Here is one method::\n$ git log -p\nYou need to realize,\nhowever, that this method does show changes that are not significant\n(e.g. a package's size changing by a few bytes).\nThere is a command-line tool called ``buildhistory-diff``, though,\nthat queries the Git repository and prints just the differences that\nmight be significant in human-readable form. Here is an example::\n$ poky/poky/scripts/buildhistory-diff . HEAD^\nChanges to images/qemux86_64/glibc/core-image-minimal (files-in-image.txt):\n/etc/anotherpkg.conf was added\n/sbin/anotherpkg was added\n* (installed-package-names.txt):\n*   anotherpkg was added\nChanges to images/qemux86_64/glibc/core-image-minimal (installed-package-names.txt):\nanotherpkg was added\npackages/qemux86_64-poky-linux/v86d: PACKAGES: added \"v86d-extras\""}
{"text": "Maintaining Build Output Quality\n* PR changed from \"r0\" to \"r1\"\n* PV changed from \"0.1.10\" to \"0.1.12\"\npackages/qemux86_64-poky-linux/v86d/v86d: PKGSIZE changed from 110579 to 144381 (+30%)\n* PR changed from \"r0\" to \"r1\"\n* PV changed from \"0.1.10\" to \"0.1.12\"\n.. note::\nThe ``buildhistory-diff`` tool requires the ``GitPython``\npackage. Be sure to install it using Pip3 as follows::\n$ pip3 install GitPython --user\nAlternatively, you can install ``python3-git`` using the appropriate\ndistribution package manager (e.g. ``apt``, ``dnf``, or ``zipper``).\nTo see changes to the build history using a web interface, follow the\ninstruction in the ``README`` file\n:yocto_git:`here </buildhistory-web/>`.\nHere is a sample screenshot of the interface:\n.. image:: figures/buildhistory-web.png\n:width: 100%"}
{"text": "\nWorking with Pre-Built Libraries\n********************************\nIntroduction\n============\nSome library vendors do not release source code for their software but do\nrelease pre-built binaries. When shared libraries are built, they should\nbe versioned (see `this article\n<https://tldp.org/HOWTO/Program-Library-HOWTO/shared-libraries.html>`__\nfor some background), but sometimes this is not done.\nTo summarize, a versioned library must meet two conditions:\n#.    The filename must have the version appended, for example: ``libfoo.so.1.2.3``.\n#.    The library must have the ELF tag ``SONAME`` set to the major version\nof the library, for example: ``libfoo.so.1``. You can check this by\nrunning ``readelf -d filename | grep SONAME``.\nThis section shows how to deal with both versioned and unversioned\npre-built libraries.\nVersioned Libraries\n===================\nIn this example we work with pre-built libraries for the FT4222H USB I/O chip.\nLibraries are built for several target architecture variants and packaged in\nan archive as follows::\n├── build-arm-hisiv300\n│   └── libft4222.so.1.4.4.44\n├── build-arm-v5-sf\n│   └── libft4222.so.1.4.4.44\n├── build-arm-v6-hf\n│   └── libft4222.so.1.4.4.44\n├── build-arm-v7-hf\n│   └── libft4222.so.1.4.4.44\n├── build-arm-v8\n│   └── libft4222.so.1.4.4.44\n├── build-i386\n│   └── libft4222.so.1.4.4.44\n├── build-i486\n│   └── libft4222.so.1.4.4.44\n├── build-mips-eglibc-hf\n│   └── libft4222.so.1.4.4.44\n├── build-pentium\n│   └── libft4222.so.1.4.4.44\n├── build-x86_64\n│   └── libft4222.so.1.4.4.44\n├── examples\n│   ├── get-version.c\n│   ├── i2cm.c\n│   ├── spim.c\n│   └── spis.c\n├── ftd2xx.h\n├── install4222.sh\n├── libft4222.h\n├── ReadMe.txt\n└── WinTypes.h\nTo write a recipe to use such a library in your system:\n-  The vendor will probably have a proprietary licence, so set\n:term:`LICENSE_FLAGS` in your recipe.\n-  The vendor provides a tarball containing libraries so set :term:`SRC_URI`\nappropriately.\n-  Set :term:`COMPATIBLE_HOST` so that the recipe cannot be used with an\nunsupported architecture. In the following example, we only support the 32\nand 64 bit variants of the ``x86`` architecture.\n-  As the vendor provides versioned libraries, we can use ``oe_soinstall``\nfrom :ref:`ref-classes-utils` to install the shared library and create\nsymbolic links. If the vendor does not do this, we need to follow the\nnon-versioned library guidelines in the next section.\n-  As the vendor likely used :term:`LDFLAGS` different from those in your Yocto\nProject build, disable the corresponding checks by adding ``ldflags``\nto :term:`INSANE_SKIP`.\n-  The vendor will typically ship release builds without debugging symbols.\nAvoid errors by preventing the packaging task from stripping out the symbols\nand adding them to a separate debug package. This is done by setting the\n``INHIBIT_`` flags shown below.\nThe complete recipe would look like this::\nSUMMARY = \"FTDI FT4222H Library\"\nSECTION = \"libs\"\nLICENSE_FLAGS = \"ftdi\"\nLICENSE = \"CLOSED\"\nCOMPATIBLE_HOST = \"(i.86|x86_64).*-linux\"\n# Sources available in a .tgz file in .zip archive\n# at https://ftdichip.com/wp-content/uploads/2021/01/libft4222-linux-1.4.4.44.zip\n# Found on https://ftdichip.com/software-examples/ft4222h-software-examples/\n# Since dealing with this particular type of archive is out of topic here,\n# we use a local link.\nSRC_URI = \"file://libft4222-linux-${PV}.tgz\"\nS = \"${WORKDIR}\"\nARCH_DIR:x86-64 = \"build-x86_64\"\nARCH_DIR:i586 = \"build-i386\"\nARCH_DIR:i686 = \"build-i386\"\nINSANE_SKIP:${PN} = \"ldflags\"\nINHIBIT_PACKAGE_STRIP = \"1\"\nINHIBIT_SYSROOT_STRIP = \"1\"\nINHIBIT_PACKAGE_DEBUG_SPLIT = \"1\"\ndo_install () {\ninstall -m 0755 -d ${D}${libdir}\noe_soinstall ${S}/${ARCH_DIR}/libft4222.so.${PV} ${D}${libdir}\ninstall -d ${D}${includedir}\ninstall -m 0755 ${S}/*.h ${D}${includedir}\n}\nIf the precompiled binaries are not statically linked and have dependencies on\nother libraries, then by adding those libraries to :term:`DEPENDS`, the linking\ncan be examined and the appropriate :term:`RDEPENDS` automatically added."}
{"text": "\nNon-Versioned Libraries\n=======================\nSome Background\n---------------\nLibraries in Linux systems are generally versioned so that it is possible\nto have multiple versions of the same library installed, which eases upgrades\nand support for older software. For example, suppose that in a versioned\nlibrary, an actual library is called ``libfoo.so.1.2``, a symbolic link named\n``libfoo.so.1`` points to ``libfoo.so.1.2``, and a symbolic link named\n``libfoo.so`` points to ``libfoo.so.1.2``. Given these conditions, when you\nlink a binary against a library, you typically provide the unversioned file\nname (i.e. ``-lfoo`` to the linker). However, the linker follows the symbolic\nlink and actually links against the versioned filename. The unversioned symbolic\nlink is only used at development time. Consequently, the library is packaged\nalong with the headers in the development package ``${PN}-dev`` along with the\nactual library and versioned symbolic links in ``${PN}``. Because versioned\nlibraries are far more common than unversioned libraries, the default packaging\nrules assume versioned libraries.\nYocto Library Packaging Overview\n--------------------------------\nIt follows that packaging an unversioned library requires a bit of work in the\nrecipe. By default, ``libfoo.so`` gets packaged into ``${PN}-dev``, which\ntriggers a QA warning that a non-symlink library is in a ``-dev`` package,\nand binaries in the same recipe link to the library in ``${PN}-dev``,\nwhich triggers more QA warnings. To solve this problem, you need to package the\nunversioned library into ``${PN}`` where it belongs. The following are the abridged\ndefault :term:`FILES` variables in ``bitbake.conf``::\nSOLIBS = \".so.*\"\nSOLIBSDEV = \".so\"\nFILES:${PN} = \"... ${libdir}/lib*${SOLIBS} ...\"\nFILES_SOLIBSDEV ?= \"... ${libdir}/lib*${SOLIBSDEV} ...\"\nFILES:${PN}-dev = \"... ${FILES_SOLIBSDEV} ...\"\n:term:`SOLIBS` defines a pattern that matches real shared object libraries.\n:term:`SOLIBSDEV` matches the development form (unversioned symlink). These two\nvariables are then used in ``FILES:${PN}`` and ``FILES:${PN}-dev``, which puts\nthe real libraries into ``${PN}`` and the unversioned symbolic link into ``${PN}-dev``.\nTo package unversioned libraries, you need to modify the variables in the recipe\nas follows::\nSOLIBS = \".so\"\nFILES_SOLIBSDEV = \"\"\nThe modifications cause the ``.so`` file to be the real library\nand unset :term:`FILES_SOLIBSDEV` so that no libraries get packaged into\n``${PN}-dev``. The changes are required because unless :term:`PACKAGES` is changed,\n``${PN}-dev`` collects files before `${PN}`. ``${PN}-dev`` must not collect any of\nthe files you want in ``${PN}``.\nFinally, loadable modules, essentially unversioned libraries that are linked\nat runtime using ``dlopen()`` instead of at build time, should generally be\ninstalled in a private directory. However, if they are installed in ``${libdir}``,\nthen the modules can be treated as unversioned libraries.\nExample\n-------\nThe example below installs an unversioned x86-64 pre-built library named\n``libfoo.so``. The :term:`COMPATIBLE_HOST` variable limits recipes to the\nx86-64 architecture while the :term:`INSANE_SKIP`, :term:`INHIBIT_PACKAGE_STRIP`\nand :term:`INHIBIT_SYSROOT_STRIP` variables are all set as in the above\nversioned library example. The \"magic\" is setting the :term:`SOLIBS` and\n:term:`FILES_SOLIBSDEV` variables as explained above::\nSUMMARY = \"libfoo sample recipe\"\nSECTION = \"libs\"\nLICENSE = \"CLOSED\"\nSRC_URI = \"file://libfoo.so\"\nCOMPATIBLE_HOST = \"x86_64.*-linux\"\nINSANE_SKIP:${PN} = \"ldflags\"\nINHIBIT_PACKAGE_STRIP = \"1\"\nINHIBIT_SYSROOT_STRIP = \"1\"\nSOLIBS = \".so\"\nFILES_SOLIBSDEV = \"\"\ndo_install () {\ninstall -d ${D}${libdir}\ninstall -m 0755 ${WORKDIR}/libfoo.so ${D}${libdir}\n}"}
{"text": "\nCreating a Software Bill of Materials\n*************************************\nOnce you are able to build an image for your project, once the licenses for\neach software component are all identified (see\n\":ref:`dev-manual/licenses:working with licenses`\") and once vulnerability\nfixes are applied (see \":ref:`dev-manual/vulnerabilities:checking\nfor vulnerabilities`\"), the OpenEmbedded build system can generate\na description of all the components you used, their licenses, their dependencies,\ntheir sources, the changes that were applied to them and the known\nvulnerabilities that were fixed.\nThis description is generated in the form of a *Software Bill of Materials*\n(:term:`SBOM`), using the :term:`SPDX` standard.\nWhen you release software, this is the most standard way to provide information\nabout the Software Supply Chain of your software image and SDK. The\n:term:`SBOM` tooling is often used to ensure open source license compliance by\nproviding the license texts used in the product which legal departments and end\nusers can read in standardized format.\n:term:`SBOM` information is also critical to performing vulnerability exposure\nassessments, as all the components used in the Software Supply Chain are listed.\nThe OpenEmbedded build system doesn't generate such information by default.\nTo make this happen, you must inherit the\n:ref:`ref-classes-create-spdx` class from a configuration file::\nINHERIT += \"create-spdx\"\nYou then get :term:`SPDX` output in JSON format as an\n``IMAGE-MACHINE.spdx.json`` file in ``tmp/deploy/images/MACHINE/`` inside the\n:term:`Build Directory`.\nThis is a toplevel file accompanied by an ``IMAGE-MACHINE.spdx.index.json``\ncontaining an index of JSON :term:`SPDX` files for individual recipes, together\nwith an ``IMAGE-MACHINE.spdx.tar.zst`` compressed archive containing all such\nfiles.\nThe :ref:`ref-classes-create-spdx` class offers options to include\nmore information in the output :term:`SPDX` data, such as making the generated\nfiles more human readable (:term:`SPDX_PRETTY`), adding compressed archives of\nthe files in the generated target packages (:term:`SPDX_ARCHIVE_PACKAGED`),\nadding a description of the source files used to generate host tools and target\npackages (:term:`SPDX_INCLUDE_SOURCES`) and adding archives of these source\nfiles themselves (:term:`SPDX_ARCHIVE_SOURCES`).\nThough the toplevel :term:`SPDX` output is available in\n``tmp/deploy/images/MACHINE/`` inside the :term:`Build Directory`, ancillary\ngenerated files are available in ``tmp/deploy/spdx/MACHINE`` too, such as:\n-  The individual :term:`SPDX` JSON files in the ``IMAGE-MACHINE.spdx.tar.zst``\narchive.\n-  Compressed archives of the files in the generated target packages,\nin ``packages/packagename.tar.zst`` (when :term:`SPDX_ARCHIVE_PACKAGED`\nis set).\n-  Compressed archives of the source files used to build the host tools\nand the target packages in ``recipes/recipe-packagename.tar.zst``\n(when :term:`SPDX_ARCHIVE_SOURCES` is set). Those are needed to fulfill\n\"source code access\" license requirements.\nSee also the :term:`SPDX_CUSTOM_ANNOTATION_VARS` variable which allows\nto associate custom notes to a recipe.\nSee the `tools page <https://spdx.dev/resources/tools/>`__ on the :term:`SPDX`\nproject website for a list of tools to consume and transform the :term:`SPDX`\ndata generated by the OpenEmbedded build system.\nSee also Joshua Watt's\n`Automated SBoM generation with OpenEmbedded and the Yocto Project <https://youtu.be/Q5UQUM6zxVU>`__\npresentation at FOSDEM 2023."}
{"text": "\nEfficiently Fetching Source Files During a Build\n************************************************\nThe OpenEmbedded build system works with source files located through\nthe :term:`SRC_URI` variable. When\nyou build something using BitBake, a big part of the operation is\nlocating and downloading all the source tarballs. For images,\ndownloading all the source for various packages can take a significant\namount of time.\nThis section shows you how you can use mirrors to speed up fetching\nsource files and how you can pre-fetch files all of which leads to more\nefficient use of resources and time.\nSetting up Effective Mirrors\n============================\nA good deal that goes into a Yocto Project build is simply downloading\nall of the source tarballs. Maybe you have been working with another\nbuild system for which you have built up a\nsizable directory of source tarballs. Or, perhaps someone else has such\na directory for which you have read access. If so, you can save time by\nadding statements to your configuration file so that the build process\nchecks local directories first for existing tarballs before checking the\nInternet.\nHere is an efficient way to set it up in your ``local.conf`` file::\nSOURCE_MIRROR_URL ?= \"file:///home/you/your-download-dir/\"\nINHERIT += \"own-mirrors\"\nBB_GENERATE_MIRROR_TARBALLS = \"1\"\n# BB_NO_NETWORK = \"1\"\nIn the previous example, the\n:term:`BB_GENERATE_MIRROR_TARBALLS`\nvariable causes the OpenEmbedded build system to generate tarballs of\nthe Git repositories and store them in the\n:term:`DL_DIR` directory. Due to\nperformance reasons, generating and storing these tarballs is not the\nbuild system's default behavior.\nYou can also use the\n:term:`PREMIRRORS` variable. For\nan example, see the variable's glossary entry in the Yocto Project\nReference Manual.\nGetting Source Files and Suppressing the Build\n==============================================\nAnother technique you can use to ready yourself for a successive string\nof build operations, is to pre-fetch all the source files without\nactually starting a build. This technique lets you work through any\ndownload issues and ultimately gathers all the source files into your\ndownload directory :ref:`structure-build-downloads`,\nwhich is located with :term:`DL_DIR`.\nUse the following BitBake command form to fetch all the necessary\nsources without starting the build::\n$ bitbake target --runall=fetch\nThis\nvariation of the BitBake command guarantees that you have all the\nsources for that BitBake target should you disconnect from the Internet\nand want to do the build later offline."}
{"text": "\nUsing x32 psABI\n***************\nx32 processor-specific Application Binary Interface (`x32\npsABI <https://software.intel.com/en-us/node/628948>`__) is a native\n32-bit processor-specific ABI for Intel 64 (x86-64) architectures. An\nABI defines the calling conventions between functions in a processing\nenvironment. The interface determines what registers are used and what\nthe sizes are for various C data types.\nSome processing environments prefer using 32-bit applications even when\nrunning on Intel 64-bit platforms. Consider the i386 psABI, which is a\nvery old 32-bit ABI for Intel 64-bit platforms. The i386 psABI does not\nprovide efficient use and access of the Intel 64-bit processor\nresources, leaving the system underutilized. Now consider the x86_64\npsABI. This ABI is newer and uses 64-bits for data sizes and program\npointers. The extra bits increase the footprint size of the programs,\nlibraries, and also increases the memory and file system size\nrequirements. Executing under the x32 psABI enables user programs to\nutilize CPU and system resources more efficiently while keeping the\nmemory footprint of the applications low. Extra bits are used for\nregisters but not for addressing mechanisms.\nThe Yocto Project supports the final specifications of x32 psABI as\nfollows:\n-  You can create packages and images in x32 psABI format on x86_64\narchitecture targets.\n-  You can successfully build recipes with the x32 toolchain.\n-  You can create and boot ``core-image-minimal`` and\n``core-image-sato`` images.\n-  There is RPM Package Manager (RPM) support for x32 binaries.\n-  There is support for large images.\nTo use the x32 psABI, you need to edit your ``conf/local.conf``\nconfiguration file as follows::\nMACHINE = \"qemux86-64\"\nDEFAULTTUNE = \"x86-64-x32\"\nbaselib = \"${@d.getVar('BASE_LIB:tune-' + (d.getVar('DEFAULTTUNE') \\\nor 'INVALID')) or 'lib'}\"\nOnce you have set\nup your configuration file, use BitBake to build an image that supports\nthe x32 psABI. Here is an example::\n$ bitbake core-image-sato"}
{"text": "\nUsing a Python Development Shell\n********************************\nSimilar to working within a development shell as described in the\nprevious section, you can also spawn and work within an interactive\nPython development shell. When debugging certain commands or even when\njust editing packages, ``pydevshell`` can be a useful tool. When you\ninvoke the ``pydevshell`` task, all tasks up to and including\n:ref:`ref-tasks-patch` are run for the\nspecified target. Then a new terminal is opened. Additionally, key\nPython objects and code are available in the same way they are to\nBitBake tasks, in particular, the data store 'd'. So, commands such as\nthe following are useful when exploring the data store and running\nfunctions::\npydevshell> d.getVar(\"STAGING_DIR\")\n'/media/build1/poky/build/tmp/sysroots'\npydevshell> d.getVar(\"STAGING_DIR\", False)\n'${TMPDIR}/sysroots'\npydevshell> d.setVar(\"FOO\", \"bar\")\npydevshell> d.getVar(\"FOO\")\n'bar'\npydevshell> d.delVar(\"FOO\")\npydevshell> d.getVar(\"FOO\")\npydevshell> bb.build.exec_func(\"do_unpack\", d)\npydevshell>\nSee the \":ref:`bitbake-user-manual/bitbake-user-manual-metadata:functions you can call from within python`\"\nsection in the BitBake User Manual for details about available functions.\nThe commands execute just as if the OpenEmbedded build\nsystem were executing them. Consequently, working this way can be\nhelpful when debugging a build or preparing software to be used with the\nOpenEmbedded build system.\nFollowing is an example that uses ``pydevshell`` on a target named\n``matchbox-desktop``::\n$ bitbake matchbox-desktop -c pydevshell\nThis command spawns a terminal and places you in an interactive Python\ninterpreter within the OpenEmbedded build environment. The\n:term:`OE_TERMINAL` variable\ncontrols what type of shell is opened.\nWhen you are finished using ``pydevshell``, you can exit the shell\neither by using Ctrl+d or closing the terminal window."}
{"text": "\nWorking With Libraries\n**********************\nLibraries are an integral part of your system. This section describes\nsome common practices you might find helpful when working with libraries\nto build your system:\n-  :ref:`How to include static library files\n<dev-manual/libraries:including static library files>`\n-  :ref:`How to use the Multilib feature to combine multiple versions of\nlibrary files into a single image\n<dev-manual/libraries:combining multiple versions of library files into one image>`\n-  :ref:`How to install multiple versions of the same library in parallel on\nthe same system\n<dev-manual/libraries:installing multiple versions of the same library>`\nIncluding Static Library Files\n==============================\nIf you are building a library and the library offers static linking, you\ncan control which static library files (``*.a`` files) get included in\nthe built library.\nThe :term:`PACKAGES` and\n:term:`FILES:* <FILES>` variables in the\n``meta/conf/bitbake.conf`` configuration file define how files installed\nby the :ref:`ref-tasks-install` task are packaged. By default, the :term:`PACKAGES`\nvariable includes ``${PN}-staticdev``, which represents all static\nlibrary files.\n.. note::\nSome previously released versions of the Yocto Project defined the\nstatic library files through ``${PN}-dev``.\nFollowing is part of the BitBake configuration file, where you can see\nhow the static library files are defined::\nPACKAGE_BEFORE_PN ?= \"\"\nPACKAGES = \"${PN}-src ${PN}-dbg ${PN}-staticdev ${PN}-dev ${PN}-doc ${PN}-locale ${PACKAGE_BEFORE_PN} ${PN}\"\nPACKAGES_DYNAMIC = \"^${PN}-locale-.*\"\nFILES = \"\"\nFILES:${PN} = \"${bindir}/* ${sbindir}/* ${libexecdir}/* ${libdir}/lib*${SOLIBS} \\\n${sysconfdir} ${sharedstatedir} ${localstatedir} \\\n${base_bindir}/* ${base_sbindir}/* \\\n${base_libdir}/*${SOLIBS} \\\n${base_prefix}/lib/udev ${prefix}/lib/udev \\\n${base_libdir}/udev ${libdir}/udev \\\n${datadir}/${BPN} ${libdir}/${BPN}/* \\\n${datadir}/pixmaps ${datadir}/applications \\\n${datadir}/idl ${datadir}/omf ${datadir}/sounds \\\n${libdir}/bonobo/servers\"\nFILES:${PN}-bin = \"${bindir}/* ${sbindir}/*\"\nFILES:${PN}-doc = \"${docdir} ${mandir} ${infodir} ${datadir}/gtk-doc \\\n${datadir}/gnome/help\"\nSECTION:${PN}-doc = \"doc\"\nFILES_SOLIBSDEV ?= \"${base_libdir}/lib*${SOLIBSDEV} ${libdir}/lib*${SOLIBSDEV}\"\nFILES:${PN}-dev = \"${includedir} ${FILES_SOLIBSDEV} ${libdir}/*.la \\\n${libdir}/*.o ${libdir}/pkgconfig ${datadir}/pkgconfig \\\n${datadir}/aclocal ${base_libdir}/*.o \\\n${libdir}/${BPN}/*.la ${base_libdir}/*.la \\\n${libdir}/cmake ${datadir}/cmake\"\nSECTION:${PN}-dev = \"devel\"\nALLOW_EMPTY:${PN}-dev = \"1\"\nRDEPENDS:${PN}-dev = \"${PN} (= ${EXTENDPKGV})\"\nFILES:${PN}-staticdev = \"${libdir}/*.a ${base_libdir}/*.a ${libdir}/${BPN}/*.a\"\nSECTION:${PN}-staticdev = \"devel\"\nRDEPENDS:${PN}-staticdev = \"${PN}-dev (= ${EXTENDPKGV})\"\nCombining Multiple Versions of Library Files into One Image\n===========================================================\nThe build system offers the ability to build libraries with different\ntarget optimizations or architecture formats and combine these together\ninto one system image. You can link different binaries in the image\nagainst the different libraries as needed for specific use cases. This\nfeature is called \"Multilib\".\nAn example would be where you have most of a system compiled in 32-bit\nmode using 32-bit libraries, but you have something large, like a\ndatabase engine, that needs to be a 64-bit application and uses 64-bit\nlibraries. Multilib allows you to get the best of both 32-bit and 64-bit\nlibraries.\nWhile the Multilib feature is most commonly used for 32 and 64-bit\ndifferences, the approach the build system uses facilitates different\ntarget optimizations. You could compile some binaries to use one set of\nlibraries and other binaries to use a different set of libraries. The\nlibraries could differ in architecture, compiler options, or other\noptimizations.\nThere are several examples in the ``meta-skeleton`` layer found in the\n:term:`Source Directory`:\n-  :oe_git:`conf/multilib-example.conf </openembedded-core/tree/meta-skeleton/conf/multilib-example.conf>`\nconfiguration file.\n-  :oe_git:`conf/multilib-example2.conf </openembedded-core/tree/meta-skeleton/conf/multilib-example2.conf>`\nconfiguration file.\n-  :oe_git:`recipes-multilib/images/core-image-multilib-example.bb </openembedded-core/tree/meta-skeleton/recipes-multilib/images/core-image-multilib-example.bb>`\nrecipe\nPreparing to Use Multilib\n-------------------------\nUser-specific requirements drive the Multilib feature. Consequently,\nthere is no one \"out-of-the-box\" configuration that would\nmeet your needs.\nIn order to enable Multilib, you first need to ensure your recipe is\nextended to support multiple libraries. Many standard recipes are\nalready extended and support multiple libraries. You can check in the\n``meta/conf/multilib.conf`` configuration file in the\n:term:`Source Directory` to see how this is\ndone using the\n:term:`BBCLASSEXTEND` variable.\nEventually, all recipes will be covered and this list will not be\nneeded."}
{"text": "\nFor the most part, the :ref:`Multilib <ref-classes-multilib*>`\nclass extension works automatically to\nextend the package name from ``${PN}`` to ``${MLPREFIX}${PN}``, where\n:term:`MLPREFIX` is the particular multilib (e.g. \"lib32-\" or \"lib64-\").\nStandard variables such as\n:term:`DEPENDS`,\n:term:`RDEPENDS`,\n:term:`RPROVIDES`,\n:term:`RRECOMMENDS`,\n:term:`PACKAGES`, and\n:term:`PACKAGES_DYNAMIC` are\nautomatically extended by the system. If you are extending any manual\ncode in the recipe, you can use the ``${MLPREFIX}`` variable to ensure\nthose names are extended correctly.\nUsing Multilib\n--------------\nAfter you have set up the recipes, you need to define the actual\ncombination of multiple libraries you want to build. You accomplish this\nthrough your ``local.conf`` configuration file in the\n:term:`Build Directory`. An example configuration would be as follows::\nMACHINE = \"qemux86-64\"\nrequire conf/multilib.conf\nMULTILIBS = \"multilib:lib32\"\nDEFAULTTUNE:virtclass-multilib-lib32 = \"x86\"\nIMAGE_INSTALL:append = \" lib32-glib-2.0\"\nThis example enables an additional library named\n``lib32`` alongside the normal target packages. When combining these\n\"lib32\" alternatives, the example uses \"x86\" for tuning. For information\non this particular tuning, see\n``meta/conf/machine/include/ia32/arch-ia32.inc``.\nThe example then includes ``lib32-glib-2.0`` in all the images, which\nillustrates one method of including a multiple library dependency. You\ncan use a normal image build to include this dependency, for example::\n$ bitbake core-image-sato\nYou can also build Multilib packages\nspecifically with a command like this::\n$ bitbake lib32-glib-2.0\nAdditional Implementation Details\n---------------------------------\nThere are generic implementation details as well as details that are specific to\npackage management systems. Following are implementation details\nthat exist regardless of the package management system:\n-  The typical convention used for the class extension code as used by\nMultilib assumes that all package names specified in\n:term:`PACKAGES` that contain\n``${PN}`` have ``${PN}`` at the start of the name. When that\nconvention is not followed and ``${PN}`` appears at the middle or the\nend of a name, problems occur.\n-  The :term:`TARGET_VENDOR`\nvalue under Multilib will be extended to \"-vendormlmultilib\" (e.g.\n\"-pokymllib32\" for a \"lib32\" Multilib with Poky). The reason for this\nslightly unwieldy contraction is that any \"-\" characters in the\nvendor string presently break Autoconf's ``config.sub``, and other\nseparators are problematic for different reasons.\nHere are the implementation details for the RPM Package Management System:\n-  A unique architecture is defined for the Multilib packages, along\nwith creating a unique deploy folder under ``tmp/deploy/rpm`` in the\n:term:`Build Directory`. For example, consider ``lib32`` in a\n``qemux86-64`` image. The possible architectures in the system are \"all\",\n\"qemux86_64\", \"lib32:qemux86_64\", and \"lib32:x86\".\n-  The ``${MLPREFIX}`` variable is stripped from ``${PN}`` during RPM\npackaging. The naming for a normal RPM package and a Multilib RPM\npackage in a ``qemux86-64`` system resolves to something similar to\n``bash-4.1-r2.x86_64.rpm`` and ``bash-4.1.r2.lib32_x86.rpm``,\nrespectively.\n-  When installing a Multilib image, the RPM backend first installs the\nbase image and then installs the Multilib libraries.\n-  The build system relies on RPM to resolve the identical files in the\ntwo (or more) Multilib packages.\nHere are the implementation details for the IPK Package Management System:\n-  The ``${MLPREFIX}`` is not stripped from ``${PN}`` during IPK\npackaging. The naming for a normal RPM package and a Multilib IPK\npackage in a ``qemux86-64`` system resolves to something like\n``bash_4.1-r2.x86_64.ipk`` and ``lib32-bash_4.1-rw:x86.ipk``,\nrespectively.\n-  The IPK deploy folder is not modified with ``${MLPREFIX}`` because\npackages with and without the Multilib feature can exist in the same\nfolder due to the ``${PN}`` differences.\n-  IPK defines a sanity check for Multilib installation using certain\nrules for file comparison, overridden, etc.\nInstalling Multiple Versions of the Same Library\n================================================\nThere are be situations where you need to install and use multiple versions\nof the same library on the same system at the same time. This\nalmost always happens when a library API changes and you have\nmultiple pieces of software that depend on the separate versions of the\nlibrary. To accommodate these situations, you can install multiple\nversions of the same library in parallel on the same system.\nThe process is straightforward as long as the libraries use proper\nversioning. With properly versioned libraries, all you need to do to\nindividually specify the libraries is create separate, appropriately\nnamed recipes where the :term:`PN` part of\nthe name includes a portion that differentiates each library version\n(e.g. the major part of the version number). Thus, instead of having a\nsingle recipe that loads one version of a library (e.g. ``clutter``),\nyou provide multiple recipes that result in different versions of the\nlibraries you want. As an example, the following two recipes would allow\nthe two separate versions of the ``clutter`` library to co-exist on the\nsame system:\n.. code-block:: none"}
{"text": "Working With Libraries\nclutter-1.6_1.6.20.bb\nclutter-1.8_1.8.4.bb\nAdditionally, if\nyou have other recipes that depend on a given library, you need to use\nthe :term:`DEPENDS` variable to\ncreate the dependency. Continuing with the same example, if you want to\nhave a recipe depend on the 1.8 version of the ``clutter`` library, use\nthe following in your recipe::\nDEPENDS = \"clutter-1.8\""}
{"text": "\nMaking Images More Secure\n*************************\nSecurity is of increasing concern for embedded devices. Consider the\nissues and problems discussed in just this sampling of work found across\nthe Internet:\n-  *\"*\\ `Security Risks of Embedded\nSystems <https://www.schneier.com/blog/archives/2014/01/security_risks_9.html>`__\\ *\"*\nby Bruce Schneier\n-  *\"*\\ `Internet Census\n2012 <http://census2012.sourceforge.net/paper.html>`__\\ *\"* by Carna\nBotnet\n-  *\"*\\ `Security Issues for Embedded\nDevices <https://elinux.org/images/6/6f/Security-issues.pdf>`__\\ *\"*\nby Jake Edge\nWhen securing your image is of concern, there are steps, tools, and\nvariables that you can consider to help you reach the security goals you\nneed for your particular device. Not all situations are identical when\nit comes to making an image secure. Consequently, this section provides\nsome guidance and suggestions for consideration when you want to make\nyour image more secure.\n.. note::\nBecause the security requirements and risks are different for every\ntype of device, this section cannot provide a complete reference on\nsecuring your custom OS. It is strongly recommended that you also\nconsult other sources of information on embedded Linux system\nhardening and on security.\nGeneral Considerations\n======================\nThere are general considerations that help you create more secure images.\nYou should consider the following suggestions to make your device\nmore secure:\n-  Scan additional code you are adding to the system (e.g. application\ncode) by using static analysis tools. Look for buffer overflows and\nother potential security problems.\n-  Pay particular attention to the security for any web-based\nadministration interface.\nWeb interfaces typically need to perform administrative functions and\ntend to need to run with elevated privileges. Thus, the consequences\nresulting from the interface's security becoming compromised can be\nserious. Look for common web vulnerabilities such as\ncross-site-scripting (XSS), unvalidated inputs, and so forth.\nAs with system passwords, the default credentials for accessing a\nweb-based interface should not be the same across all devices. This\nis particularly true if the interface is enabled by default as it can\nbe assumed that many end-users will not change the credentials.\n-  Ensure you can update the software on the device to mitigate\nvulnerabilities discovered in the future. This consideration\nespecially applies when your device is network-enabled.\n-  Regularly scan and apply fixes for CVE security issues affecting\nall software components in the product, see \":ref:`dev-manual/vulnerabilities:checking for vulnerabilities`\".\n-  Regularly update your version of Poky and OE-Core from their upstream\ndevelopers, e.g. to apply updates and security fixes from stable\nand :term:`LTS` branches.\n-  Ensure you remove or disable debugging functionality before producing\nthe final image. For information on how to do this, see the\n\":ref:`dev-manual/securing-images:considerations specific to the openembedded build system`\"\nsection.\n-  Ensure you have no network services listening that are not needed.\n-  Remove any software from the image that is not needed.\n-  Enable hardware support for secure boot functionality when your\ndevice supports this functionality.\nSecurity Flags\n==============\nThe Yocto Project has security flags that you can enable that help make\nyour build output more secure. The security flags are in the\n``meta/conf/distro/include/security_flags.inc`` file in your\n:term:`Source Directory` (e.g. ``poky``).\n.. note::\nDepending on the recipe, certain security flags are enabled and\ndisabled by default.\nUse the following line in your ``local.conf`` file or in your custom\ndistribution configuration file to enable the security compiler and\nlinker flags for your build::\nrequire conf/distro/include/security_flags.inc\nConsiderations Specific to the OpenEmbedded Build System\n========================================================\nYou can take some steps that are specific to the OpenEmbedded build\nsystem to make your images more secure:\n-  Ensure \"debug-tweaks\" is not one of your selected\n:term:`IMAGE_FEATURES`.\nWhen creating a new project, the default is to provide you with an\ninitial ``local.conf`` file that enables this feature using the\n:term:`EXTRA_IMAGE_FEATURES`\nvariable with the line::\nEXTRA_IMAGE_FEATURES = \"debug-tweaks\"\nTo disable that feature, simply comment out that line in your\n``local.conf`` file, or make sure :term:`IMAGE_FEATURES` does not contain\n\"debug-tweaks\" before producing your final image. Among other things,\nleaving this in place sets the root password as blank, which makes\nlogging in for debugging or inspection easy during development but\nalso means anyone can easily log in during production.\n-  It is possible to set a root password for the image and also to set\npasswords for any extra users you might add (e.g. administrative or\nservice type users). When you set up passwords for multiple images or\nusers, you should not duplicate passwords.\nTo set up passwords, use the :ref:`ref-classes-extrausers` class, which\nis the preferred method. For an example on how to set up both root and\nuser passwords, see the \":ref:`ref-classes-extrausers`\" section.\n.. note::"}
{"text": "Making Images More Secure\nWhen adding extra user accounts or setting a root password, be\ncautious about setting the same password on every device. If you\ndo this, and the password you have set is exposed, then every\ndevice is now potentially compromised. If you need this access but\nwant to ensure security, consider setting a different, random\npassword for each device. Typically, you do this as a separate\nstep after you deploy the image onto the device.\n-  Consider enabling a Mandatory Access Control (MAC) framework such as\nSMACK or SELinux and tuning it appropriately for your device's usage.\nYou can find more information in the\n:yocto_git:`meta-selinux </meta-selinux/>` layer.\nTools for Hardening Your Image\n==============================\nThe Yocto Project provides tools for making your image more secure. You\ncan find these tools in the ``meta-security`` layer of the\n:yocto_git:`Yocto Project Source Repositories <>`."}
{"text": "\nAdding a New Machine\n********************\nAdding a new machine to the Yocto Project is a straightforward process.\nThis section describes how to add machines that are similar to those\nthat the Yocto Project already supports.\n.. note::\nAlthough well within the capabilities of the Yocto Project, adding a\ntotally new architecture might require changes to ``gcc``/``glibc``\nand to the site information, which is beyond the scope of this\nmanual.\nFor a complete example that shows how to add a new machine, see the\n\":ref:`bsp-guide/bsp:creating a new bsp layer using the \\`\\`bitbake-layers\\`\\` script`\"\nsection in the Yocto Project Board Support Package (BSP) Developer's\nGuide.\nAdding the Machine Configuration File\n=====================================\nTo add a new machine, you need to add a new machine configuration file\nto the layer's ``conf/machine`` directory. This configuration file\nprovides details about the device you are adding.\nThe OpenEmbedded build system uses the root name of the machine\nconfiguration file to reference the new machine. For example, given a\nmachine configuration file named ``crownbay.conf``, the build system\nrecognizes the machine as \"crownbay\".\nThe most important variables you must set in your machine configuration\nfile or include from a lower-level configuration file are as follows:\n-  :term:`TARGET_ARCH` (e.g. \"arm\")\n-  ``PREFERRED_PROVIDER_virtual/kernel``\n-  :term:`MACHINE_FEATURES` (e.g. \"screen wifi\")\nYou might also need these variables:\n-  :term:`SERIAL_CONSOLES` (e.g. \"115200;ttyS0 115200;ttyS1\")\n-  :term:`KERNEL_IMAGETYPE` (e.g. \"zImage\")\n-  :term:`IMAGE_FSTYPES` (e.g. \"tar.gz jffs2\")\nYou can find full details on these variables in the reference section.\nYou can leverage existing machine ``.conf`` files from\n``meta-yocto-bsp/conf/machine/``.\nAdding a Kernel for the Machine\n===============================\nThe OpenEmbedded build system needs to be able to build a kernel for the\nmachine. You need to either create a new kernel recipe for this machine,\nor extend an existing kernel recipe. You can find several kernel recipe\nexamples in the Source Directory at ``meta/recipes-kernel/linux`` that\nyou can use as references.\nIf you are creating a new kernel recipe, normal recipe-writing rules\napply for setting up a :term:`SRC_URI`. Thus, you need to specify any\nnecessary patches and set :term:`S` to point at the source code. You need to\ncreate a :ref:`ref-tasks-configure` task that configures the unpacked kernel with\na ``defconfig`` file. You can do this by using a ``make defconfig``\ncommand or, more commonly, by copying in a suitable ``defconfig`` file\nand then running ``make oldconfig``. By making use of ``inherit kernel``\nand potentially some of the ``linux-*.inc`` files, most other\nfunctionality is centralized and the defaults of the class normally work\nwell.\nIf you are extending an existing kernel recipe, it is usually a matter\nof adding a suitable ``defconfig`` file. The file needs to be added into\na location similar to ``defconfig`` files used for other machines in a\ngiven kernel recipe. A possible way to do this is by listing the file in\nthe :term:`SRC_URI` and adding the machine to the expression in\n:term:`COMPATIBLE_MACHINE`::\nCOMPATIBLE_MACHINE = '(qemux86|qemumips)'\nFor more information on ``defconfig`` files, see the\n\":ref:`kernel-dev/common:changing the configuration`\"\nsection in the Yocto Project Linux Kernel Development Manual.\nAdding a Formfactor Configuration File\n======================================\nA formfactor configuration file provides information about the target\nhardware for which the image is being built and information that the\nbuild system cannot obtain from other sources such as the kernel. Some\nexamples of information contained in a formfactor configuration file\ninclude framebuffer orientation, whether or not the system has a\nkeyboard, the positioning of the keyboard in relation to the screen, and\nthe screen resolution.\nThe build system uses reasonable defaults in most cases. However, if\ncustomization is necessary, you need to create a ``machconfig`` file in\nthe ``meta/recipes-bsp/formfactor/files`` directory. This directory\ncontains directories for specific machines such as ``qemuarm`` and\n``qemux86``. For information about the settings available and the\ndefaults, see the ``meta/recipes-bsp/formfactor/files/config`` file\nfound in the same area.\nFollowing is an example for \"qemuarm\" machine::\nHAVE_TOUCHSCREEN=1\nHAVE_KEYBOARD=1\nDISPLAY_CAN_ROTATE=0\nDISPLAY_ORIENTATION=0\n#DISPLAY_WIDTH_PIXELS=640\n#DISPLAY_HEIGHT_PIXELS=480\n#DISPLAY_BPP=16\nDISPLAY_DPI=150\nDISPLAY_SUBPIXEL_ORDER=vrgb"}
{"text": "\nConserving Disk Space\n*********************\nConserving Disk Space During Builds\n===================================\nTo help conserve disk space during builds, you can add the following\nstatement to your project's ``local.conf`` configuration file found in\nthe :term:`Build Directory`::\nINHERIT += \"rm_work\"\nAdding this statement deletes the work directory used for\nbuilding a recipe once the recipe is built. For more information on\n\"rm_work\", see the :ref:`ref-classes-rm-work` class in the\nYocto Project Reference Manual.\nWhen you inherit this class and build a ``core-image-sato`` image for a\n``qemux86-64`` machine from an Ubuntu 22.04 x86-64 system, you end up with a\nfinal disk usage of 22 Gbytes instead of &MIN_DISK_SPACE; Gbytes. However,\n&MIN_DISK_SPACE_RM_WORK; Gbytes of initial free disk space are still needed to\ncreate temporary files before they can be deleted.\nPurging Obsolete Shared State Cache Files\n=========================================\nAfter multiple build iterations, the Shared State (sstate) cache can contain\nmultiple cache files for a given package, consuming a substantial amount of\ndisk space. However, only the most recent ones are likely to be reused.\nThe following command is a quick way to purge all the cache files which\nhaven't been used for a least a specified number of days::\nfind build/sstate-cache -type f -mtime +$DAYS -delete\nThe above command relies on the fact that BitBake touches the sstate cache\nfiles as it accesses them, when it has write access to the cache.\nYou could use ``-atime`` instead of ``-mtime`` if the partition isn't mounted\nwith the ``noatime`` option for a read only cache.\nFor more advanced needs, OpenEmbedded-Core also offers a more elaborate\ncommand. It has the ability to purge all but the newest cache files on each\narchitecture, and also to remove files that it considers unreachable by\nexploring a set of build configurations. However, this command\nrequires a full build environment to be available and doesn't work well\ncovering multiple releases. It won't work either on limited environments\nsuch as BSD based NAS::\nsstate-cache-management.sh --remove-duplicated --cache-dir=build/sstate-cache\nThis command will ask you to confirm the deletions it identifies.\nRun ``sstate-cache-management.sh`` for more details about this script.\n.. note::\nAs this command is much more cautious and selective, removing only cache files,\nit will execute much slower than the simple ``find`` command described above.\nTherefore, it may not be your best option to trim huge cache directories."}
{"text": "\nDebugging Tools and Techniques\n******************************\nThe exact method for debugging build failures depends on the nature of\nthe problem and on the system's area from which the bug originates.\nStandard debugging practices such as comparison against the last known\nworking version with examination of the changes and the re-application\nof steps to identify the one causing the problem are valid for the Yocto\nProject just as they are for any other system. Even though it is\nimpossible to detail every possible potential failure, this section\nprovides some general tips to aid in debugging given a variety of\nsituations.\n.. note::\nA useful feature for debugging is the error reporting tool.\nConfiguring the Yocto Project to use this tool causes the\nOpenEmbedded build system to produce error reporting commands as part\nof the console output. You can enter the commands after the build\ncompletes to log error information into a common database, that can\nhelp you figure out what might be going wrong. For information on how\nto enable and use this feature, see the\n\":ref:`dev-manual/error-reporting-tool:using the error reporting tool`\"\nsection.\nThe following list shows the debugging topics in the remainder of this\nsection:\n-  \":ref:`dev-manual/debugging:viewing logs from failed tasks`\" describes\nhow to find and view logs from tasks that failed during the build\nprocess.\n-  \":ref:`dev-manual/debugging:viewing variable values`\" describes how to\nuse the BitBake ``-e`` option to examine variable values after a\nrecipe has been parsed.\n-  \":ref:`dev-manual/debugging:viewing package information with \\`\\`oe-pkgdata-util\\`\\``\"\ndescribes how to use the ``oe-pkgdata-util`` utility to query\n:term:`PKGDATA_DIR` and\ndisplay package-related information for built packages.\n-  \":ref:`dev-manual/debugging:viewing dependencies between recipes and tasks`\"\ndescribes how to use the BitBake ``-g`` option to display recipe\ndependency information used during the build.\n-  \":ref:`dev-manual/debugging:viewing task variable dependencies`\" describes\nhow to use the ``bitbake-dumpsig`` command in conjunction with key\nsubdirectories in the :term:`Build Directory` to determine variable\ndependencies.\n-  \":ref:`dev-manual/debugging:running specific tasks`\" describes\nhow to use several BitBake options (e.g. ``-c``, ``-C``, and ``-f``)\nto run specific tasks in the build chain. It can be useful to run\ntasks \"out-of-order\" when trying isolate build issues.\n-  \":ref:`dev-manual/debugging:general BitBake problems`\" describes how\nto use BitBake's ``-D`` debug output option to reveal more about what\nBitBake is doing during the build.\n-  \":ref:`dev-manual/debugging:building with no dependencies`\"\ndescribes how to use the BitBake ``-b`` option to build a recipe\nwhile ignoring dependencies.\n-  \":ref:`dev-manual/debugging:recipe logging mechanisms`\"\ndescribes how to use the many recipe logging functions to produce\ndebugging output and report errors and warnings.\n-  \":ref:`dev-manual/debugging:debugging parallel make races`\"\ndescribes how to debug situations where the build consists of several\nparts that are run simultaneously and when the output or result of\none part is not ready for use with a different part of the build that\ndepends on that output.\n-  \":ref:`dev-manual/debugging:debugging with the gnu project debugger (gdb) remotely`\"\ndescribes how to use GDB to allow you to examine running programs, which can\nhelp you fix problems.\n-  \":ref:`dev-manual/debugging:debugging with the gnu project debugger (gdb) on the target`\"\ndescribes how to use GDB directly on target hardware for debugging.\n-  \":ref:`dev-manual/debugging:other debugging tips`\" describes\nmiscellaneous debugging tips that can be useful.\nViewing Logs from Failed Tasks\n==============================\nYou can find the log for a task in the file\n``${``\\ :term:`WORKDIR`\\ ``}/temp/log.do_``\\ `taskname`.\nFor example, the log for the\n:ref:`ref-tasks-compile` task of the\nQEMU minimal image for the x86 machine (``qemux86``) might be in\n``tmp/work/qemux86-poky-linux/core-image-minimal/1.0-r0/temp/log.do_compile``.\nTo see the commands :term:`BitBake` ran\nto generate a log, look at the corresponding ``run.do_``\\ `taskname` file\nin the same directory.\n``log.do_``\\ `taskname` and ``run.do_``\\ `taskname` are actually symbolic\nlinks to ``log.do_``\\ `taskname`\\ ``.``\\ `pid` and\n``log.run_``\\ `taskname`\\ ``.``\\ `pid`, where `pid` is the PID the task had\nwhen it ran. The symlinks always point to the files corresponding to the\nmost recent run.\nViewing Variable Values\n=======================\nSometimes you need to know the value of a variable as a result of\nBitBake's parsing step. This could be because some unexpected behavior\noccurred in your project. Perhaps an attempt to :ref:`modify a variable\n<bitbake-user-manual/bitbake-user-manual-metadata:modifying existing\nvariables>` did not work out as expected.\nBitBake's ``-e`` option is used to display variable values after\nparsing. The following command displays the variable values after the\nconfiguration files (i.e. ``local.conf``, ``bblayers.conf``,\n``bitbake.conf`` and so forth) have been parsed::\n$ bitbake -e\nThe following command displays variable values after a specific recipe has\nbeen parsed. The variables include those from the configuration as well::\n$ bitbake -e recipename\n.. note::\nEach recipe has its own private set of variables (datastore).\nInternally, after parsing the configuration, a copy of the resulting"}
{"text": "\ndatastore is made prior to parsing each recipe. This copying implies\nthat variables set in one recipe will not be visible to other\nrecipes.\nLikewise, each task within a recipe gets a private datastore based on\nthe recipe datastore, which means that variables set within one task\nwill not be visible to other tasks.\nIn the output of ``bitbake -e``, each variable is preceded by a\ndescription of how the variable got its value, including temporary\nvalues that were later overridden. This description also includes\nvariable flags (varflags) set on the variable. The output can be very\nhelpful during debugging.\nVariables that are exported to the environment are preceded by\n``export`` in the output of ``bitbake -e``. See the following example::\nexport CC=\"i586-poky-linux-gcc -m32 -march=i586 --sysroot=/home/ulf/poky/build/tmp/sysroots/qemux86\"\nIn addition to variable values, the output of the ``bitbake -e`` and\n``bitbake -e`` recipe commands includes the following information:\n-  The output starts with a tree listing all configuration files and\nclasses included globally, recursively listing the files they include\nor inherit in turn. Much of the behavior of the OpenEmbedded build\nsystem (including the behavior of the :ref:`ref-manual/tasks:normal recipe build tasks`) is\nimplemented in the :ref:`ref-classes-base` class and the\nclasses it inherits, rather than being built into BitBake itself.\n-  After the variable values, all functions appear in the output. For\nshell functions, variables referenced within the function body are\nexpanded. If a function has been modified using overrides or using\noverride-style operators like ``:append`` and ``:prepend``, then the\nfinal assembled function body appears in the output.\nViewing Package Information with ``oe-pkgdata-util``\n====================================================\nYou can use the ``oe-pkgdata-util`` command-line utility to query\n:term:`PKGDATA_DIR` and display\nvarious package-related information. When you use the utility, you must\nuse it to view information on packages that have already been built.\nFollowing are a few of the available ``oe-pkgdata-util`` subcommands.\n.. note::\nYou can use the standard \\* and ? globbing wildcards as part of\npackage names and paths.\n-  ``oe-pkgdata-util list-pkgs [pattern]``: Lists all packages\nthat have been built, optionally limiting the match to packages that\nmatch pattern.\n-  ``oe-pkgdata-util list-pkg-files package ...``: Lists the\nfiles and directories contained in the given packages.\n.. note::\nA different way to view the contents of a package is to look at\nthe\n``${``\\ :term:`WORKDIR`\\ ``}/packages-split``\ndirectory of the recipe that generates the package. This directory\nis created by the\n:ref:`ref-tasks-package` task\nand has one subdirectory for each package the recipe generates,\nwhich contains the files stored in that package.\nIf you want to inspect the ``${WORKDIR}/packages-split``\ndirectory, make sure that :ref:`ref-classes-rm-work` is not\nenabled when you build the recipe.\n-  ``oe-pkgdata-util find-path path ...``: Lists the names of\nthe packages that contain the given paths. For example, the following\ntells us that ``/usr/share/man/man1/make.1`` is contained in the\n``make-doc`` package::\n$ oe-pkgdata-util find-path /usr/share/man/man1/make.1\nmake-doc: /usr/share/man/man1/make.1\n-  ``oe-pkgdata-util lookup-recipe package ...``: Lists the name\nof the recipes that produce the given packages.\nFor more information on the ``oe-pkgdata-util`` command, use the help\nfacility::\n$ oe-pkgdata-util --help\n$ oe-pkgdata-util subcommand --help\nViewing Dependencies Between Recipes and Tasks\n==============================================\nSometimes it can be hard to see why BitBake wants to build other recipes\nbefore the one you have specified. Dependency information can help you\nunderstand why a recipe is built.\nTo generate dependency information for a recipe, run the following\ncommand::\n$ bitbake -g recipename\nThis command writes the following files in the current directory:\n-  ``pn-buildlist``: A list of recipes/targets involved in building\n`recipename`. \"Involved\" here means that at least one task from the\nrecipe needs to run when building `recipename` from scratch. Targets\nthat are in\n:term:`ASSUME_PROVIDED`\nare not listed.\n-  ``task-depends.dot``: A graph showing dependencies between tasks.\nThe graphs are in :wikipedia:`DOT <DOT_%28graph_description_language%29>`\nformat and can be converted to images (e.g. using the ``dot`` tool from\n`Graphviz <https://www.graphviz.org/>`__).\n.. note::\n-  DOT files use a plain text format. The graphs generated using the\n``bitbake -g`` command are often so large as to be difficult to\nread without special pruning (e.g. with BitBake's ``-I`` option)\nand processing. Despite the form and size of the graphs, the\ncorresponding ``.dot`` files can still be possible to read and\nprovide useful information.\nAs an example, the ``task-depends.dot`` file contains lines such\nas the following::\n\"libxslt.do_configure\" -> \"libxml2.do_populate_sysroot\"\nThe above example line reveals that the\n:ref:`ref-tasks-configure`\ntask in ``libxslt`` depends on the\n:ref:`ref-tasks-populate_sysroot`\ntask in ``libxml2``, which is a normal"}
{"text": "\n:term:`DEPENDS` dependency\nbetween the two recipes.\n-  For an example of how ``.dot`` files can be processed, see the\n``scripts/contrib/graph-tool`` Python script, which finds and\ndisplays paths between graph nodes.\nYou can use a different method to view dependency information by using\nthe following command::\n$ bitbake -g -u taskexp recipename\nThis command\ndisplays a GUI window from which you can view build-time and runtime\ndependencies for the recipes involved in building recipename.\nViewing Task Variable Dependencies\n==================================\nAs mentioned in the\n\":ref:`bitbake-user-manual/bitbake-user-manual-execution:checksums (signatures)`\"\nsection of the BitBake User Manual, BitBake tries to automatically determine\nwhat variables a task depends on so that it can rerun the task if any values of\nthe variables change. This determination is usually reliable. However, if you\ndo things like construct variable names at runtime, then you might have to\nmanually declare dependencies on those variables using ``vardeps`` as described\nin the \":ref:`bitbake-user-manual/bitbake-user-manual-metadata:variable flags`\"\nsection of the BitBake User Manual.\nIf you are unsure whether a variable dependency is being picked up\nautomatically for a given task, you can list the variable dependencies\nBitBake has determined by doing the following:\n#. Build the recipe containing the task::\n$ bitbake recipename\n#. Inside the :term:`STAMPS_DIR`\ndirectory, find the signature data (``sigdata``) file that\ncorresponds to the task. The ``sigdata`` files contain a pickled\nPython database of all the metadata that went into creating the input\nchecksum for the task. As an example, for the\n:ref:`ref-tasks-fetch` task of the\n``db`` recipe, the ``sigdata`` file might be found in the following\nlocation::\n${BUILDDIR}/tmp/stamps/i586-poky-linux/db/6.0.30-r1.do_fetch.sigdata.7c048c18222b16ff0bcee2000ef648b1\nFor tasks that are accelerated through the shared state\n(:ref:`sstate <overview-manual/concepts:shared state cache>`) cache, an\nadditional ``siginfo`` file is written into\n:term:`SSTATE_DIR` along with\nthe cached task output. The ``siginfo`` files contain exactly the\nsame information as ``sigdata`` files.\n#. Run ``bitbake-dumpsig`` on the ``sigdata`` or ``siginfo`` file. Here\nis an example::\n$ bitbake-dumpsig ${BUILDDIR}/tmp/stamps/i586-poky-linux/db/6.0.30-r1.do_fetch.sigdata.7c048c18222b16ff0bcee2000ef648b1\nIn the output of the above command, you will find a line like the\nfollowing, which lists all the (inferred) variable dependencies for\nthe task. This list also includes indirect dependencies from\nvariables depending on other variables, recursively::\nTask dependencies: ['PV', 'SRCREV', 'SRC_URI', 'SRC_URI[sha256sum]', 'base_do_fetch']\n.. note::\nFunctions (e.g. ``base_do_fetch``) also count as variable dependencies.\nThese functions in turn depend on the variables they reference.\nThe output of ``bitbake-dumpsig`` also includes the value each\nvariable had, a list of dependencies for each variable, and\n:term:`BB_BASEHASH_IGNORE_VARS`\ninformation.\nThere is also a ``bitbake-diffsigs`` command for comparing two\n``siginfo`` or ``sigdata`` files. This command can be helpful when\ntrying to figure out what changed between two versions of a task. If you\ncall ``bitbake-diffsigs`` with just one file, the command behaves like\n``bitbake-dumpsig``.\nYou can also use BitBake to dump out the signature construction\ninformation without executing tasks by using either of the following\nBitBake command-line options::\n‐‐dump-signatures=SIGNATURE_HANDLER\n-S SIGNATURE_HANDLER\n.. note::\nTwo common values for `SIGNATURE_HANDLER` are \"none\" and \"printdiff\", which\ndump only the signature or compare the dumped signature with the cached one,\nrespectively.\nUsing BitBake with either of these options causes BitBake to dump out\n``sigdata`` files in the ``stamps`` directory for every task it would\nhave executed instead of building the specified target package.\nViewing Metadata Used to Create the Input Signature of a Shared State Task\n==========================================================================\nSeeing what metadata went into creating the input signature of a shared\nstate (sstate) task can be a useful debugging aid. This information is\navailable in signature information (``siginfo``) files in\n:term:`SSTATE_DIR`. For\ninformation on how to view and interpret information in ``siginfo``\nfiles, see the\n\":ref:`dev-manual/debugging:viewing task variable dependencies`\" section.\nFor conceptual information on shared state, see the\n\":ref:`overview-manual/concepts:shared state`\"\nsection in the Yocto Project Overview and Concepts Manual.\nInvalidating Shared State to Force a Task to Run\n================================================\nThe OpenEmbedded build system uses\n:ref:`checksums <overview-manual/concepts:checksums (signatures)>` and\n:ref:`overview-manual/concepts:shared state` cache to avoid unnecessarily\nrebuilding tasks. Collectively, this scheme is known as \"shared state\ncode\".\nAs with all schemes, this one has some drawbacks. It is possible that\nyou could make implicit changes to your code that the checksum\ncalculations do not take into account. These implicit changes affect a\ntask's output but do not trigger the shared state code into rebuilding a\nrecipe. Consider an example during which a tool changes its output.\nAssume that the output of ``rpmdeps`` changes. The result of the change\nshould be that all the ``package`` and ``package_write_rpm`` shared"}
{"text": "\nstate cache items become invalid. However, because the change to the\noutput is external to the code and therefore implicit, the associated\nshared state cache items do not become invalidated. In this case, the\nbuild process uses the cached items rather than running the task again.\nObviously, these types of implicit changes can cause problems.\nTo avoid these problems during the build, you need to understand the\neffects of any changes you make. Realize that changes you make directly\nto a function are automatically factored into the checksum calculation.\nThus, these explicit changes invalidate the associated area of shared\nstate cache. However, you need to be aware of any implicit changes that\nare not obvious changes to the code and could affect the output of a\ngiven task.\nWhen you identify an implicit change, you can easily take steps to\ninvalidate the cache and force the tasks to run. The steps you can take\nare as simple as changing a function's comments in the source code. For\nexample, to invalidate package shared state files, change the comment\nstatements of\n:ref:`ref-tasks-package` or the\ncomments of one of the functions it calls. Even though the change is\npurely cosmetic, it causes the checksum to be recalculated and forces\nthe build system to run the task again.\n.. note::\nFor an example of a commit that makes a cosmetic change to invalidate\nshared state, see this\n:yocto_git:`commit </poky/commit/meta/classes/package.bbclass?id=737f8bbb4f27b4837047cb9b4fbfe01dfde36d54>`.\nRunning Specific Tasks\n======================\nAny given recipe consists of a set of tasks. The standard BitBake\nbehavior in most cases is: :ref:`ref-tasks-fetch`, :ref:`ref-tasks-unpack`, :ref:`ref-tasks-patch`,\n:ref:`ref-tasks-configure`, :ref:`ref-tasks-compile`, :ref:`ref-tasks-install`, :ref:`ref-tasks-package`,\n:ref:`do_package_write_* <ref-tasks-package_write_deb>`, and :ref:`ref-tasks-build`. The default task is\n:ref:`ref-tasks-build` and any tasks on which it depends build first. Some tasks,\nsuch as :ref:`ref-tasks-devshell`, are not part of the default build chain. If you\nwish to run a task that is not part of the default build chain, you can\nuse the ``-c`` option in BitBake. Here is an example::\n$ bitbake matchbox-desktop -c devshell\nThe ``-c`` option respects task dependencies, which means that all other\ntasks (including tasks from other recipes) that the specified task\ndepends on will be run before the task. Even when you manually specify a\ntask to run with ``-c``, BitBake will only run the task if it considers\nit \"out of date\". See the\n\":ref:`overview-manual/concepts:stamp files and the rerunning of tasks`\"\nsection in the Yocto Project Overview and Concepts Manual for how\nBitBake determines whether a task is \"out of date\".\nIf you want to force an up-to-date task to be rerun (e.g. because you\nmade manual modifications to the recipe's\n:term:`WORKDIR` that you want to try\nout), then you can use the ``-f`` option.\n.. note::\nThe reason ``-f`` is never required when running the\n:ref:`ref-tasks-devshell` task is because the\n[\\ :ref:`nostamp <bitbake-user-manual/bitbake-user-manual-metadata:variable flags>`\\ ]\nvariable flag is already set for the task.\nThe following example shows one way you can use the ``-f`` option::\n$ bitbake matchbox-desktop\n.\n.\nmake some changes to the source code in the work directory\n.\n.\n$ bitbake matchbox-desktop -c compile -f\n$ bitbake matchbox-desktop\nThis sequence first builds and then recompiles ``matchbox-desktop``. The\nlast command reruns all tasks (basically the packaging tasks) after the\ncompile. BitBake recognizes that the :ref:`ref-tasks-compile` task was rerun and\ntherefore understands that the other tasks also need to be run again.\nAnother, shorter way to rerun a task and all\n:ref:`ref-manual/tasks:normal recipe build tasks`\nthat depend on it is to use the ``-C`` option.\n.. note::\nThis option is upper-cased and is separate from the ``-c``\noption, which is lower-cased.\nUsing this option invalidates the given task and then runs the\n:ref:`ref-tasks-build` task, which is\nthe default task if no task is given, and the tasks on which it depends.\nYou could replace the final two commands in the previous example with\nthe following single command::\n$ bitbake matchbox-desktop -C compile\nInternally, the ``-f`` and ``-C`` options work by tainting (modifying)\nthe input checksum of the specified task. This tainting indirectly\ncauses the task and its dependent tasks to be rerun through the normal\ntask dependency mechanisms.\n.. note::\nBitBake explicitly keeps track of which tasks have been tainted in\nthis fashion, and will print warnings such as the following for\nbuilds involving such tasks:\n.. code-block:: none\nWARNING: /home/ulf/poky/meta/recipes-sato/matchbox-desktop/matchbox-desktop_2.1.bb.do_compile is tainted from a forced run\nThe purpose of the warning is to let you know that the work directory\nand build output might not be in the clean state they would be in for\na \"normal\" build, depending on what actions you took. To get rid of\nsuch warnings, you can remove the work directory and rebuild the\nrecipe, as follows::\n$ bitbake matchbox-desktop -c clean\n$ bitbake matchbox-desktop\nYou can view a list of tasks in a given package by running the\n:ref:`ref-tasks-listtasks` task as follows::\n$ bitbake matchbox-desktop -c listtasks\nThe results appear as output to the console and are also in\nthe file ``${WORKDIR}/temp/log.do_listtasks``."}
{"text": "\nGeneral BitBake Problems\n========================\nYou can see debug output from BitBake by using the ``-D`` option. The\ndebug output gives more information about what BitBake is doing and the\nreason behind it. Each ``-D`` option you use increases the logging\nlevel. The most common usage is ``-DDD``.\nThe output from ``bitbake -DDD -v targetname`` can reveal why BitBake\nchose a certain version of a package or why BitBake picked a certain\nprovider. This command could also help you in a situation where you\nthink BitBake did something unexpected.\nBuilding with No Dependencies\n=============================\nTo build a specific recipe (``.bb`` file), you can use the following\ncommand form::\n$ bitbake -b somepath/somerecipe.bb\nThis command form does\nnot check for dependencies. Consequently, you should use it only when\nyou know existing dependencies have been met.\n.. note::\nYou can also specify fragments of the filename. In this case, BitBake\nchecks for a unique match.\nRecipe Logging Mechanisms\n=========================\nThe Yocto Project provides several logging functions for producing\ndebugging output and reporting errors and warnings. For Python\nfunctions, the following logging functions are available. All of these functions\nlog to ``${T}/log.do_``\\ `task`, and can also log to standard output\n(stdout) with the right settings:\n-  ``bb.plain(msg)``: Writes msg as is to the log while also\nlogging to stdout.\n-  ``bb.note(msg)``: Writes \"NOTE: msg\" to the log. Also logs to\nstdout if BitBake is called with \"-v\".\n-  ``bb.debug(level, msg)``: Writes \"DEBUG: msg\" to the log. Also logs to\nstdout if the log level is greater than or equal to level. See the\n\":ref:`bitbake-user-manual/bitbake-user-manual-intro:usage and syntax`\"\noption in the BitBake User Manual for more information.\n-  ``bb.warn(msg)``: Writes \"WARNING: msg\" to the log while also\nlogging to stdout.\n-  ``bb.error(msg)``: Writes \"ERROR: msg\" to the log while also\nlogging to standard out (stdout).\n.. note::\nCalling this function does not cause the task to fail.\n-  ``bb.fatal(msg)``: This logging function is similar to\n``bb.error(msg)`` but also causes the calling task to fail.\n.. note::\n``bb.fatal()`` raises an exception, which means you do not need to put a\n\"return\" statement after the function.\nThe same logging functions are also available in shell functions, under\nthe names ``bbplain``, ``bbnote``, ``bbdebug``, ``bbwarn``, ``bberror``,\nand ``bbfatal``. The :ref:`ref-classes-logging` class\nimplements these functions. See that class in the ``meta/classes``\nfolder of the :term:`Source Directory` for information.\nLogging With Python\n-------------------\nWhen creating recipes using Python and inserting code that handles build\nlogs, keep in mind the goal is to have informative logs while keeping\nthe console as \"silent\" as possible. Also, if you want status messages\nin the log, use the \"debug\" loglevel.\nFollowing is an example written in Python. The code handles logging for\na function that determines the number of tasks needed to be run. See the\n\":ref:`ref-tasks-listtasks`\"\nsection for additional information::\npython do_listtasks() {\nbb.debug(2, \"Starting to figure out the task list\")\nif noteworthy_condition:\nbb.note(\"There are 47 tasks to run\")\nbb.debug(2, \"Got to point xyz\")\nif warning_trigger:\nbb.warn(\"Detected warning_trigger, this might be a problem later.\")\nif recoverable_error:\nbb.error(\"Hit recoverable_error, you really need to fix this!\")\nif fatal_error:\nbb.fatal(\"fatal_error detected, unable to print the task list\")\nbb.plain(\"The tasks present are abc\")\nbb.debug(2, \"Finished figuring out the tasklist\")\n}\nLogging With Bash\n-----------------\nWhen creating recipes using Bash and inserting code that handles build\nlogs, you have the same goals --- informative with minimal console output.\nThe syntax you use for recipes written in Bash is similar to that of\nrecipes written in Python described in the previous section.\nFollowing is an example written in Bash. The code logs the progress of\nthe ``do_my_function`` function::\ndo_my_function() {\nbbdebug 2 \"Running do_my_function\"\nif [ exceptional_condition ]; then\nbbnote \"Hit exceptional_condition\"\nfi\nbbdebug 2  \"Got to point xyz\"\nif [ warning_trigger ]; then\nbbwarn \"Detected warning_trigger, this might cause a problem later.\"\nfi\nif [ recoverable_error ]; then\nbberror \"Hit recoverable_error, correcting\"\nfi\nif [ fatal_error ]; then\nbbfatal \"fatal_error detected\"\nfi\nbbdebug 2 \"Completed do_my_function\""}
{"text": "\n}\nDebugging Parallel Make Races\n=============================\nA parallel ``make`` race occurs when the build consists of several parts\nthat are run simultaneously and a situation occurs when the output or\nresult of one part is not ready for use with a different part of the\nbuild that depends on that output. Parallel make races are annoying and\ncan sometimes be difficult to reproduce and fix. However, there are some simple\ntips and tricks that can help you debug and fix them. This section\npresents a real-world example of an error encountered on the Yocto\nProject autobuilder and the process used to fix it.\n.. note::\nIf you cannot properly fix a ``make`` race condition, you can work around it\nby clearing either the :term:`PARALLEL_MAKE` or :term:`PARALLEL_MAKEINST`\nvariables.\nThe Failure\n-----------\nFor this example, assume that you are building an image that depends on\nthe \"neard\" package. And, during the build, BitBake runs into problems\nand creates the following output.\n.. note::\nThis example log file has longer lines artificially broken to make\nthe listing easier to read.\nIf you examine the output or the log file, you see the failure during\n``make``:\n.. code-block:: none\n| DEBUG: SITE files ['endian-little', 'bit-32', 'ix86-common', 'common-linux', 'common-glibc', 'i586-linux', 'common']\n| DEBUG: Executing shell function do_compile\n| NOTE: make -j 16\n| make --no-print-directory all-am\n| /bin/mkdir -p include/near\n| /bin/mkdir -p include/near\n| /bin/mkdir -p include/near\n| ln -s /home/pokybuild/yocto-autobuilder/nightly-x86/build/build/tmp/work/i586-poky-linux/neard/\n0.14-r0/neard-0.14/include/types.h include/near/types.h\n| ln -s /home/pokybuild/yocto-autobuilder/nightly-x86/build/build/tmp/work/i586-poky-linux/neard/\n0.14-r0/neard-0.14/include/log.h include/near/log.h\n| ln -s /home/pokybuild/yocto-autobuilder/nightly-x86/build/build/tmp/work/i586-poky-linux/neard/\n0.14-r0/neard-0.14/include/plugin.h include/near/plugin.h\n| /bin/mkdir -p include/near\n| /bin/mkdir -p include/near\n| /bin/mkdir -p include/near\n| ln -s /home/pokybuild/yocto-autobuilder/nightly-x86/build/build/tmp/work/i586-poky-linux/neard/\n0.14-r0/neard-0.14/include/tag.h include/near/tag.h\n| /bin/mkdir -p include/near\n| ln -s /home/pokybuild/yocto-autobuilder/nightly-x86/build/build/tmp/work/i586-poky-linux/neard/\n0.14-r0/neard-0.14/include/adapter.h include/near/adapter.h\n| /bin/mkdir -p include/near\n| ln -s /home/pokybuild/yocto-autobuilder/nightly-x86/build/build/tmp/work/i586-poky-linux/neard/\n0.14-r0/neard-0.14/include/ndef.h include/near/ndef.h\n| ln -s /home/pokybuild/yocto-autobuilder/nightly-x86/build/build/tmp/work/i586-poky-linux/neard/\n0.14-r0/neard-0.14/include/tlv.h include/near/tlv.h\n| /bin/mkdir -p include/near\n| /bin/mkdir -p include/near\n| ln -s /home/pokybuild/yocto-autobuilder/nightly-x86/build/build/tmp/work/i586-poky-linux/neard/\n0.14-r0/neard-0.14/include/setting.h include/near/setting.h\n| /bin/mkdir -p include/near\n| /bin/mkdir -p include/near\n| /bin/mkdir -p include/near\n| ln -s /home/pokybuild/yocto-autobuilder/nightly-x86/build/build/tmp/work/i586-poky-linux/neard/\n0.14-r0/neard-0.14/include/device.h include/near/device.h\n| ln -s /home/pokybuild/yocto-autobuilder/nightly-x86/build/build/tmp/work/i586-poky-linux/neard/\n0.14-r0/neard-0.14/include/nfc_copy.h include/near/nfc_copy.h\n| ln -s /home/pokybuild/yocto-autobuilder/nightly-x86/build/build/tmp/work/i586-poky-linux/neard/\n0.14-r0/neard-0.14/include/snep.h include/near/snep.h\n| ln -s /home/pokybuild/yocto-autobuilder/nightly-x86/build/build/tmp/work/i586-poky-linux/neard/\n0.14-r0/neard-0.14/include/version.h include/near/version.h\n| ln -s /home/pokybuild/yocto-autobuilder/nightly-x86/build/build/tmp/work/i586-poky-linux/neard/\n0.14-r0/neard-0.14/include/dbus.h include/near/dbus.h\n| ./src/genbuiltin nfctype1 nfctype2 nfctype3 nfctype4 p2p > src/builtin.h\n| i586-poky-linux-gcc  -m32 -march=i586 --sysroot=/home/pokybuild/yocto-autobuilder/nightly-x86/\nbuild/build/tmp/sysroots/qemux86 -DHAVE_CONFIG_H -I. -I./include -I./src -I./gdbus  -I/home/pokybuild/\nyocto-autobuilder/nightly-x86/build/build/tmp/sysroots/qemux86/usr/include/glib-2.0\n-I/home/pokybuild/yocto-autobuilder/nightly-x86/build/build/tmp/sysroots/qemux86/usr/\nlib/glib-2.0/include  -I/home/pokybuild/yocto-autobuilder/nightly-x86/build/build/\ntmp/sysroots/qemux86/usr/include/dbus-1.0 -I/home/pokybuild/yocto-autobuilder/\nnightly-x86/build/build/tmp/sysroots/qemux86/usr/lib/dbus-1.0/include  -I/home/pokybuild/yocto-autobuilder/\nnightly-x86/build/build/tmp/sysroots/qemux86/usr/include/libnl3\n-DNEAR_PLUGIN_BUILTIN -DPLUGINDIR=\\\"\"/usr/lib/near/plugins\"\\\"\n-DCONFIGDIR=\\\"\"/etc/neard\\\"\" -O2 -pipe -g -feliminate-unused-debug-types -c\n-o tools/snep-send.o tools/snep-send.c\n| In file included from tools/snep-send.c:16:0:\n| tools/../src/near.h:41:23: fatal error: near/dbus.h: No such file or directory\n|  #include <near/dbus.h>\n|                        ^\n| compilation terminated.\n| make[1]: *** [tools/snep-send.o] Error 1\n| make[1]: *** Waiting for unfinished jobs....\n| make: *** [all] Error 2\n| ERROR: oe_runmake failed\nReproducing the Error\n---------------------\nBecause race conditions are intermittent, they do not manifest\nthemselves every time you do the build. In fact, most times the build\nwill complete without problems even though the potential race condition\nexists. Thus, once the error surfaces, you need a way to reproduce it.\nIn this example, compiling the \"neard\" package is causing the problem.\nSo the first thing to do is build \"neard\" locally. Before you start the\nbuild, set the\n:term:`PARALLEL_MAKE` variable"}
{"text": "\nin your ``local.conf`` file to a high number (e.g. \"-j 20\"). Using a\nhigh value for :term:`PARALLEL_MAKE` increases the chances of the race\ncondition showing up::\n$ bitbake neard\nOnce the local build for \"neard\" completes, start a ``devshell`` build::\n$ bitbake neard -c devshell\nFor information on how to use a ``devshell``, see the\n\":ref:`dev-manual/development-shell:using a development shell`\" section.\nIn the ``devshell``, do the following::\n$ make clean\n$ make tools/snep-send.o\nThe ``devshell`` commands cause the failure to clearly\nbe visible. In this case, there is a missing dependency for the ``neard``\nMakefile target. Here is some abbreviated, sample output with the\nmissing dependency clearly visible at the end::\ni586-poky-linux-gcc  -m32 -march=i586 --sysroot=/home/scott-lenovo/......\n.\n.\n.\ntools/snep-send.c\nIn file included from tools/snep-send.c:16:0:\ntools/../src/near.h:41:23: fatal error: near/dbus.h: No such file or directory\n#include <near/dbus.h>\n^\ncompilation terminated.\nmake: *** [tools/snep-send.o] Error 1\n$\nCreating a Patch for the Fix\n----------------------------\nBecause there is a missing dependency for the Makefile target, you need\nto patch the ``Makefile.am`` file, which is generated from\n``Makefile.in``. You can use Quilt to create the patch::\n$ quilt new parallelmake.patch\nPatch patches/parallelmake.patch is now on top\n$ quilt add Makefile.am\nFile Makefile.am added to patch patches/parallelmake.patch\nFor more information on using Quilt, see the\n\":ref:`dev-manual/quilt:using quilt in your workflow`\" section.\nAt this point you need to make the edits to ``Makefile.am`` to add the\nmissing dependency. For our example, you have to add the following line\nto the file::\ntools/snep-send.$(OBJEXT): include/near/dbus.h\nOnce you have edited the file, use the ``refresh`` command to create the\npatch::\n$ quilt refresh\nRefreshed patch patches/parallelmake.patch\nOnce the patch file is created, you need to add it back to the originating\nrecipe folder. Here is an example assuming a top-level\n:term:`Source Directory` named ``poky``::\n$ cp patches/parallelmake.patch poky/meta/recipes-connectivity/neard/neard\nThe final thing you need to do to implement the fix in the build is to\nupdate the \"neard\" recipe (i.e. ``neard-0.14.bb``) so that the\n:term:`SRC_URI` statement includes\nthe patch file. The recipe file is in the folder above the patch. Here\nis what the edited :term:`SRC_URI` statement would look like::\nSRC_URI = \"${KERNELORG_MIRROR}/linux/network/nfc/${BPN}-${PV}.tar.xz \\\nfile://neard.in \\\nfile://neard.service.in \\\nfile://parallelmake.patch \\\n\"\nWith the patch complete and moved to the correct folder and the\n:term:`SRC_URI` statement updated, you can exit the ``devshell``::\n$ exit\nTesting the Build\n-----------------\nWith everything in place, you can get back to trying the build again\nlocally::\n$ bitbake neard\nThis build should succeed.\nNow you can open up a ``devshell`` again and repeat the clean and make\noperations as follows::\n$ bitbake neard -c devshell\n$ make clean\n$ make tools/snep-send.o\nThe build should work without issue.\nAs with all solved problems, if they originated upstream, you need to\nsubmit the fix for the recipe in OE-Core and upstream so that the\nproblem is taken care of at its source. See the\n\":doc:`../contributor-guide/submit-changes`\" section for more information.\nDebugging With the GNU Project Debugger (GDB) Remotely\n======================================================\nGDB allows you to examine running programs, which in turn helps you to\nunderstand and fix problems. It also allows you to perform post-mortem\nstyle analysis of program crashes. GDB is available as a package within\nthe Yocto Project and is installed in SDK images by default. See the\n\":ref:`ref-manual/images:Images`\" chapter in the Yocto\nProject Reference Manual for a description of these images. You can find\ninformation on GDB at https://sourceware.org/gdb/.\n.. note::\nFor best results, install debug (``-dbg``) packages for the applications you\nare going to debug. Doing so makes extra debug symbols available that give\nyou more meaningful output.\nSometimes, due to memory or disk space constraints, it is not possible\nto use GDB directly on the remote target to debug applications. These\nconstraints arise because GDB needs to load the debugging information\nand the binaries of the process being debugged. Additionally, GDB needs\nto perform many computations to locate information such as function\nnames, variable names and values, stack traces and so forth --- even\nbefore starting the debugging process. These extra computations place\nmore load on the target system and can alter the characteristics of the"}
{"text": "\nprogram being debugged.\nTo help get past the previously mentioned constraints, there are two\nmethods you can use: running a debuginfod server and using gdbserver.\nUsing the debuginfod server method\n----------------------------------\n``debuginfod`` from ``elfutils`` is a way to distribute ``debuginfo`` files.\nRunning a ``debuginfod`` server makes debug symbols readily available,\nwhich means you don't need to download debugging information\nand the binaries of the process being debugged. You can just fetch\ndebug symbols from the server.\nTo run a ``debuginfod`` server, you need to do the following:\n-  Ensure that ``debuginfod`` is present in :term:`DISTRO_FEATURES`\n(it already is in ``OpenEmbedded-core`` defaults and ``poky`` reference distribution).\nIf not, set in your distro config file or in ``local.conf``::\nDISTRO_FEATURES:append = \" debuginfod\"\nThis distro feature enables the server and client library in ``elfutils``,\nand enables ``debuginfod`` support in clients (at the moment, ``gdb`` and ``binutils``).\n-  Run the following commands to launch the ``debuginfod`` server on the host::\n$ oe-debuginfod\n-  To use ``debuginfod`` on the target, you need to know the ip:port where\n``debuginfod`` is listening on the host (port defaults to 8002), and export\nthat into the shell environment, for example in ``qemu``::\nroot@qemux86-64:~# export DEBUGINFOD_URLS=\"http://192.168.7.1:8002/\"\n-  Then debug info fetching should simply work when running the target ``gdb``,\n``readelf`` or ``objdump``, for example::\nroot@qemux86-64:~# gdb /bin/cat\n...\nReading symbols from /bin/cat...\nDownloading separate debug info for /bin/cat...\nReading symbols from /home/root/.cache/debuginfod_client/923dc4780cfbc545850c616bffa884b6b5eaf322/debuginfo...\n-  It's also possible to use ``debuginfod-find`` to just query the server::\nroot@qemux86-64:~# debuginfod-find debuginfo /bin/ls\n/home/root/.cache/debuginfod_client/356edc585f7f82d46f94fcb87a86a3fe2d2e60bd/debuginfo\nUsing the gdbserver method\n--------------------------\ngdbserver, which runs on the remote target and does not load any\ndebugging information from the debugged process. Instead, a GDB instance\nprocesses the debugging information that is run on a remote computer -\nthe host GDB. The host GDB then sends control commands to gdbserver to\nmake it stop or start the debugged program, as well as read or write\nmemory regions of that debugged program. All the debugging information\nloaded and processed as well as all the heavy debugging is done by the\nhost GDB. Offloading these processes gives the gdbserver running on the\ntarget a chance to remain small and fast.\nBecause the host GDB is responsible for loading the debugging\ninformation and for doing the necessary processing to make actual\ndebugging happen, you have to make sure the host can access the\nunstripped binaries complete with their debugging information and also\nbe sure the target is compiled with no optimizations. The host GDB must\nalso have local access to all the libraries used by the debugged\nprogram. Because gdbserver does not need any local debugging\ninformation, the binaries on the remote target can remain stripped.\nHowever, the binaries must also be compiled without optimization so they\nmatch the host's binaries.\nTo remain consistent with GDB documentation and terminology, the binary\nbeing debugged on the remote target machine is referred to as the\n\"inferior\" binary. For documentation on GDB see the `GDB\nsite <https://sourceware.org/gdb/documentation/>`__.\nThe following steps show you how to debug using the GNU project\ndebugger.\n#. *Configure your build system to construct the companion debug\nfilesystem:*\nIn your ``local.conf`` file, set the following::\nIMAGE_GEN_DEBUGFS = \"1\"\nIMAGE_FSTYPES_DEBUGFS = \"tar.bz2\"\nThese options cause the\nOpenEmbedded build system to generate a special companion filesystem\nfragment, which contains the matching source and debug symbols to\nyour deployable filesystem. The build system does this by looking at\nwhat is in the deployed filesystem, and pulling the corresponding\n``-dbg`` packages.\nThe companion debug filesystem is not a complete filesystem, but only\ncontains the debug fragments. This filesystem must be combined with\nthe full filesystem for debugging. Subsequent steps in this procedure\nshow how to combine the partial filesystem with the full filesystem.\n#. *Configure the system to include gdbserver in the target filesystem:*\nMake the following addition in your ``local.conf`` file::\nEXTRA_IMAGE_FEATURES:append = \" tools-debug\"\nThe change makes\nsure the ``gdbserver`` package is included.\n#. *Build the environment:*\nUse the following command to construct the image and the companion\nDebug Filesystem::\n$ bitbake image\nBuild the cross GDB component and\nmake it available for debugging. Build the SDK that matches the\nimage. Building the SDK is best for a production build that can be\nused later for debugging, especially during long term maintenance::\n$ bitbake -c populate_sdk image\nAlternatively, you can build the minimal toolchain components that\nmatch the target. Doing so creates a smaller than typical SDK and\nonly contains a minimal set of components with which to build simple\ntest applications, as well as run the debugger::\n$ bitbake meta-toolchain\nA final method is to build Gdb itself within the build system::\n$ bitbake gdb-cross-<architecture>\nDoing so produces a temporary copy of\n``cross-gdb`` you can use for debugging during development. While\nthis is the quickest approach, the two previous methods in this step\nare better when considering long-term maintenance strategies."}
{"text": "\n.. note::\nIf you run ``bitbake gdb-cross``, the OpenEmbedded build system suggests\nthe actual image (e.g. ``gdb-cross-i586``). The suggestion is usually the\nactual name you want to use.\n#. *Set up the* ``debugfs``\\ *:*\nRun the following commands to set up the ``debugfs``::\n$ mkdir debugfs\n$ cd debugfs\n$ tar xvfj build-dir/tmp/deploy/images/machine/image.rootfs.tar.bz2\n$ tar xvfj build-dir/tmp/deploy/images/machine/image-dbg.rootfs.tar.bz2\n#. *Set up GDB:*\nInstall the SDK (if you built one) and then source the correct\nenvironment file. Sourcing the environment file puts the SDK in your\n``PATH`` environment variable and sets ``$GDB`` to the SDK's debugger.\nIf you are using the build system, Gdb is located in\n`build-dir`\\ ``/tmp/sysroots/``\\ `host`\\ ``/usr/bin/``\\ `architecture`\\ ``/``\\ `architecture`\\ ``-gdb``\n#. *Boot the target:*\nFor information on how to run QEMU, see the `QEMU\nDocumentation <https://wiki.qemu.org/Documentation/GettingStartedDevelopers>`__.\n.. note::\nBe sure to verify that your host can access the target via TCP.\n#. *Debug a program:*\nDebugging a program involves running gdbserver on the target and then\nrunning Gdb on the host. The example in this step debugs ``gzip``:\n.. code-block:: shell\nroot@qemux86:~# gdbserver localhost:1234 /bin/gzip —help\nFor\nadditional gdbserver options, see the `GDB Server\nDocumentation <https://www.gnu.org/software/gdb/documentation/>`__.\nAfter running gdbserver on the target, you need to run Gdb on the\nhost and configure it and connect to the target. Use these commands::\n$ cd directory-holding-the-debugfs-directory\n$ arch-gdb\n(gdb) set sysroot debugfs\n(gdb) set substitute-path /usr/src/debug debugfs/usr/src/debug\n(gdb) target remote IP-of-target:1234\nAt this\npoint, everything should automatically load (i.e. matching binaries,\nsymbols and headers).\n.. note::\nThe Gdb ``set`` commands in the previous example can be placed into the\nusers ``~/.gdbinit`` file. Upon starting, Gdb automatically runs whatever\ncommands are in that file.\n#. *Deploying without a full image rebuild:*\nIn many cases, during development you want a quick method to deploy a\nnew binary to the target and debug it, without waiting for a full\nimage build.\nOne approach to solving this situation is to just build the component\nyou want to debug. Once you have built the component, copy the\nexecutable directly to both the target and the host ``debugfs``.\nIf the binary is processed through the debug splitting in\nOpenEmbedded, you should also copy the debug items (i.e. ``.debug``\ncontents and corresponding ``/usr/src/debug`` files) from the work\ndirectory. Here is an example::\n$ bitbake bash\n$ bitbake -c devshell bash\n$ cd ..\n$ scp packages-split/bash/bin/bash target:/bin/bash\n$ cp -a packages-split/bash-dbg/\\* path/debugfs\nDebugging with the GNU Project Debugger (GDB) on the Target\n===========================================================\nThe previous section addressed using GDB remotely for debugging\npurposes, which is the most usual case due to the inherent hardware\nlimitations on many embedded devices. However, debugging in the target\nhardware itself is also possible with more powerful devices. This\nsection describes what you need to do in order to support using GDB to\ndebug on the target hardware.\nTo support this kind of debugging, you need do the following:\n-  Ensure that GDB is on the target. You can do this by making\nthe following addition to your ``local.conf`` file::\nEXTRA_IMAGE_FEATURES:append = \" tools-debug\"\n-  Ensure that debug symbols are present. You can do so by adding the\ncorresponding ``-dbg`` package to :term:`IMAGE_INSTALL`::\nIMAGE_INSTALL:append = \" packagename-dbg\"\nAlternatively, you can add the following to ``local.conf`` to include\nall the debug symbols::\nEXTRA_IMAGE_FEATURES:append = \" dbg-pkgs\"\n.. note::\nTo improve the debug information accuracy, you can reduce the level\nof optimization used by the compiler. For example, when adding the\nfollowing line to your ``local.conf`` file, you will reduce optimization\nfrom :term:`FULL_OPTIMIZATION` of \"-O2\" to :term:`DEBUG_OPTIMIZATION`\nof \"-O -fno-omit-frame-pointer\"::\nDEBUG_BUILD = \"1\"\nConsider that this will reduce the application's performance and is\nrecommended only for debugging purposes.\nEnabling Minidebuginfo\n======================\nEnabling the :term:`DISTRO_FEATURES` minidebuginfo adds a compressed ELF section ``.gnu_debugdata``\nto all binary files, containing only function names, and thus increasing the size of the\nbinaries only by 5 to 10%. For comparison, full debug symbols can be 10 times as big as\na stripped binary, and it is thus not always possible to deploy full debug symbols.\nMinidebuginfo data allows, on the one side, to retrieve a call-stack using\nGDB (command backtrace) without deploying full debug symbols to the target. It also\nallows to retrieve a symbolicated call-stack when using ``systemd-coredump`` to manage\ncoredumps (commands ``coredumpctl list`` and ``coredumpctl info``).\nThis feature was created by Fedora, see https://fedoraproject.org/wiki/Features/MiniDebugInfo for\nmore details.\nOther Debugging Tips\n===================="}
{"text": "\nHere are some other tips that you might find useful:\n-  When adding new packages, it is worth watching for undesirable items\nmaking their way into compiler command lines. For example, you do not\nwant references to local system files like ``/usr/lib/`` or\n``/usr/include/``.\n-  If you want to remove the ``psplash`` boot splashscreen, add\n``psplash=false`` to the kernel command line. Doing so prevents\n``psplash`` from loading and thus allows you to see the console. It\nis also possible to switch out of the splashscreen by switching the\nvirtual console (e.g. Fn+Left or Fn+Right on a Zaurus).\n-  Removing :term:`TMPDIR` (usually ``tmp/``, within the\n:term:`Build Directory`) can often fix temporary build issues. Removing\n:term:`TMPDIR` is usually a relatively cheap operation, because task output\nwill be cached in :term:`SSTATE_DIR` (usually ``sstate-cache/``, which is\nalso in the :term:`Build Directory`).\n.. note::\nRemoving :term:`TMPDIR` might be a workaround rather than a fix.\nConsequently, trying to determine the underlying cause of an issue before\nremoving the directory is a good idea.\n-  Understanding how a feature is used in practice within existing\nrecipes can be very helpful. It is recommended that you configure\nsome method that allows you to quickly search through files.\nUsing GNU Grep, you can use the following shell function to\nrecursively search through common recipe-related files, skipping\nbinary files, ``.git`` directories, and the :term:`Build Directory`\n(assuming its name starts with \"build\")::\ng() {\ngrep -Ir \\\n--exclude-dir=.git \\\n--exclude-dir='build*' \\\n--include='*.bb*' \\\n--include='*.inc*' \\\n--include='*.conf*' \\\n--include='*.py*' \\\n\"$@\"\n}\nFollowing are some usage examples::\n$ g FOO # Search recursively for \"FOO\"\n$ g -i foo # Search recursively for \"foo\", ignoring case\n$ g -w FOO # Search recursively for \"FOO\" as a word, ignoring e.g. \"FOOBAR\"\nIf figuring\nout how some feature works requires a lot of searching, it might\nindicate that the documentation should be extended or improved. In\nsuch cases, consider filing a documentation bug using the Yocto\nProject implementation of\n:yocto_bugs:`Bugzilla <>`. For information on\nhow to submit a bug against the Yocto Project, see the Yocto Project\nBugzilla :yocto_wiki:`wiki page </Bugzilla_Configuration_and_Bug_Tracking>`\nand the \":doc:`../contributor-guide/report-defect`\" section.\n.. note::\nThe manuals might not be the right place to document variables\nthat are purely internal and have a limited scope (e.g. internal\nvariables used to implement a single ``.bbclass`` file)."}
{"text": "\nPerforming Automated Runtime Testing\n************************************\nThe OpenEmbedded build system makes available a series of automated\ntests for images to verify runtime functionality. You can run these\ntests on either QEMU or actual target hardware. Tests are written in\nPython making use of the ``unittest`` module, and the majority of them\nrun commands on the target system over SSH. This section describes how\nyou set up the environment to use these tests, run available tests, and\nwrite and add your own tests.\nFor information on the test and QA infrastructure available within the\nYocto Project, see the \":ref:`ref-manual/release-process:testing and quality assurance`\"\nsection in the Yocto Project Reference Manual.\nEnabling Tests\n==============\nDepending on whether you are planning to run tests using QEMU or on the\nhardware, you have to take different steps to enable the tests. See the\nfollowing subsections for information on how to enable both types of\ntests.\nEnabling Runtime Tests on QEMU\n------------------------------\nIn order to run tests, you need to do the following:\n-  *Set up to avoid interaction with sudo for networking:* To\naccomplish this, you must do one of the following:\n-  Add ``NOPASSWD`` for your user in ``/etc/sudoers`` either for all\ncommands or just for ``runqemu-ifup``. You must provide the full\npath as that can change if you are using multiple clones of the\nsource repository.\n.. note::\nOn some distributions, you also need to comment out \"Defaults\nrequiretty\" in ``/etc/sudoers``.\n-  Manually configure a tap interface for your system.\n-  Run as root the script in ``scripts/runqemu-gen-tapdevs``, which\nshould generate a list of tap devices. This is the option\ntypically chosen for Autobuilder-type environments.\n.. note::\n-  Be sure to use an absolute path when calling this script\nwith sudo.\n-  The package recipe ``qemu-helper-native`` is required to run\nthis script. Build the package using the following command::\n$ bitbake qemu-helper-native\n-  *Set the DISPLAY variable:* You need to set this variable so that\nyou have an X server available (e.g. start ``vncserver`` for a\nheadless machine).\n-  *Be sure your host's firewall accepts incoming connections from\n192.168.7.0/24:* Some of the tests (in particular DNF tests) start an\nHTTP server on a random high number port, which is used to serve\nfiles to the target. The DNF module serves\n``${WORKDIR}/oe-rootfs-repo`` so it can run DNF channel commands.\nThat means your host's firewall must accept incoming connections from\n192.168.7.0/24, which is the default IP range used for tap devices by\n``runqemu``.\n-  *Be sure your host has the correct packages installed:* Depending\nyour host's distribution, you need to have the following packages\ninstalled:\n-  Ubuntu and Debian: ``sysstat`` and ``iproute2``\n-  openSUSE: ``sysstat`` and ``iproute2``\n-  Fedora: ``sysstat`` and ``iproute``\n-  CentOS: ``sysstat`` and ``iproute``\nOnce you start running the tests, the following happens:\n#. A copy of the root filesystem is written to ``${WORKDIR}/testimage``.\n#. The image is booted under QEMU using the standard ``runqemu`` script.\n#. A default timeout of 500 seconds occurs to allow for the boot process\nto reach the login prompt. You can change the timeout period by\nsetting\n:term:`TEST_QEMUBOOT_TIMEOUT`\nin the ``local.conf`` file.\n#. Once the boot process is reached and the login prompt appears, the\ntests run. The full boot log is written to\n``${WORKDIR}/testimage/qemu_boot_log``.\n#. Each test module loads in the order found in :term:`TEST_SUITES`. You can\nfind the full output of the commands run over SSH in\n``${WORKDIR}/testimgage/ssh_target_log``.\n#. If no failures occur, the task running the tests ends successfully.\nYou can find the output from the ``unittest`` in the task log at\n``${WORKDIR}/temp/log.do_testimage``.\nEnabling Runtime Tests on Hardware\n----------------------------------\nThe OpenEmbedded build system can run tests on real hardware, and for\ncertain devices it can also deploy the image to be tested onto the\ndevice beforehand.\nFor automated deployment, a \"controller image\" is installed onto the\nhardware once as part of setup. Then, each time tests are to be run, the\nfollowing occurs:\n#. The controller image is booted into and used to write the image to be\ntested to a second partition.\n#. The device is then rebooted using an external script that you need to\nprovide.\n#. The device boots into the image to be tested.\nWhen running tests (independent of whether the image has been deployed\nautomatically or not), the device is expected to be connected to a\nnetwork on a pre-determined IP address. You can either use static IP\naddresses written into the image, or set the image to use DHCP and have\nyour DHCP server on the test network assign a known IP address based on\nthe MAC address of the device.\nIn order to run tests on hardware, you need to set :term:`TEST_TARGET` to an\nappropriate value. For QEMU, you do not have to change anything, the\ndefault value is \"qemu\". For running tests on hardware, the following\noptions are available:\n-  *\"simpleremote\":* Choose \"simpleremote\" if you are going to run tests"}
{"text": "\non a target system that is already running the image to be tested and\nis available on the network. You can use \"simpleremote\" in\nconjunction with either real hardware or an image running within a\nseparately started QEMU or any other virtual machine manager.\n-  *\"SystemdbootTarget\":* Choose \"SystemdbootTarget\" if your hardware is\nan EFI-based machine with ``systemd-boot`` as bootloader and\n``core-image-testmaster`` (or something similar) is installed. Also,\nyour hardware under test must be in a DHCP-enabled network that gives\nit the same IP address for each reboot.\nIf you choose \"SystemdbootTarget\", there are additional requirements\nand considerations. See the\n\":ref:`dev-manual/runtime-testing:selecting systemdboottarget`\" section, which\nfollows, for more information.\n-  *\"BeagleBoneTarget\":* Choose \"BeagleBoneTarget\" if you are deploying\nimages and running tests on the BeagleBone \"Black\" or original\n\"White\" hardware. For information on how to use these tests, see the\ncomments at the top of the BeagleBoneTarget\n``meta-yocto-bsp/lib/oeqa/controllers/beaglebonetarget.py`` file.\n-  *\"GrubTarget\":* Choose \"GrubTarget\" if you are deploying images and running\ntests on any generic PC that boots using GRUB. For information on how\nto use these tests, see the comments at the top of the GrubTarget\n``meta-yocto-bsp/lib/oeqa/controllers/grubtarget.py`` file.\n-  *\"your-target\":* Create your own custom target if you want to run\ntests when you are deploying images and running tests on a custom\nmachine within your BSP layer. To do this, you need to add a Python\nunit that defines the target class under ``lib/oeqa/controllers/``\nwithin your layer. You must also provide an empty ``__init__.py``.\nFor examples, see files in ``meta-yocto-bsp/lib/oeqa/controllers/``.\nSelecting SystemdbootTarget\n---------------------------\nIf you did not set :term:`TEST_TARGET` to \"SystemdbootTarget\", then you do\nnot need any information in this section. You can skip down to the\n\":ref:`dev-manual/runtime-testing:running tests`\" section.\nIf you did set :term:`TEST_TARGET` to \"SystemdbootTarget\", you also need to\nperform a one-time setup of your controller image by doing the following:\n#. *Set EFI_PROVIDER:* Be sure that :term:`EFI_PROVIDER` is as follows::\nEFI_PROVIDER = \"systemd-boot\"\n#. *Build the controller image:* Build the ``core-image-testmaster`` image.\nThe ``core-image-testmaster`` recipe is provided as an example for a\n\"controller\" image and you can customize the image recipe as you would\nany other recipe.\nHere are the image recipe requirements:\n-  Inherits ``core-image`` so that kernel modules are installed.\n-  Installs normal linux utilities not BusyBox ones (e.g. ``bash``,\n``coreutils``, ``tar``, ``gzip``, and ``kmod``).\n-  Uses a custom :term:`Initramfs` image with a custom\ninstaller. A normal image that you can install usually creates a\nsingle root filesystem partition. This image uses another installer that\ncreates a specific partition layout. Not all Board Support\nPackages (BSPs) can use an installer. For such cases, you need to\nmanually create the following partition layout on the target:\n-  First partition mounted under ``/boot``, labeled \"boot\".\n-  The main root filesystem partition where this image gets installed,\nwhich is mounted under ``/``.\n-  Another partition labeled \"testrootfs\" where test images get\ndeployed.\n#. *Install image:* Install the image that you just built on the target\nsystem.\nThe final thing you need to do when setting :term:`TEST_TARGET` to\n\"SystemdbootTarget\" is to set up the test image:\n#. *Set up your local.conf file:* Make sure you have the following\nstatements in your ``local.conf`` file::\nIMAGE_FSTYPES += \"tar.gz\"\nIMAGE_CLASSES += \"testimage\"\nTEST_TARGET = \"SystemdbootTarget\"\nTEST_TARGET_IP = \"192.168.2.3\"\n#. *Build your test image:* Use BitBake to build the image::\n$ bitbake core-image-sato\nPower Control\n-------------\nFor most hardware targets other than \"simpleremote\", you can control\npower:\n-  You can use :term:`TEST_POWERCONTROL_CMD` together with\n:term:`TEST_POWERCONTROL_EXTRA_ARGS` as a command that runs on the host\nand does power cycling. The test code passes one argument to that\ncommand: off, on or cycle (off then on). Here is an example that\ncould appear in your ``local.conf`` file::\nTEST_POWERCONTROL_CMD = \"powercontrol.exp test 10.11.12.1 nuc1\"\nIn this example, the expect\nscript does the following:\n.. code-block:: shell\nssh test@10.11.12.1 \"pyctl nuc1 arg\"\nIt then runs a Python script that controls power for a label called\n``nuc1``.\n.. note::\nYou need to customize :term:`TEST_POWERCONTROL_CMD` and\n:term:`TEST_POWERCONTROL_EXTRA_ARGS` for your own setup. The one requirement\nis that it accepts \"on\", \"off\", and \"cycle\" as the last argument.\n-  When no command is defined, it connects to the device over SSH and\nuses the classic reboot command to reboot the device. Classic reboot\nis fine as long as the machine actually reboots (i.e. the SSH test\nhas not failed). It is useful for scenarios where you have a simple\nsetup, typically with a single board, and where some manual\ninteraction is okay from time to time.\nIf you have no hardware to automatically perform power control but still\nwish to experiment with automated hardware testing, you can use the\n``dialog-power-control`` script that shows a dialog prompting you to perform\nthe required power action. This script requires either KDialog or Zenity\nto be installed. To use this script, set the\n:term:`TEST_POWERCONTROL_CMD`"}
{"text": "\nvariable as follows::\nTEST_POWERCONTROL_CMD = \"${COREBASE}/scripts/contrib/dialog-power-control\"\nSerial Console Connection\n-------------------------\nFor test target classes requiring a serial console to interact with the\nbootloader (e.g. BeagleBoneTarget and GrubTarget),\nyou need to specify a command to use to connect to the serial console of\nthe target machine by using the\n:term:`TEST_SERIALCONTROL_CMD`\nvariable and optionally the\n:term:`TEST_SERIALCONTROL_EXTRA_ARGS`\nvariable.\nThese cases could be a serial terminal program if the machine is\nconnected to a local serial port, or a ``telnet`` or ``ssh`` command\nconnecting to a remote console server. Regardless of the case, the\ncommand simply needs to connect to the serial console and forward that\nconnection to standard input and output as any normal terminal program\ndoes. For example, to use the picocom terminal program on serial device\n``/dev/ttyUSB0`` at 115200bps, you would set the variable as follows::\nTEST_SERIALCONTROL_CMD = \"picocom /dev/ttyUSB0 -b 115200\"\nFor local\ndevices where the serial port device disappears when the device reboots,\nan additional \"serdevtry\" wrapper script is provided. To use this\nwrapper, simply prefix the terminal command with\n``${COREBASE}/scripts/contrib/serdevtry``::\nTEST_SERIALCONTROL_CMD = \"${COREBASE}/scripts/contrib/serdevtry picocom -b 115200 /dev/ttyUSB0\"\nRunning Tests\n=============\nYou can start the tests automatically or manually:\n-  *Automatically running tests:* To run the tests automatically after the\nOpenEmbedded build system successfully creates an image, first set the\n:term:`TESTIMAGE_AUTO` variable to \"1\" in your ``local.conf`` file in the\n:term:`Build Directory`::\nTESTIMAGE_AUTO = \"1\"\nNext, build your image. If the image successfully builds, the\ntests run::\nbitbake core-image-sato\n-  *Manually running tests:* To manually run the tests, first globally\ninherit the :ref:`ref-classes-testimage` class by editing your\n``local.conf`` file::\nIMAGE_CLASSES += \"testimage\"\nNext, use BitBake to run the tests::\nbitbake -c testimage image\nAll test files reside in ``meta/lib/oeqa/runtime/cases`` in the\n:term:`Source Directory`. A test name maps\ndirectly to a Python module. Each test module may contain a number of\nindividual tests. Tests are usually grouped together by the area tested\n(e.g tests for systemd reside in ``meta/lib/oeqa/runtime/cases/systemd.py``).\nYou can add tests to any layer provided you place them in the proper\narea and you extend :term:`BBPATH` in\nthe ``local.conf`` file as normal. Be sure that tests reside in\n``layer/lib/oeqa/runtime/cases``.\n.. note::\nBe sure that module names do not collide with module names used in\nthe default set of test modules in ``meta/lib/oeqa/runtime/cases``.\nYou can change the set of tests run by appending or overriding\n:term:`TEST_SUITES` variable in\n``local.conf``. Each name in :term:`TEST_SUITES` represents a required test\nfor the image. Test modules named within :term:`TEST_SUITES` cannot be\nskipped even if a test is not suitable for an image (e.g. running the\nRPM tests on an image without ``rpm``). Appending \"auto\" to\n:term:`TEST_SUITES` causes the build system to try to run all tests that are\nsuitable for the image (i.e. each test module may elect to skip itself).\nThe order you list tests in :term:`TEST_SUITES` is important and influences\ntest dependencies. Consequently, tests that depend on other tests should\nbe added after the test on which they depend. For example, since the\n``ssh`` test depends on the ``ping`` test, \"ssh\" needs to come after\n\"ping\" in the list. The test class provides no re-ordering or dependency\nhandling.\n.. note::\nEach module can have multiple classes with multiple test methods.\nAnd, Python ``unittest`` rules apply.\nHere are some things to keep in mind when running tests:\n-  The default tests for the image are defined as::\nDEFAULT_TEST_SUITES:pn-image = \"ping ssh df connman syslog xorg scp vnc date rpm dnf dmesg\"\n-  Add your own test to the list of the by using the following::\nTEST_SUITES:append = \" mytest\"\n-  Run a specific list of tests as follows::\nTEST_SUITES = \"test1 test2 test3\"\nRemember, order is important. Be sure to place a test that is\ndependent on another test later in the order.\nExporting Tests\n===============\nYou can export tests so that they can run independently of the build\nsystem. Exporting tests is required if you want to be able to hand the\ntest execution off to a scheduler. You can only export tests that are\ndefined in :term:`TEST_SUITES`.\nIf your image is already built, make sure the following are set in your\n``local.conf`` file::\nINHERIT += \"testexport\"\nTEST_TARGET_IP = \"IP-address-for-the-test-target\"\nTEST_SERVER_IP = \"IP-address-for-the-test-server\"\nYou can then export the tests with the\nfollowing BitBake command form::\n$ bitbake image -c testexport\nExporting the tests places them in the :term:`Build Directory` in\n``tmp/testexport/``\\ image, which is controlled by the :term:`TEST_EXPORT_DIR`\nvariable.\nYou can now run the tests outside of the build environment::\n$ cd tmp/testexport/image"}
{"text": "\n$ ./runexported.py testdata.json\nHere is a complete example that shows IP addresses and uses the\n``core-image-sato`` image::\nINHERIT += \"testexport\"\nTEST_TARGET_IP = \"192.168.7.2\"\nTEST_SERVER_IP = \"192.168.7.1\"\nUse BitBake to export the tests::\n$ bitbake core-image-sato -c testexport\nRun the tests outside of\nthe build environment using the following::\n$ cd tmp/testexport/core-image-sato\n$ ./runexported.py testdata.json\nWriting New Tests\n=================\nAs mentioned previously, all new test files need to be in the proper\nplace for the build system to find them. New tests for additional\nfunctionality outside of the core should be added to the layer that adds\nthe functionality, in ``layer/lib/oeqa/runtime/cases`` (as long as\n:term:`BBPATH` is extended in the\nlayer's ``layer.conf`` file as normal). Just remember the following:\n-  Filenames need to map directly to test (module) names.\n-  Do not use module names that collide with existing core tests.\n-  Minimally, an empty ``__init__.py`` file must be present in the runtime\ndirectory.\nTo create a new test, start by copying an existing module (e.g.\n``oe_syslog.py`` or ``gcc.py`` are good ones to use). Test modules can use\ncode from ``meta/lib/oeqa/utils``, which are helper classes.\n.. note::\nStructure shell commands such that you rely on them and they return a\nsingle code for success. Be aware that sometimes you will need to\nparse the output. See the ``df.py`` and ``date.py`` modules for examples.\nYou will notice that all test classes inherit ``oeRuntimeTest``, which\nis found in ``meta/lib/oetest.py``. This base class offers some helper\nattributes, which are described in the following sections:\nClass Methods\n-------------\nClass methods are as follows:\n-  *hasPackage(pkg):* Returns \"True\" if ``pkg`` is in the installed\npackage list of the image, which is based on the manifest file that\nis generated during the :ref:`ref-tasks-rootfs` task.\n-  *hasFeature(feature):* Returns \"True\" if the feature is in\n:term:`IMAGE_FEATURES` or\n:term:`DISTRO_FEATURES`.\nClass Attributes\n----------------\nClass attributes are as follows:\n-  *pscmd:* Equals \"ps -ef\" if ``procps`` is installed in the image.\nOtherwise, ``pscmd`` equals \"ps\" (busybox).\n-  *tc:* The called test context, which gives access to the\nfollowing attributes:\n-  *d:* The BitBake datastore, which allows you to use stuff such\nas ``oeRuntimeTest.tc.d.getVar(\"VIRTUAL-RUNTIME_init_manager\")``.\n-  *testslist and testsrequired:* Used internally. The tests\ndo not need these.\n-  *filesdir:* The absolute path to\n``meta/lib/oeqa/runtime/files``, which contains helper files for\ntests meant for copying on the target such as small files written\nin C for compilation.\n-  *target:* The target controller object used to deploy and\nstart an image on a particular target (e.g. Qemu, SimpleRemote,\nand SystemdbootTarget). Tests usually use the following:\n-  *ip:* The target's IP address.\n-  *server_ip:* The host's IP address, which is usually used\nby the DNF test suite.\n-  *run(cmd, timeout=None):* The single, most used method.\nThis command is a wrapper for: ``ssh root@host \"cmd\"``. The\ncommand returns a tuple: (status, output), which are what their\nnames imply - the return code of \"cmd\" and whatever output it\nproduces. The optional timeout argument represents the number\nof seconds the test should wait for \"cmd\" to return. If the\nargument is \"None\", the test uses the default instance's\ntimeout period, which is 300 seconds. If the argument is \"0\",\nthe test runs until the command returns.\n-  *copy_to(localpath, remotepath):*\n``scp localpath root@ip:remotepath``.\n-  *copy_from(remotepath, localpath):*\n``scp root@host:remotepath localpath``.\nInstance Attributes\n-------------------\nThere is a single instance attribute, which is ``target``. The ``target``\ninstance attribute is identical to the class attribute of the same name,\nwhich is described in the previous section. This attribute exists as\nboth an instance and class attribute so tests can use\n``self.target.run(cmd)`` in instance methods instead of\n``oeRuntimeTest.tc.target.run(cmd)``.\nInstalling Packages in the DUT Without the Package Manager\n==========================================================\nWhen a test requires a package built by BitBake, it is possible to\ninstall that package. Installing the package does not require a package\nmanager be installed in the device under test (DUT). It does, however,\nrequire an SSH connection and the target must be using the\n``sshcontrol`` class.\n.. note::\nThis method uses ``scp`` to copy files from the host to the target, which\ncauses permissions and special attributes to be lost.\nA JSON file is used to define the packages needed by a test. This file\nmust be in the same path as the file used to define the tests.\nFurthermore, the filename must map directly to the test module name with\na ``.json`` extension.\nThe JSON file must include an object with the test name as keys of an"}
{"text": "Performing Automated Runtime Testing\nobject or an array. This object (or array of objects) uses the following\ndata:\n-  \"pkg\" --- a mandatory string that is the name of the package to be\ninstalled.\n-  \"rm\" --- an optional boolean, which defaults to \"false\", that specifies\nto remove the package after the test.\n-  \"extract\" --- an optional boolean, which defaults to \"false\", that\nspecifies if the package must be extracted from the package format.\nWhen set to \"true\", the package is not automatically installed into\nthe DUT.\nFollowing is an example JSON file that handles test \"foo\" installing\npackage \"bar\" and test \"foobar\" installing packages \"foo\" and \"bar\".\nOnce the test is complete, the packages are removed from the DUT::\n{\n\"foo\": {\n\"pkg\": \"bar\"\n},\n\"foobar\": [\n{\n\"pkg\": \"foo\",\n\"rm\": true\n},\n{\n\"pkg\": \"bar\",\n\"rm\": true\n}\n]\n}"}
{"text": "\n.. _init-manager:\nSelecting an Initialization Manager\n***********************************\nBy default, the Yocto Project uses :wikipedia:`SysVinit <Init#SysV-style>` as\nthe initialization manager. There is also support for BusyBox init, a simpler\nimplementation, as well as support for :wikipedia:`systemd <Systemd>`, which\nis a full replacement for init with parallel starting of services, reduced\nshell overhead, increased security and resource limits for services, and other\nfeatures that are used by many distributions.\nWithin the system, SysVinit and BusyBox init treat system components as\nservices. These services are maintained as shell scripts stored in the\n``/etc/init.d/`` directory.\nSysVinit is more elaborate than BusyBox init and organizes services in\ndifferent run levels. This organization is maintained by putting links\nto the services in the ``/etc/rcN.d/`` directories, where `N/` is one\nof the following options: \"S\", \"0\", \"1\", \"2\", \"3\", \"4\", \"5\", or \"6\".\n.. note::\nEach runlevel has a dependency on the previous runlevel. This\ndependency allows the services to work properly.\nBoth SysVinit and BusyBox init are configured through the ``/etc/inittab``\nfile, with a very similar syntax, though of course BusyBox init features\nare more limited.\nIn comparison, systemd treats components as units. Using units is a\nbroader concept as compared to using a service. A unit includes several\ndifferent types of entities. ``Service`` is one of the types of entities.\nThe runlevel concept in SysVinit corresponds to the concept of a target\nin systemd, where target is also a type of supported unit.\nIn systems with SysVinit or BusyBox init, services load sequentially (i.e. one\nby one) during init and parallelization is not supported. With systemd, services\nstart in parallel. This method can have an impact on the startup performance\nof a given service, though systemd will also provide more services by default,\ntherefore increasing the total system boot time. systemd also substantially\nincreases system size because of its multiple components and the extra\ndependencies it pulls.\nOn the contrary, BusyBox init is the simplest and the lightest solution and\nalso comes with BusyBox mdev as device manager, a lighter replacement to\n:wikipedia:`udev <Udev>`, which SysVinit and systemd both use.\nThe \":ref:`device-manager`\" chapter has more details about device managers.\nUsing SysVinit with udev\n=========================\nSysVinit with the udev device manager corresponds to the\ndefault setting in Poky. This corresponds to setting::\nINIT_MANAGER = \"sysvinit\"\nUsing BusyBox init with BusyBox mdev\n====================================\nBusyBox init with BusyBox mdev is the simplest and lightest solution\nfor small root filesystems. All you need is BusyBox, which most systems\nhave anyway::\nINIT_MANAGER = \"mdev-busybox\"\nUsing systemd\n=============\nThe last option is to use systemd together with the udev device\nmanager. This is the most powerful and versatile solution, especially\nfor more complex systems::\nINIT_MANAGER = \"systemd\"\nThis will enable systemd and remove sysvinit components from the image.\nSee :yocto_git:`meta/conf/distro/include/init-manager-systemd.inc\n</poky/tree/meta/conf/distro/include/init-manager-systemd.inc>` for exact\ndetails on what this does.\nControling systemd from the target command line\n-----------------------------------------------\nHere is a quick reference for controling systemd from the command line on the\ntarget. Instead of opening and sometimes modifying files, most interaction\nhappens through the ``systemctl`` and ``journalctl`` commands:\n-  ``systemctl status``: show the status of all services\n-  ``systemctl status <service>``: show the status of one service\n-  ``systemctl [start|stop] <service>``: start or stop a service\n-  ``systemctl [enable|disable] <service>``: enable or disable a service at boot time\n-  ``systemctl list-units``: list all available units\n-  ``journalctl -a``: show all logs for all services\n-  ``journalctl -f``: show only the last log entries, and keep printing updates as they arrive\n-  ``journalctl -u``: show only logs from a particular service\nUsing systemd-journald without a traditional syslog daemon\n----------------------------------------------------------\nCounter-intuitively, ``systemd-journald`` is not a syslog runtime or provider,\nand the proper way to use ``systemd-journald`` as your sole logging mechanism is to\neffectively disable syslog entirely by setting these variables in your distribution\nconfiguration file::\nVIRTUAL-RUNTIME_syslog = \"\"\nVIRTUAL-RUNTIME_base-utils-syslog = \"\"\nDoing so will prevent ``rsyslog`` / ``busybox-syslog`` from being pulled in by\ndefault, leaving only ``systemd-journald``.\nSummary\n-------\nThe Yocto Project supports three different initialization managers, offering\nincreasing levels of complexity and functionality:\n.. list-table::\n:widths: 40 20 20 20\n:header-rows: 1\n* -\n- BusyBox init\n- SysVinit\n- systemd\n* - Size\n- Small\n- Small\n- Big [#footnote-systemd-size]_\n* - Complexity\n- Small"}
{"text": ".. _init-manager:Selecting an Initialization Manager\n- Medium\n- High\n* - Support for boot profiles\n- No\n- Yes (\"runlevels\")\n- Yes (\"targets\")\n* - Services defined as\n- Shell scripts\n- Shell scripts\n- Description files\n* - Starting services in parallel\n- No\n- No\n- Yes\n* - Setting service resource limits\n- No\n- No\n- Yes\n* - Support service isolation\n- No\n- No\n- Yes\n* - Integrated logging\n- No\n- No\n- Yes\n.. [#footnote-systemd-size] Using systemd increases the ``core-image-minimal``\nimage size by 160\\% for ``qemux86-64`` on Mickledore (4.2), compared to SysVinit."}
{"text": "\nWorking With Licenses\n*********************\nAs mentioned in the \":ref:`overview-manual/development-environment:licensing`\"\nsection in the Yocto Project Overview and Concepts Manual, open source\nprojects are open to the public and they consequently have different\nlicensing structures in place. This section describes the mechanism by\nwhich the :term:`OpenEmbedded Build System`\ntracks changes to\nlicensing text and covers how to maintain open source license compliance\nduring your project's lifecycle. The section also describes how to\nenable commercially licensed recipes, which by default are disabled.\nTracking License Changes\n========================\nThe license of an upstream project might change in the future. In order\nto prevent these changes going unnoticed, the\n:term:`LIC_FILES_CHKSUM`\nvariable tracks changes to the license text. The checksums are validated\nat the end of the configure step, and if the checksums do not match, the\nbuild will fail.\nSpecifying the ``LIC_FILES_CHKSUM`` Variable\n--------------------------------------------\nThe :term:`LIC_FILES_CHKSUM` variable contains checksums of the license text\nin the source code for the recipe. Following is an example of how to\nspecify :term:`LIC_FILES_CHKSUM`::\nLIC_FILES_CHKSUM = \"file://COPYING;md5=xxxx \\\nfile://licfile1.txt;beginline=5;endline=29;md5=yyyy \\\nfile://licfile2.txt;endline=50;md5=zzzz \\\n...\"\n.. note::\n-  When using \"beginline\" and \"endline\", realize that line numbering\nbegins with one and not zero. Also, the included lines are\ninclusive (i.e. lines five through and including 29 in the\nprevious example for ``licfile1.txt``).\n-  When a license check fails, the selected license text is included\nas part of the QA message. Using this output, you can determine\nthe exact start and finish for the needed license text.\nThe build system uses the :term:`S`\nvariable as the default directory when searching files listed in\n:term:`LIC_FILES_CHKSUM`. The previous example employs the default\ndirectory.\nConsider this next example::\nLIC_FILES_CHKSUM = \"file://src/ls.c;beginline=5;endline=16;\\\nmd5=bb14ed3c4cda583abc85401304b5cd4e\"\nLIC_FILES_CHKSUM = \"file://${WORKDIR}/license.html;md5=5c94767cedb5d6987c902ac850ded2c6\"\nThe first line locates a file in ``${S}/src/ls.c`` and isolates lines\nfive through 16 as license text. The second line refers to a file in\n:term:`WORKDIR`.\nNote that :term:`LIC_FILES_CHKSUM` variable is mandatory for all recipes,\nunless the :term:`LICENSE` variable is set to \"CLOSED\".\nExplanation of Syntax\n---------------------\nAs mentioned in the previous section, the :term:`LIC_FILES_CHKSUM` variable\nlists all the important files that contain the license text for the\nsource code. It is possible to specify a checksum for an entire file, or\na specific section of a file (specified by beginning and ending line\nnumbers with the \"beginline\" and \"endline\" parameters, respectively).\nThe latter is useful for source files with a license notice header,\nREADME documents, and so forth. If you do not use the \"beginline\"\nparameter, then it is assumed that the text begins on the first line of\nthe file. Similarly, if you do not use the \"endline\" parameter, it is\nassumed that the license text ends with the last line of the file.\nThe \"md5\" parameter stores the md5 checksum of the license text. If the\nlicense text changes in any way as compared to this parameter then a\nmismatch occurs. This mismatch triggers a build failure and notifies the\ndeveloper. Notification allows the developer to review and address the\nlicense text changes. Also note that if a mismatch occurs during the\nbuild, the correct md5 checksum is placed in the build log and can be\neasily copied to the recipe.\nThere is no limit to how many files you can specify using the\n:term:`LIC_FILES_CHKSUM` variable. Generally, however, every project\nrequires a few specifications for license tracking. Many projects have a\n\"COPYING\" file that stores the license information for all the source\ncode files. This practice allows you to just track the \"COPYING\" file as\nlong as it is kept up to date.\n.. note::\n-  If you specify an empty or invalid \"md5\" parameter,\n:term:`BitBake` returns an md5\nmis-match error and displays the correct \"md5\" parameter value\nduring the build. The correct parameter is also captured in the\nbuild log.\n-  If the whole file contains only license text, you do not need to\nuse the \"beginline\" and \"endline\" parameters.\nEnabling Commercially Licensed Recipes\n======================================\nBy default, the OpenEmbedded build system disables components that have\ncommercial or other special licensing requirements. Such requirements\nare defined on a recipe-by-recipe basis through the\n:term:`LICENSE_FLAGS` variable\ndefinition in the affected recipe. For instance, the\n``poky/meta/recipes-multimedia/gstreamer/gst-plugins-ugly`` recipe\ncontains the following statement::\nLICENSE_FLAGS = \"commercial\"\nHere is a\nslightly more complicated example that contains both an explicit recipe\nname and version (after variable expansion)::\nLICENSE_FLAGS = \"license_${PN}_${PV}\"\nIt is possible to give more details about a specific license\nusing flags on the :term:`LICENSE_FLAGS_DETAILS` variable::\nLICENSE_FLAGS_DETAILS[my-eula-license] = \"For further details, see https://example.com/eula.\""}
{"text": "\nIf set, this will be displayed to the user if the license hasn't been accepted.\nIn order for a component restricted by a\n:term:`LICENSE_FLAGS` definition to be enabled and included in an image, it\nneeds to have a matching entry in the global\n:term:`LICENSE_FLAGS_ACCEPTED`\nvariable, which is a variable typically defined in your ``local.conf``\nfile. For example, to enable the\n``poky/meta/recipes-multimedia/gstreamer/gst-plugins-ugly`` package, you\ncould add either the string \"commercial_gst-plugins-ugly\" or the more\ngeneral string \"commercial\" to :term:`LICENSE_FLAGS_ACCEPTED`. See the\n\":ref:`dev-manual/licenses:license flag matching`\" section for a full\nexplanation of how :term:`LICENSE_FLAGS` matching works. Here is the\nexample::\nLICENSE_FLAGS_ACCEPTED = \"commercial_gst-plugins-ugly\"\nLikewise, to additionally enable the package built from the recipe\ncontaining ``LICENSE_FLAGS = \"license_${PN}_${PV}\"``, and assuming that\nthe actual recipe name was ``emgd_1.10.bb``, the following string would\nenable that package as well as the original ``gst-plugins-ugly``\npackage::\nLICENSE_FLAGS_ACCEPTED = \"commercial_gst-plugins-ugly license_emgd_1.10\"\nAs a convenience, you do not need to specify the\ncomplete license string for every package. You can use\nan abbreviated form, which consists of just the first portion or\nportions of the license string before the initial underscore character\nor characters. A partial string will match any license that contains the\ngiven string as the first portion of its license. For example, the\nfollowing value will also match both of the packages\npreviously mentioned as well as any other packages that have licenses\nstarting with \"commercial\" or \"license\"::\nLICENSE_FLAGS_ACCEPTED = \"commercial license\"\nLicense Flag Matching\n---------------------\nLicense flag matching allows you to control what recipes the\nOpenEmbedded build system includes in the build. Fundamentally, the\nbuild system attempts to match :term:`LICENSE_FLAGS` strings found in\nrecipes against strings found in :term:`LICENSE_FLAGS_ACCEPTED`.\nA match causes the build system to include a recipe in the\nbuild, while failure to find a match causes the build system to exclude\na recipe.\nIn general, license flag matching is simple. However, understanding some\nconcepts will help you correctly and effectively use matching.\nBefore a flag defined by a particular recipe is tested against the\nentries of :term:`LICENSE_FLAGS_ACCEPTED`, the expanded\nstring ``_${PN}`` is appended to the flag. This expansion makes each\n:term:`LICENSE_FLAGS` value recipe-specific. After expansion, the\nstring is then matched against the entries. Thus, specifying\n``LICENSE_FLAGS = \"commercial\"`` in recipe \"foo\", for example, results\nin the string ``\"commercial_foo\"``. And, to create a match, that string\nmust appear among the entries of :term:`LICENSE_FLAGS_ACCEPTED`.\nJudicious use of the :term:`LICENSE_FLAGS` strings and the contents of the\n:term:`LICENSE_FLAGS_ACCEPTED` variable allows you a lot of flexibility for\nincluding or excluding recipes based on licensing. For example, you can\nbroaden the matching capabilities by using license flags string subsets\nin :term:`LICENSE_FLAGS_ACCEPTED`.\n.. note::\nWhen using a string subset, be sure to use the part of the expanded\nstring that precedes the appended underscore character (e.g.\n``usethispart_1.3``, ``usethispart_1.4``, and so forth).\nFor example, simply specifying the string \"commercial\" in the\n:term:`LICENSE_FLAGS_ACCEPTED` variable matches any expanded\n:term:`LICENSE_FLAGS` definition that starts with the string\n\"commercial\" such as \"commercial_foo\" and \"commercial_bar\", which\nare the strings the build system automatically generates for\nhypothetical recipes named \"foo\" and \"bar\" assuming those recipes simply\nspecify the following::\nLICENSE_FLAGS = \"commercial\"\nThus, you can choose to exhaustively enumerate each license flag in the\nlist and allow only specific recipes into the image, or you can use a\nstring subset that causes a broader range of matches to allow a range of\nrecipes into the image.\nThis scheme works even if the :term:`LICENSE_FLAGS` string already has\n``_${PN}`` appended. For example, the build system turns the license\nflag \"commercial_1.2_foo\" into \"commercial_1.2_foo_foo\" and would match\nboth the general \"commercial\" and the specific \"commercial_1.2_foo\"\nstrings found in the :term:`LICENSE_FLAGS_ACCEPTED` variable, as expected.\nHere are some other scenarios:\n-  You can specify a versioned string in the recipe such as\n\"commercial_foo_1.2\" in a \"foo\" recipe. The build system expands this\nstring to \"commercial_foo_1.2_foo\". Combine this license flag with a\n:term:`LICENSE_FLAGS_ACCEPTED` variable that has the string\n\"commercial\" and you match the flag along with any other flag that\nstarts with the string \"commercial\".\n-  Under the same circumstances, you can add \"commercial_foo\" in the\n:term:`LICENSE_FLAGS_ACCEPTED` variable and the build system not only\nmatches \"commercial_foo_1.2\" but also matches any license flag with\nthe string \"commercial_foo\", regardless of the version.\n-  You can be very specific and use both the package and version parts\nin the :term:`LICENSE_FLAGS_ACCEPTED` list (e.g.\n\"commercial_foo_1.2\") to specifically match a versioned recipe.\nOther Variables Related to Commercial Licenses\n----------------------------------------------\nThere are other helpful variables related to commercial license handling,\ndefined in the\n``poky/meta/conf/distro/include/default-distrovars.inc`` file::\nCOMMERCIAL_AUDIO_PLUGINS ?= \"\"\nCOMMERCIAL_VIDEO_PLUGINS ?= \"\"\nIf you want to enable these components, you can do so by making sure you have\nstatements similar to the following in your ``local.conf`` configuration file::\nCOMMERCIAL_AUDIO_PLUGINS = \"gst-plugins-ugly-mad \\\ngst-plugins-ugly-mpegaudioparse\""}
{"text": "\nCOMMERCIAL_VIDEO_PLUGINS = \"gst-plugins-ugly-mpeg2dec \\\ngst-plugins-ugly-mpegstream gst-plugins-bad-mpegvideoparse\"\nLICENSE_FLAGS_ACCEPTED = \"commercial_gst-plugins-ugly commercial_gst-plugins-bad commercial_qmmp\"\nOf course, you could also create a matching list for those components using the\nmore general \"commercial\" string in the :term:`LICENSE_FLAGS_ACCEPTED` variable,\nbut that would also enable all the other packages with :term:`LICENSE_FLAGS`\ncontaining \"commercial\", which you may or may not want::\nLICENSE_FLAGS_ACCEPTED = \"commercial\"\nSpecifying audio and video plugins as part of the\n:term:`COMMERCIAL_AUDIO_PLUGINS` and :term:`COMMERCIAL_VIDEO_PLUGINS` statements\n(along with :term:`LICENSE_FLAGS_ACCEPTED`) includes the plugins or\ncomponents into built images, thus adding support for media formats or\ncomponents.\n.. note::\nGStreamer \"ugly\" and \"bad\" plugins are actually available through\nopen source licenses. However, the \"ugly\" ones can be subject to software\npatents in some countries, making it necessary to pay licensing fees\nto distribute them. The \"bad\" ones are just deemed unreliable by the\nGStreamer community and should therefore be used with care.\nMaintaining Open Source License Compliance During Your Product's Lifecycle\n==========================================================================\nOne of the concerns for a development organization using open source\nsoftware is how to maintain compliance with various open source\nlicensing during the lifecycle of the product. While this section does\nnot provide legal advice or comprehensively cover all scenarios, it does\npresent methods that you can use to assist you in meeting the compliance\nrequirements during a software release.\nWith hundreds of different open source licenses that the Yocto Project\ntracks, it is difficult to know the requirements of each and every\nlicense. However, the requirements of the major FLOSS licenses can begin\nto be covered by assuming that there are three main areas of concern:\n-  Source code must be provided.\n-  License text for the software must be provided.\n-  Compilation scripts and modifications to the source code must be\nprovided.\nThere are other requirements beyond the scope of these three and the\nmethods described in this section (e.g. the mechanism through which\nsource code is distributed).\nAs different organizations have different ways of releasing software,\nthere can be multiple ways of meeting license obligations. At\nleast, we describe here two methods for achieving compliance:\n-  The first method is to use OpenEmbedded's ability to provide\nthe source code, provide a list of licenses, as well as\ncompilation scripts and source code modifications.\nThe remainder of this section describes supported methods to meet\nthe previously mentioned three requirements.\n-  The second method is to generate a *Software Bill of Materials*\n(:term:`SBoM`), as described in the \":doc:`/dev-manual/sbom`\" section.\nNot only do you generate :term:`SPDX` output which can be used meet\nlicense compliance requirements (except for sharing the build system\nand layers sources for the time being), but this output also includes\ncomponent version and patch information which can be used\nfor vulnerability assessment.\nWhatever method you choose, prior to releasing images, sources,\nand the build system, you should audit all artifacts to ensure\ncompleteness.\n.. note::\nThe Yocto Project generates a license manifest during image creation\nthat is located in\n``${DEPLOY_DIR}/licenses/${SSTATE_PKGARCH}/<image-name>-<machine>.rootfs-<datestamp>/``\nto assist with any audits.\nProviding the Source Code\n-------------------------\nCompliance activities should begin before you generate the final image.\nThe first thing you should look at is the requirement that tops the list\nfor most compliance groups --- providing the source. The Yocto Project has\na few ways of meeting this requirement.\nOne of the easiest ways to meet this requirement is to provide the\nentire :term:`DL_DIR` used by the\nbuild. This method, however, has a few issues. The most obvious is the\nsize of the directory since it includes all sources used in the build\nand not just the source used in the released image. It will include\ntoolchain source, and other artifacts, which you would not generally\nrelease. However, the more serious issue for most companies is\naccidental release of proprietary software. The Yocto Project provides\nan :ref:`ref-classes-archiver` class to help avoid some of these concerns.\nBefore you employ :term:`DL_DIR` or the :ref:`ref-classes-archiver` class, you\nneed to decide how you choose to provide source. The source\n:ref:`ref-classes-archiver` class can generate tarballs and SRPMs and can\ncreate them with various levels of compliance in mind.\nOne way of doing this (but certainly not the only way) is to release\njust the source as a tarball. You can do this by adding the following to\nthe ``local.conf`` file found in the :term:`Build Directory`::\nINHERIT += \"archiver\"\nARCHIVER_MODE[src] = \"original\"\nDuring the creation of your\nimage, the source from all recipes that deploy packages to the image is\nplaced within subdirectories of ``DEPLOY_DIR/sources`` based on the\n:term:`LICENSE` for each recipe.\nReleasing the entire directory enables you to comply with requirements\nconcerning providing the unmodified source. It is important to note that\nthe size of the directory can get large.\nA way to help mitigate the size issue is to only release tarballs for\nlicenses that require the release of source. Let us assume you are only\nconcerned with GPL code as identified by running the following script:\n.. code-block:: shell\n# Script to archive a subset of packages matching specific license(s)\n# Source and license files are copied into sub folders of package folder\n# Must be run from build folder\n#!/bin/bash"}
{"text": "\nsrc_release_dir=\"source-release\"\nmkdir -p $src_release_dir\nfor a in tmp/deploy/sources/*; do\nfor d in $a/*; do\n# Get package name from path\np=`basename $d`\np=${p%-*}\np=${p%-*}\n# Only archive GPL packages (update *GPL* regex for your license check)\nnumfiles=`ls tmp/deploy/licenses/$p/*GPL* 2> /dev/null | wc -l`\nif [ $numfiles -ge 1 ]; then\necho Archiving $p\nmkdir -p $src_release_dir/$p/source\ncp $d/* $src_release_dir/$p/source 2> /dev/null\nmkdir -p $src_release_dir/$p/license\ncp tmp/deploy/licenses/$p/* $src_release_dir/$p/license 2> /dev/null\nfi\ndone\ndone\nAt this point, you\ncould create a tarball from the ``gpl_source_release`` directory and\nprovide that to the end user. This method would be a step toward\nachieving compliance with section 3a of GPLv2 and with section 6 of\nGPLv3.\nProviding License Text\n----------------------\nOne requirement that is often overlooked is inclusion of license text.\nThis requirement also needs to be dealt with prior to generating the\nfinal image. Some licenses require the license text to accompany the\nbinary. You can achieve this by adding the following to your\n``local.conf`` file::\nCOPY_LIC_MANIFEST = \"1\"\nCOPY_LIC_DIRS = \"1\"\nLICENSE_CREATE_PACKAGE = \"1\"\nAdding these statements to the\nconfiguration file ensures that the licenses collected during package\ngeneration are included on your image.\n.. note::\nSetting all three variables to \"1\" results in the image having two\ncopies of the same license file. One copy resides in\n``/usr/share/common-licenses`` and the other resides in\n``/usr/share/license``.\nThe reason for this behavior is because\n:term:`COPY_LIC_DIRS` and\n:term:`COPY_LIC_MANIFEST`\nadd a copy of the license when the image is built but do not offer a\npath for adding licenses for newly installed packages to an image.\n:term:`LICENSE_CREATE_PACKAGE`\nadds a separate package and an upgrade path for adding licenses to an\nimage.\nAs the source :ref:`ref-classes-archiver` class has already archived the\noriginal unmodified source that contains the license files, you would have\nalready met the requirements for inclusion of the license information\nwith source as defined by the GPL and other open source licenses.\nProviding Compilation Scripts and Source Code Modifications\n-----------------------------------------------------------\nAt this point, we have addressed all we need prior to generating the\nimage. The next two requirements are addressed during the final\npackaging of the release.\nBy releasing the version of the OpenEmbedded build system and the layers\nused during the build, you will be providing both compilation scripts\nand the source code modifications in one step.\nIf the deployment team has a :ref:`overview-manual/concepts:bsp layer`\nand a distro layer, and those\nthose layers are used to patch, compile, package, or modify (in any way)\nany open source software included in your released images, you might be\nrequired to release those layers under section 3 of GPLv2 or section 1\nof GPLv3. One way of doing that is with a clean checkout of the version\nof the Yocto Project and layers used during your build. Here is an\nexample:\n.. code-block:: shell\n# We built using the dunfell branch of the poky repo\n$ git clone -b dunfell git://git.yoctoproject.org/poky\n$ cd poky\n# We built using the release_branch for our layers\n$ git clone -b release_branch git://git.mycompany.com/meta-my-bsp-layer\n$ git clone -b release_branch git://git.mycompany.com/meta-my-software-layer\n# clean up the .git repos\n$ find . -name \".git\" -type d -exec rm -rf {} \\;\nOne thing a development organization might want to consider for end-user\nconvenience is to modify\n``meta-poky/conf/templates/default/bblayers.conf.sample`` to ensure that when\nthe end user utilizes the released build system to build an image, the\ndevelopment organization's layers are included in the ``bblayers.conf`` file\nautomatically::\n# POKY_BBLAYERS_CONF_VERSION is increased each time build/conf/bblayers.conf\n# changes incompatibly\nPOKY_BBLAYERS_CONF_VERSION = \"2\"\nBBPATH = \"${TOPDIR}\"\nBBFILES ?= \"\"\nBBLAYERS ?= \" \\\n##OEROOT##/meta \\\n##OEROOT##/meta-poky \\\n##OEROOT##/meta-yocto-bsp \\\n##OEROOT##/meta-mylayer \\\n\"\nCreating and\nproviding an archive of the :term:`Metadata`\nlayers (recipes, configuration files, and so forth) enables you to meet\nyour requirements to include the scripts to control compilation as well"}
{"text": "\nas any modifications to the original source.\nCompliance Limitations with Executables Built from Static Libraries\n-------------------------------------------------------------------\nWhen package A is added to an image via the :term:`RDEPENDS` or :term:`RRECOMMENDS`\nmechanisms as well as explicitly included in the image recipe with\n:term:`IMAGE_INSTALL`, and depends on a static linked library recipe B\n(``DEPENDS += \"B\"``), package B will neither appear in the generated license\nmanifest nor in the generated source tarballs.  This occurs as the\n:ref:`ref-classes-license` and :ref:`ref-classes-archiver` classes assume that\nonly packages included via :term:`RDEPENDS` or :term:`RRECOMMENDS`\nend up in the image.\nAs a result, potential obligations regarding license compliance for package B\nmay not be met.\nThe Yocto Project doesn't enable static libraries by default, in part because\nof this issue. Before a solution to this limitation is found, you need to\nkeep in mind that if your root filesystem is built from static libraries,\nyou will need to manually ensure that your deliveries are compliant\nwith the licenses of these libraries.\nCopying Non Standard Licenses\n=============================\nSome packages, such as the linux-firmware package, have many licenses\nthat are not in any way common. You can avoid adding a lot of these\ntypes of common license files, which are only applicable to a specific\npackage, by using the\n:term:`NO_GENERIC_LICENSE`\nvariable. Using this variable also avoids QA errors when you use a\nnon-common, non-CLOSED license in a recipe.\nHere is an example that uses the ``LICENSE.Abilis.txt`` file as\nthe license from the fetched source::\nNO_GENERIC_LICENSE[Firmware-Abilis] = \"LICENSE.Abilis.txt\""}
{"text": "\nUsing an External SCM\n*********************\nIf you're working on a recipe that pulls from an external Source Code\nManager (SCM), it is possible to have the OpenEmbedded build system\nnotice new recipe changes added to the SCM and then build the resulting\npackages that depend on the new recipes by using the latest versions.\nThis only works for SCMs from which it is possible to get a sensible\nrevision number for changes. Currently, you can do this with Apache\nSubversion (SVN), Git, and Bazaar (BZR) repositories.\nTo enable this behavior, the :term:`PV` of\nthe recipe needs to reference\n:term:`SRCPV`. Here is an example::\nPV = \"1.2.3+git${SRCPV}\"\nThen, you can add the following to your\n``local.conf``::\nSRCREV:pn-PN = \"${AUTOREV}\"\n:term:`PN` is the name of the recipe for\nwhich you want to enable automatic source revision updating.\nIf you do not want to update your local configuration file, you can add\nthe following directly to the recipe to finish enabling the feature::\nSRCREV = \"${AUTOREV}\"\nThe Yocto Project provides a distribution named ``poky-bleeding``, whose\nconfiguration file contains the line::\nrequire conf/distro/include/poky-floating-revisions.inc\nThis line pulls in the\nlisted include file that contains numerous lines of exactly that form::\n#SRCREV:pn-opkg-native ?= \"${AUTOREV}\"\n#SRCREV:pn-opkg-sdk ?= \"${AUTOREV}\"\n#SRCREV:pn-opkg ?= \"${AUTOREV}\"\n#SRCREV:pn-opkg-utils-native ?= \"${AUTOREV}\"\n#SRCREV:pn-opkg-utils ?= \"${AUTOREV}\"\nSRCREV:pn-gconf-dbus ?= \"${AUTOREV}\"\nSRCREV:pn-matchbox-common ?= \"${AUTOREV}\"\nSRCREV:pn-matchbox-config-gtk ?= \"${AUTOREV}\"\nSRCREV:pn-matchbox-desktop ?= \"${AUTOREV}\"\nSRCREV:pn-matchbox-keyboard ?= \"${AUTOREV}\"\nSRCREV:pn-matchbox-panel-2 ?= \"${AUTOREV}\"\nSRCREV:pn-matchbox-themes-extra ?= \"${AUTOREV}\"\nSRCREV:pn-matchbox-terminal ?= \"${AUTOREV}\"\nSRCREV:pn-matchbox-wm ?= \"${AUTOREV}\"\nSRCREV:pn-settings-daemon ?= \"${AUTOREV}\"\nSRCREV:pn-screenshot ?= \"${AUTOREV}\"\n. . .\nThese lines allow you to\nexperiment with building a distribution that tracks the latest\ndevelopment source for numerous packages.\n.. note::\nThe ``poky-bleeding`` distribution is not tested on a regular basis. Keep\nthis in mind if you use it."}
{"text": "\nCreating Partitioned Images Using Wic\n*************************************\nCreating an image for a particular hardware target using the\nOpenEmbedded build system does not necessarily mean you can boot that\nimage as is on your device. Physical devices accept and boot images in\nvarious ways depending on the specifics of the device. Usually,\ninformation about the hardware can tell you what image format the device\nrequires. Should your device require multiple partitions on an SD card,\nflash, or an HDD, you can use the OpenEmbedded Image Creator, Wic, to\ncreate the properly partitioned image.\nThe ``wic`` command generates partitioned images from existing\nOpenEmbedded build artifacts. Image generation is driven by partitioning\ncommands contained in an OpenEmbedded kickstart file (``.wks``)\nspecified either directly on the command line or as one of a selection\nof canned kickstart files as shown with the ``wic list images`` command\nin the\n\":ref:`dev-manual/wic:generate an image using an existing kickstart file`\"\nsection. When you apply the command to a given set of build artifacts, the\nresult is an image or set of images that can be directly written onto media and\nused on a particular system.\n.. note::\nFor a kickstart file reference, see the\n\":ref:`ref-manual/kickstart:openembedded kickstart (\\`\\`.wks\\`\\`) reference`\"\nChapter in the Yocto Project Reference Manual.\nThe ``wic`` command and the infrastructure it is based on is by\ndefinition incomplete. The purpose of the command is to allow the\ngeneration of customized images, and as such, was designed to be\ncompletely extensible through a plugin interface. See the\n\":ref:`dev-manual/wic:using the wic plugin interface`\" section\nfor information on these plugins.\nThis section provides some background information on Wic, describes what\nyou need to have in place to run the tool, provides instruction on how\nto use the Wic utility, provides information on using the Wic plugins\ninterface, and provides several examples that show how to use Wic.\nBackground\n==========\nThis section provides some background on the Wic utility. While none of\nthis information is required to use Wic, you might find it interesting.\n-  The name \"Wic\" is derived from OpenEmbedded Image Creator (oeic). The\n\"oe\" diphthong in \"oeic\" was promoted to the letter \"w\", because\n\"oeic\" is both difficult to remember and to pronounce.\n-  Wic is loosely based on the Meego Image Creator (``mic``) framework.\nThe Wic implementation has been heavily modified to make direct use\nof OpenEmbedded build artifacts instead of package installation and\nconfiguration, which are already incorporated within the OpenEmbedded\nartifacts.\n-  Wic is a completely independent standalone utility that initially\nprovides easier-to-use and more flexible replacements for an existing\nfunctionality in OE-Core's :ref:`ref-classes-image-live`\nclass. The difference between Wic and those examples is that with Wic\nthe functionality of those scripts is implemented by a\ngeneral-purpose partitioning language, which is based on Redhat\nkickstart syntax.\nRequirements\n============\nIn order to use the Wic utility with the OpenEmbedded Build system, your\nsystem needs to meet the following requirements:\n-  The Linux distribution on your development host must support the\nYocto Project. See the \":ref:`system-requirements-supported-distros`\"\nsection in the Yocto Project Reference Manual for the list of\ndistributions that support the Yocto Project.\n-  The standard system utilities, such as ``cp``, must be installed on\nyour development host system.\n-  You must have sourced the build environment setup script (i.e.\n:ref:`structure-core-script`) found in the :term:`Build Directory`.\n-  You need to have the build artifacts already available, which\ntypically means that you must have already created an image using the\nOpenEmbedded build system (e.g. ``core-image-minimal``). While it\nmight seem redundant to generate an image in order to create an image\nusing Wic, the current version of Wic requires the artifacts in the\nform generated by the OpenEmbedded build system.\n-  You must build several native tools, which are built to run on the\nbuild system::\n$ bitbake wic-tools\n-  Include \"wic\" as part of the\n:term:`IMAGE_FSTYPES`\nvariable.\n-  Include the name of the :ref:`wic kickstart file <openembedded-kickstart-wks-reference>`\nas part of the :term:`WKS_FILE` variable. If multiple candidate files can\nbe provided by different layers, specify all the possible names through the\n:term:`WKS_FILES` variable instead.\nGetting Help\n============\nYou can get general help for the ``wic`` command by entering the ``wic``\ncommand by itself or by entering the command with a help argument as\nfollows::\n$ wic -h\n$ wic --help\n$ wic help\nCurrently, Wic supports seven commands: ``cp``, ``create``, ``help``,\n``list``, ``ls``, ``rm``, and ``write``. You can get help for all these\ncommands except \"help\" by using the following form::\n$ wic help command\nFor example, the following command returns help for the ``write``\ncommand::\n$ wic help write\nWic supports help for three topics: ``overview``, ``plugins``, and\n``kickstart``. You can get help for any topic using the following form::\n$ wic help topic"}
{"text": "\nFor example, the following returns overview help for Wic::\n$ wic help overview\nThere is one additional level of help for Wic. You can get help on\nindividual images through the ``list`` command. You can use the ``list``\ncommand to return the available Wic images as follows::\n$ wic list images\ngenericx86                    \t\tCreate an EFI disk image for genericx86*\nbeaglebone-yocto              \t\tCreate SD card image for Beaglebone\nqemuriscv                     \t\tCreate qcow2 image for RISC-V QEMU machines\nmkefidisk                     \t\tCreate an EFI disk image\nqemuloongarch                 \t\tCreate qcow2 image for LoongArch QEMU machines\ndirectdisk-multi-rootfs       \t\tCreate multi rootfs image using rootfs plugin\ndirectdisk                    \t\tCreate a 'pcbios' direct disk image\nefi-bootdisk\nmkhybridiso                   \t\tCreate a hybrid ISO image\ndirectdisk-gpt                \t\tCreate a 'pcbios' direct disk image\nsystemd-bootdisk              \t\tCreate an EFI disk image with systemd-boot\nsdimage-bootpart              \t\tCreate SD card image with a boot partition\nqemux86-directdisk            \t\tCreate a qemu machine 'pcbios' direct disk image\ndirectdisk-bootloader-config  \t\tCreate a 'pcbios' direct disk image with custom bootloader config\nOnce you know the list of available\nWic images, you can use ``help`` with the command to get help on a\nparticular image. For example, the following command returns help on the\n\"beaglebone-yocto\" image::\n$ wic list beaglebone-yocto help\nCreates a partitioned SD card image for Beaglebone.\nBoot files are located in the first vfat partition.\nOperational Modes\n=================\nYou can use Wic in two different modes, depending on how much control\nyou need for specifying the OpenEmbedded build artifacts that are used\nfor creating the image: Raw and Cooked:\n-  *Raw Mode:* You explicitly specify build artifacts through Wic\ncommand-line arguments.\n-  *Cooked Mode:* The current\n:term:`MACHINE` setting and image\nname are used to automatically locate and provide the build\nartifacts. You just supply a kickstart file and the name of the image\nfrom which to use artifacts.\nRegardless of the mode you use, you need to have the build artifacts\nready and available.\nRaw Mode\n--------\nRunning Wic in raw mode allows you to specify all the partitions through\nthe ``wic`` command line. The primary use for raw mode is if you have\nbuilt your kernel outside of the Yocto Project :term:`Build Directory`.\nIn other words, you can point to arbitrary kernel, root filesystem locations,\nand so forth. Contrast this behavior with cooked mode where Wic looks in the\n:term:`Build Directory` (e.g. ``tmp/deploy/images/``\\ machine).\nThe general form of the ``wic`` command in raw mode is::\n$ wic create wks_file options ...\nWhere:\nwks_file:\nAn OpenEmbedded kickstart file.  You can provide\nyour own custom file or use a file from a set of\nexisting files as described by further options.\noptional arguments:\n-h, --help            show this help message and exit\n-o OUTDIR, --outdir OUTDIR\nname of directory to create image in\n-e IMAGE_NAME, --image-name IMAGE_NAME\nname of the image to use the artifacts from e.g. core-\nimage-sato\n-r ROOTFS_DIR, --rootfs-dir ROOTFS_DIR\npath to the /rootfs dir to use as the .wks rootfs\nsource\n-b BOOTIMG_DIR, --bootimg-dir BOOTIMG_DIR\npath to the dir containing the boot artifacts (e.g.\n/EFI or /syslinux dirs) to use as the .wks bootimg\nsource\n-k KERNEL_DIR, --kernel-dir KERNEL_DIR\npath to the dir containing the kernel to use in the\n.wks bootimg\n-n NATIVE_SYSROOT, --native-sysroot NATIVE_SYSROOT\npath to the native sysroot containing the tools to use\nto build the image\n-s, --skip-build-check\nskip the build check\n-f, --build-rootfs    build rootfs\n-c {gzip,bzip2,xz}, --compress-with {gzip,bzip2,xz}\ncompress image with specified compressor\n-m, --bmap            generate .bmap\n--no-fstab-update     Do not change fstab file.\n-v VARS_DIR, --vars VARS_DIR\ndirectory with <image>.env files that store bitbake\nvariables\n-D, --debug           output debug information\n.. note::\nYou do not need root privileges to run Wic. In fact, you should not\nrun as root when using the utility.\nCooked Mode\n-----------\nRunning Wic in cooked mode leverages off artifacts in the\n:term:`Build Directory`. In other words, you do not have to specify kernel or\nroot filesystem locations as part of the command. All you need to provide is\na kickstart file and the name of the image from which to use artifacts\nby using the \"-e\" option. Wic looks in the :term:`Build Directory` (e.g.\n``tmp/deploy/images/``\\ machine) for artifacts.\nThe general form of the ``wic`` command using Cooked Mode is as follows::\n$ wic create wks_file -e IMAGE_NAME"}
{"text": "\nWhere:\nwks_file:\nAn OpenEmbedded kickstart file.  You can provide\nyour own custom file or use a file from a set of\nexisting files provided with the Yocto Project\nrelease.\nrequired argument:\n-e IMAGE_NAME, --image-name IMAGE_NAME\nname of the image to use the artifacts from e.g. core-\nimage-sato\nUsing an Existing Kickstart File\n================================\nIf you do not want to create your own kickstart file, you can use an\nexisting file provided by the Wic installation. As shipped, kickstart\nfiles can be found in the :ref:`overview-manual/development-environment:yocto project source repositories` in the\nfollowing two locations::\npoky/meta-yocto-bsp/wic\npoky/scripts/lib/wic/canned-wks\nUse the following command to list the available kickstart files::\n$ wic list images\ngenericx86                    \t\tCreate an EFI disk image for genericx86*\nbeaglebone-yocto              \t\tCreate SD card image for Beaglebone\nqemuriscv                     \t\tCreate qcow2 image for RISC-V QEMU machines\nmkefidisk                     \t\tCreate an EFI disk image\nqemuloongarch                 \t\tCreate qcow2 image for LoongArch QEMU machines\ndirectdisk-multi-rootfs       \t\tCreate multi rootfs image using rootfs plugin\ndirectdisk                    \t\tCreate a 'pcbios' direct disk image\nefi-bootdisk\nmkhybridiso                   \t\tCreate a hybrid ISO image\ndirectdisk-gpt                \t\tCreate a 'pcbios' direct disk image\nsystemd-bootdisk              \t\tCreate an EFI disk image with systemd-boot\nsdimage-bootpart              \t\tCreate SD card image with a boot partition\nqemux86-directdisk            \t\tCreate a qemu machine 'pcbios' direct disk image\ndirectdisk-bootloader-config  \t\tCreate a 'pcbios' direct disk image with custom bootloader config\nWhen you use an existing file, you\ndo not have to use the ``.wks`` extension. Here is an example in Raw\nMode that uses the ``directdisk`` file::\n$ wic create directdisk -r rootfs_dir -b bootimg_dir \\\n-k kernel_dir -n native_sysroot\nHere are the actual partition language commands used in the\n``genericx86.wks`` file to generate an image::\n# short-description: Create an EFI disk image for genericx86*\n# long-description: Creates a partitioned EFI disk image for genericx86* machines\npart /boot --source bootimg-efi --sourceparams=\"loader=grub-efi\" --ondisk sda --label msdos --active --align 1024\npart / --source rootfs --ondisk sda --fstype=ext4 --label platform --align 1024 --use-uuid\npart swap --ondisk sda --size 44 --label swap1 --fstype=swap\nbootloader --ptable gpt --timeout=5 --append=\"rootfstype=ext4 console=ttyS0,115200 console=tty0\"\nUsing the Wic Plugin Interface\n==============================\nYou can extend and specialize Wic functionality by using Wic plugins.\nThis section explains the Wic plugin interface.\n.. note::\nWic plugins consist of \"source\" and \"imager\" plugins. Imager plugins\nare beyond the scope of this section.\nSource plugins provide a mechanism to customize partition content during\nthe Wic image generation process. You can use source plugins to map\nvalues that you specify using ``--source`` commands in kickstart files\n(i.e. ``*.wks``) to a plugin implementation used to populate a given\npartition.\n.. note::\nIf you use plugins that have build-time dependencies (e.g. native\ntools, bootloaders, and so forth) when building a Wic image, you need\nto specify those dependencies using the :term:`WKS_FILE_DEPENDS`\nvariable.\nSource plugins are subclasses defined in plugin files. As shipped, the\nYocto Project provides several plugin files. You can see the source\nplugin files that ship with the Yocto Project\n:yocto_git:`here </poky/tree/scripts/lib/wic/plugins/source>`.\nEach of these plugin files contains source plugins that are designed to\npopulate a specific Wic image partition.\nSource plugins are subclasses of the ``SourcePlugin`` class, which is\ndefined in the ``poky/scripts/lib/wic/pluginbase.py`` file. For example,\nthe ``BootimgEFIPlugin`` source plugin found in the ``bootimg-efi.py``\nfile is a subclass of the ``SourcePlugin`` class, which is found in the\n``pluginbase.py`` file.\nYou can also implement source plugins in a layer outside of the Source\nRepositories (external layer). To do so, be sure that your plugin files\nare located in a directory whose path is\n``scripts/lib/wic/plugins/source/`` within your external layer. When the\nplugin files are located there, the source plugins they contain are made\navailable to Wic.\nWhen the Wic implementation needs to invoke a partition-specific\nimplementation, it looks for the plugin with the same name as the\n``--source`` parameter used in the kickstart file given to that\npartition. For example, if the partition is set up using the following\ncommand in a kickstart file::\npart /boot --source bootimg-pcbios --ondisk sda --label boot --active --align 1024\nThe methods defined as class\nmembers of the matching source plugin (i.e. ``bootimg-pcbios``) in the\n``bootimg-pcbios.py`` plugin file are used.\nTo be more concrete, here is the corresponding plugin definition from\nthe ``bootimg-pcbios.py`` file for the previous command along with an\nexample method called by the Wic implementation when it needs to prepare\na partition using an implementation-specific function::\n.\n.\n.\nclass BootimgPcbiosPlugin(SourcePlugin):\n\"\"\"\nCreate MBR boot partition and install syslinux on it."}
{"text": "\n\"\"\"\nname = 'bootimg-pcbios'\n.\n.\n.\n@classmethod\ndef do_prepare_partition(cls, part, source_params, creator, cr_workdir,\noe_builddir, bootimg_dir, kernel_dir,\nrootfs_dir, native_sysroot):\n\"\"\"\nCalled to do the actual content population for a partition i.e. it\n'prepares' the partition to be incorporated into the image.\nIn this case, prepare content for legacy bios boot partition.\n\"\"\"\n.\n.\n.\nIf a\nsubclass (plugin) itself does not implement a particular function, Wic\nlocates and uses the default version in the superclass. It is for this\nreason that all source plugins are derived from the ``SourcePlugin``\nclass.\nThe ``SourcePlugin`` class defined in the ``pluginbase.py`` file defines\na set of methods that source plugins can implement or override. Any\nplugins (subclass of ``SourcePlugin``) that do not implement a\nparticular method inherit the implementation of the method from the\n``SourcePlugin`` class. For more information, see the ``SourcePlugin``\nclass in the ``pluginbase.py`` file for details:\nThe following list describes the methods implemented in the\n``SourcePlugin`` class:\n-  ``do_prepare_partition()``: Called to populate a partition with\nactual content. In other words, the method prepares the final\npartition image that is incorporated into the disk image.\n-  ``do_configure_partition()``: Called before\n``do_prepare_partition()`` to create custom configuration files for a\npartition (e.g. syslinux or grub configuration files).\n-  ``do_install_disk()``: Called after all partitions have been\nprepared and assembled into a disk image. This method provides a hook\nto allow finalization of a disk image (e.g. writing an MBR).\n-  ``do_stage_partition()``: Special content-staging hook called\nbefore ``do_prepare_partition()``. This method is normally empty.\nTypically, a partition just uses the passed-in parameters (e.g. the\nunmodified value of ``bootimg_dir``). However, in some cases, things\nmight need to be more tailored. As an example, certain files might\nadditionally need to be taken from ``bootimg_dir + /boot``. This hook\nallows those files to be staged in a customized fashion.\n.. note::\n``get_bitbake_var()`` allows you to access non-standard variables that\nyou might want to use for this behavior.\nYou can extend the source plugin mechanism. To add more hooks, create\nmore source plugin methods within ``SourcePlugin`` and the corresponding\nderived subclasses. The code that calls the plugin methods uses the\n``plugin.get_source_plugin_methods()`` function to find the method or\nmethods needed by the call. Retrieval of those methods is accomplished\nby filling up a dict with keys that contain the method names of\ninterest. On success, these will be filled in with the actual methods.\nSee the Wic implementation for examples and details.\nWic Examples\n============\nThis section provides several examples that show how to use the Wic\nutility. All the examples assume the list of requirements in the\n\":ref:`dev-manual/wic:requirements`\" section have been met. The\nexamples assume the previously generated image is\n``core-image-minimal``.\nGenerate an Image using an Existing Kickstart File\n--------------------------------------------------\nThis example runs in Cooked Mode and uses the ``mkefidisk`` kickstart\nfile::\n$ wic create mkefidisk -e core-image-minimal\nINFO: Building wic-tools...\n.\n.\n.\nINFO: The new image(s) can be found here:\n./mkefidisk-201804191017-sda.direct\nThe following build artifacts were used to create the image(s):\nROOTFS_DIR:                   /home/stephano/yocto/build/tmp-glibc/work/qemux86-oe-linux/core-image-minimal/1.0-r0/rootfs\nBOOTIMG_DIR:                  /home/stephano/yocto/build/tmp-glibc/work/qemux86-oe-linux/core-image-minimal/1.0-r0/recipe-sysroot/usr/share\nKERNEL_DIR:                   /home/stephano/yocto/build/tmp-glibc/deploy/images/qemux86\nNATIVE_SYSROOT:               /home/stephano/yocto/build/tmp-glibc/work/i586-oe-linux/wic-tools/1.0-r0/recipe-sysroot-native\nINFO: The image(s) were created using OE kickstart file:\n/home/stephano/yocto/openembedded-core/scripts/lib/wic/canned-wks/mkefidisk.wks\nThe previous example shows the easiest way to create an image by running\nin cooked mode and supplying a kickstart file and the \"-e\" option to\npoint to the existing build artifacts. Your ``local.conf`` file needs to\nhave the :term:`MACHINE` variable set\nto the machine you are using, which is \"qemux86\" in this example.\nOnce the image builds, the output provides image location, artifact use,\nand kickstart file information.\n.. note::\nYou should always verify the details provided in the output to make\nsure that the image was indeed created exactly as expected.\nContinuing with the example, you can now write the image from the\n:term:`Build Directory` onto a USB stick, or whatever media for which you\nbuilt your image, and boot from the media. You can write the image by using\n``bmaptool`` or ``dd``::\n$ oe-run-native bmap-tools-native bmaptool copy mkefidisk-201804191017-sda.direct /dev/sdX\nor ::\n$ sudo dd if=mkefidisk-201804191017-sda.direct of=/dev/sdX\n.. note::"}
{"text": "\nFor more information on how to use the ``bmaptool``\nto flash a device with an image, see the\n\":ref:`dev-manual/bmaptool:flashing images using \\`\\`bmaptool\\`\\``\"\nsection.\nUsing a Modified Kickstart File\n-------------------------------\nBecause partitioned image creation is driven by the kickstart file, it\nis easy to affect image creation by changing the parameters in the file.\nThis next example demonstrates that through modification of the\n``directdisk-gpt`` kickstart file.\nAs mentioned earlier, you can use the command ``wic list images`` to\nshow the list of existing kickstart files. The directory in which the\n``directdisk-gpt.wks`` file resides is\n``scripts/lib/image/canned-wks/``, which is located in the\n:term:`Source Directory` (e.g. ``poky``).\nBecause available files reside in this directory, you can create and add\nyour own custom files to the directory. Subsequent use of the\n``wic list images`` command would then include your kickstart files.\nIn this example, the existing ``directdisk-gpt`` file already does most\nof what is needed. However, for the hardware in this example, the image\nwill need to boot from ``sdb`` instead of ``sda``, which is what the\n``directdisk-gpt`` kickstart file uses.\nThe example begins by making a copy of the ``directdisk-gpt.wks`` file\nin the ``scripts/lib/image/canned-wks`` directory and then by changing\nthe lines that specify the target disk from which to boot::\n$ cp /home/stephano/yocto/poky/scripts/lib/wic/canned-wks/directdisk-gpt.wks \\\n/home/stephano/yocto/poky/scripts/lib/wic/canned-wks/directdisksdb-gpt.wks\nNext, the example modifies the ``directdisksdb-gpt.wks`` file and\nchanges all instances of \"``--ondisk sda``\" to \"``--ondisk sdb``\". The\nexample changes the following two lines and leaves the remaining lines\nuntouched::\npart /boot --source bootimg-pcbios --ondisk sdb --label boot --active --align 1024\npart / --source rootfs --ondisk sdb --fstype=ext4 --label platform --align 1024 --use-uuid\nOnce the lines are changed, the\nexample generates the ``directdisksdb-gpt`` image. The command points\nthe process at the ``core-image-minimal`` artifacts for the Next Unit of\nComputing (nuc) :term:`MACHINE` the\n``local.conf``::\n$ wic create directdisksdb-gpt -e core-image-minimal\nINFO: Building wic-tools...\n.\n.\n.\nInitialising tasks: 100% |#######################################| Time: 0:00:01\nNOTE: Executing SetScene Tasks\nNOTE: Executing RunQueue Tasks\nNOTE: Tasks Summary: Attempted 1161 tasks of which 1157 didn't need to be rerun and all succeeded.\nINFO: Creating image(s)...\nINFO: The new image(s) can be found here:\n./directdisksdb-gpt-201710090938-sdb.direct\nThe following build artifacts were used to create the image(s):\nROOTFS_DIR:                   /home/stephano/yocto/build/tmp-glibc/work/qemux86-oe-linux/core-image-minimal/1.0-r0/rootfs\nBOOTIMG_DIR:                  /home/stephano/yocto/build/tmp-glibc/work/qemux86-oe-linux/core-image-minimal/1.0-r0/recipe-sysroot/usr/share\nKERNEL_DIR:                   /home/stephano/yocto/build/tmp-glibc/deploy/images/qemux86\nNATIVE_SYSROOT:               /home/stephano/yocto/build/tmp-glibc/work/i586-oe-linux/wic-tools/1.0-r0/recipe-sysroot-native\nINFO: The image(s) were created using OE kickstart file:\n/home/stephano/yocto/poky/scripts/lib/wic/canned-wks/directdisksdb-gpt.wks\nContinuing with the example, you can now directly ``dd`` the image to a\nUSB stick, or whatever media for which you built your image, and boot\nthe resulting media::\n$ sudo dd if=directdisksdb-gpt-201710090938-sdb.direct of=/dev/sdb\n140966+0 records in\n140966+0 records out\n72174592 bytes (72 MB, 69 MiB) copied, 78.0282 s, 925 kB/s\n$ sudo eject /dev/sdb\nUsing a Modified Kickstart File and Running in Raw Mode\n-------------------------------------------------------\nThis next example manually specifies each build artifact (runs in Raw\nMode) and uses a modified kickstart file. The example also uses the\n``-o`` option to cause Wic to create the output somewhere other than the\ndefault output directory, which is the current directory::\n$ wic create test.wks -o /home/stephano/testwic \\\n--rootfs-dir /home/stephano/yocto/build/tmp/work/qemux86-poky-linux/core-image-minimal/1.0-r0/rootfs \\\n--bootimg-dir /home/stephano/yocto/build/tmp/work/qemux86-poky-linux/core-image-minimal/1.0-r0/recipe-sysroot/usr/share \\\n--kernel-dir /home/stephano/yocto/build/tmp/deploy/images/qemux86 \\\n--native-sysroot /home/stephano/yocto/build/tmp/work/i586-poky-linux/wic-tools/1.0-r0/recipe-sysroot-native\nINFO: Creating image(s)...\nINFO: The new image(s) can be found here:\n/home/stephano/testwic/test-201710091445-sdb.direct\nThe following build artifacts were used to create the image(s):\nROOTFS_DIR:                   /home/stephano/yocto/build/tmp-glibc/work/qemux86-oe-linux/core-image-minimal/1.0-r0/rootfs\nBOOTIMG_DIR:                  /home/stephano/yocto/build/tmp-glibc/work/qemux86-oe-linux/core-image-minimal/1.0-r0/recipe-sysroot/usr/share\nKERNEL_DIR:                   /home/stephano/yocto/build/tmp-glibc/deploy/images/qemux86\nNATIVE_SYSROOT:               /home/stephano/yocto/build/tmp-glibc/work/i586-oe-linux/wic-tools/1.0-r0/recipe-sysroot-native\nINFO: The image(s) were created using OE kickstart file:\ntest.wks\nFor this example,\n:term:`MACHINE` did not have to be\nspecified in the ``local.conf`` file since the artifact is manually\nspecified.\nUsing Wic to Manipulate an Image\n--------------------------------\nWic image manipulation allows you to shorten turnaround time during\nimage development. For example, you can use Wic to delete the kernel\npartition of a Wic image and then insert a newly built kernel. This\nsaves you time from having to rebuild the entire image each time you\nmodify the kernel.\n.. note::\nIn order to use Wic to manipulate a Wic image as in this example,\nyour development machine must have the ``mtools`` package installed."}
{"text": "\nThe following example examines the contents of the Wic image, deletes\nthe existing kernel, and then inserts a new kernel:\n#. *List the Partitions:* Use the ``wic ls`` command to list all the\npartitions in the Wic image::\n$ wic ls tmp/deploy/images/qemux86/core-image-minimal-qemux86.wic\nNum     Start        End          Size      Fstype\n1       1048576     25041919     23993344  fat16\n2      25165824     72157183     46991360  ext4\nThe previous output shows two partitions in the\n``core-image-minimal-qemux86.wic`` image.\n#. *Examine a Particular Partition:* Use the ``wic ls`` command again\nbut in a different form to examine a particular partition.\n.. note::\nYou can get command usage on any Wic command using the following\nform::\n$ wic help command\nFor example, the following command shows you the various ways to\nuse the\nwic ls\ncommand::\n$ wic help ls\nThe following command shows what is in partition one::\n$ wic ls tmp/deploy/images/qemux86/core-image-minimal-qemux86.wic:1\nVolume in drive : is boot\nVolume Serial Number is E894-1809\nDirectory for ::/\nlibcom32 c32    186500 2017-10-09  16:06\nlibutil  c32     24148 2017-10-09  16:06\nsyslinux cfg       220 2017-10-09  16:06\nvesamenu c32     27104 2017-10-09  16:06\nvmlinuz        6904608 2017-10-09  16:06\n5 files           7 142 580 bytes\n16 582 656 bytes free\nThe previous output shows five files, with the\n``vmlinuz`` being the kernel.\n.. note::\nIf you see the following error, you need to update or create a\n``~/.mtoolsrc`` file and be sure to have the line \"mtools_skip_check=1\"\nin the file. Then, run the Wic command again::\nERROR: _exec_cmd: /usr/bin/mdir -i /tmp/wic-parttfokuwra ::/ returned '1' instead of 0\noutput: Total number of sectors (47824) not a multiple of sectors per track (32)!\nAdd mtools_skip_check=1 to your .mtoolsrc file to skip this test\n#. *Remove the Old Kernel:* Use the ``wic rm`` command to remove the\n``vmlinuz`` file (kernel)::\n$ wic rm tmp/deploy/images/qemux86/core-image-minimal-qemux86.wic:1/vmlinuz\n#. *Add In the New Kernel:* Use the ``wic cp`` command to add the\nupdated kernel to the Wic image. Depending on how you built your\nkernel, it could be in different places. If you used ``devtool`` and\nan SDK to build your kernel, it resides in the ``tmp/work`` directory\nof the extensible SDK. If you used ``make`` to build the kernel, the\nkernel will be in the ``workspace/sources`` area.\nThe following example assumes ``devtool`` was used to build the\nkernel::\n$ wic cp poky_sdk/tmp/work/qemux86-poky-linux/linux-yocto/4.12.12+git999-r0/linux-yocto-4.12.12+git999/arch/x86/boot/bzImage \\\npoky/build/tmp/deploy/images/qemux86/core-image-minimal-qemux86.wic:1/vmlinuz\nOnce the new kernel is added back into the image, you can use the\n``dd`` command or :ref:`bmaptool\n<dev-manual/bmaptool:flashing images using \\`\\`bmaptool\\`\\`>`\nto flash your wic image onto an SD card or USB stick and test your\ntarget.\n.. note::\nUsing ``bmaptool`` is generally 10 to 20 times faster than using ``dd``."}
{"text": "\nCreating a Custom Template Configuration Directory\n**************************************************\nIf you are producing your own customized version of the build system for\nuse by other users, you might want to provide a custom build configuration\nthat includes all the necessary settings and layers (i.e. ``local.conf`` and\n``bblayers.conf`` that are created in a new :term:`Build Directory`) and a custom\nmessage that is shown when setting up the build. This can be done by\ncreating one or more template configuration directories in your\ncustom distribution layer.\nThis can be done by using ``bitbake-layers save-build-conf``::\n$ bitbake-layers save-build-conf ../../meta-alex/ test-1\nNOTE: Starting bitbake server...\nNOTE: Configuration template placed into /srv/work/alex/meta-alex/conf/templates/test-1\nPlease review the files in there, and particularly provide a configuration description in /srv/work/alex/meta-alex/conf/templates/test-1/conf-notes.txt\nYou can try out the configuration with\nTEMPLATECONF=/srv/work/alex/meta-alex/conf/templates/test-1 . /srv/work/alex/poky/oe-init-build-env build-try-test-1\nThe above command takes the config files from the currently active :term:`Build Directory` under ``conf``,\nreplaces site-specific paths in ``bblayers.conf`` with ``##OECORE##``-relative paths, and copies\nthe config files into a specified layer under a specified template name.\nTo use those saved templates as a starting point for a build, users should point\nto one of them with :term:`TEMPLATECONF` environment variable::\nTEMPLATECONF=/srv/work/alex/meta-alex/conf/templates/test-1 . /srv/work/alex/poky/oe-init-build-env build-try-test-1\nThe OpenEmbedded build system uses the environment variable\n:term:`TEMPLATECONF` to locate the directory from which it gathers\nconfiguration information that ultimately ends up in the\n:term:`Build Directory` ``conf`` directory.\nIf :term:`TEMPLATECONF` is not set, the default value is obtained\nfrom ``.templateconf`` file that is read from the same directory as\n``oe-init-build-env`` script. For the Poky reference distribution this\nwould be::\nTEMPLATECONF=${TEMPLATECONF:-meta-poky/conf/templates/default}\nIf you look at a configuration template directory, you will\nsee the ``bblayers.conf.sample``, ``local.conf.sample``, and\n``conf-notes.txt`` files. The build system uses these files to form the\nrespective ``bblayers.conf`` file, ``local.conf`` file, and show\nusers a note about the build they're setting up\nwhen running the ``oe-init-build-env`` setup script. These can be\nedited further if needed to improve or change the build configurations\navailable to the users."}
{"text": "\nUsing Quilt in Your Workflow\n****************************\n`Quilt <https://savannah.nongnu.org/projects/quilt>`__ is a powerful tool\nthat allows you to capture source code changes without having a clean\nsource tree. This section outlines the typical workflow you can use to\nmodify source code, test changes, and then preserve the changes in the\nform of a patch all using Quilt.\n.. note::\nWith regard to preserving changes to source files, if you clean a\nrecipe or have :ref:`ref-classes-rm-work` enabled, the\n:ref:`devtool workflow <sdk-manual/extensible:using \\`\\`devtool\\`\\` in your sdk workflow>`\nas described in the Yocto Project Application Development and the\nExtensible Software Development Kit (eSDK) manual is a safer\ndevelopment flow than the flow that uses Quilt.\nFollow these general steps:\n#. *Find the Source Code:* Temporary source code used by the\nOpenEmbedded build system is kept in the :term:`Build Directory`. See the\n\":ref:`dev-manual/temporary-source-code:finding temporary source code`\" section to\nlearn how to locate the directory that has the temporary source code for a\nparticular package.\n#. *Change Your Working Directory:* You need to be in the directory that\nhas the temporary source code. That directory is defined by the\n:term:`S` variable.\n#. *Create a New Patch:* Before modifying source code, you need to\ncreate a new patch. To create a new patch file, use ``quilt new`` as\nbelow::\n$ quilt new my_changes.patch\n#. *Notify Quilt and Add Files:* After creating the patch, you need to\nnotify Quilt about the files you plan to edit. You notify Quilt by\nadding the files to the patch you just created::\n$ quilt add file1.c file2.c file3.c\n#. *Edit the Files:* Make your changes in the source code to the files\nyou added to the patch.\n#. *Test Your Changes:* Once you have modified the source code, the\neasiest way to test your changes is by calling the :ref:`ref-tasks-compile`\ntask as shown in the following example::\n$ bitbake -c compile -f package\nThe ``-f`` or ``--force`` option forces the specified task to\nexecute. If you find problems with your code, you can just keep\nediting and re-testing iteratively until things work as expected.\n.. note::\nAll the modifications you make to the temporary source code disappear\nonce you run the :ref:`ref-tasks-clean` or :ref:`ref-tasks-cleanall`\ntasks using BitBake (i.e. ``bitbake -c clean package`` and\n``bitbake -c cleanall package``). Modifications will also disappear if\nyou use the :ref:`ref-classes-rm-work` feature as described in\nthe \":ref:`dev-manual/disk-space:conserving disk space during builds`\"\nsection.\n#. *Generate the Patch:* Once your changes work as expected, you need to\nuse Quilt to generate the final patch that contains all your\nmodifications::\n$ quilt refresh\nAt this point, the\n``my_changes.patch`` file has all your edits made to the ``file1.c``,\n``file2.c``, and ``file3.c`` files.\nYou can find the resulting patch file in the ``patches/``\nsubdirectory of the source (:term:`S`) directory.\n#. *Copy the Patch File:* For simplicity, copy the patch file into a\ndirectory named ``files``, which you can create in the same directory\nthat holds the recipe (``.bb``) file or the append (``.bbappend``)\nfile. Placing the patch here guarantees that the OpenEmbedded build\nsystem will find the patch. Next, add the patch into the :term:`SRC_URI`\nof the recipe. Here is an example::\nSRC_URI += \"file://my_changes.patch\""}
{"text": "\nFlashing Images Using ``bmaptool``\n**********************************\nA fast and easy way to flash an image to a bootable device is to use\nBmaptool, which is integrated into the OpenEmbedded build system.\nBmaptool is a generic tool that creates a file's block map (bmap) and\nthen uses that map to copy the file. As compared to traditional tools\nsuch as dd or cp, Bmaptool can copy (or flash) large files like raw\nsystem image files much faster.\n.. note::\n-  If you are using Ubuntu or Debian distributions, you can install\nthe ``bmap-tools`` package using the following command and then\nuse the tool without specifying ``PATH`` even from the root\naccount::\n$ sudo apt install bmap-tools\n-  If you are unable to install the ``bmap-tools`` package, you will\nneed to build Bmaptool before using it. Use the following command::\n$ bitbake bmap-tools-native\nFollowing, is an example that shows how to flash a Wic image. Realize\nthat while this example uses a Wic image, you can use Bmaptool to flash\nany type of image. Use these steps to flash an image using Bmaptool:\n#. *Update your local.conf File:* You need to have the following set\nin your ``local.conf`` file before building your image::\nIMAGE_FSTYPES += \"wic wic.bmap\"\n#. *Get Your Image:* Either have your image ready (pre-built with the\n:term:`IMAGE_FSTYPES`\nsetting previously mentioned) or take the step to build the image::\n$ bitbake image\n#. *Flash the Device:* Flash the device with the image by using Bmaptool\ndepending on your particular setup. The following commands assume the\nimage resides in the :term:`Build Directory`'s ``deploy/images/`` area:\n-  If you have write access to the media, use this command form::\n$ oe-run-native bmap-tools-native bmaptool copy build-directory/tmp/deploy/images/machine/image.wic /dev/sdX\n-  If you do not have write access to the media, set your permissions\nfirst and then use the same command form::\n$ sudo chmod 666 /dev/sdX\n$ oe-run-native bmap-tools-native bmaptool copy build-directory/tmp/deploy/images/machine/image.wic /dev/sdX\nFor help on the ``bmaptool`` command, use the following command::\n$ bmaptool --help"}
{"text": "\nWorking with Packages\n*********************\nThis section describes a few tasks that involve packages:\n-  :ref:`dev-manual/packages:excluding packages from an image`\n-  :ref:`dev-manual/packages:incrementing a package version`\n-  :ref:`dev-manual/packages:handling optional module packaging`\n-  :ref:`dev-manual/packages:using runtime package management`\n-  :ref:`dev-manual/packages:generating and using signed packages`\n-  :ref:`Setting up and running package test\n(ptest) <dev-manual/packages:testing packages with ptest>`\n-  :ref:`dev-manual/packages:creating node package manager (npm) packages`\n-  :ref:`dev-manual/packages:adding custom metadata to packages`\nExcluding Packages from an Image\n================================\nYou might find it necessary to prevent specific packages from being\ninstalled into an image. If so, you can use several variables to direct\nthe build system to essentially ignore installing recommended packages\nor to not install a package at all.\nThe following list introduces variables you can use to prevent packages\nfrom being installed into your image. Each of these variables only works\nwith IPK and RPM package types, not for Debian packages.\nAlso, you can use these variables from your ``local.conf`` file\nor attach them to a specific image recipe by using a recipe name\noverride. For more detail on the variables, see the descriptions in the\nYocto Project Reference Manual's glossary chapter.\n-  :term:`BAD_RECOMMENDATIONS`:\nUse this variable to specify \"recommended-only\" packages that you do\nnot want installed.\n-  :term:`NO_RECOMMENDATIONS`:\nUse this variable to prevent all \"recommended-only\" packages from\nbeing installed.\n-  :term:`PACKAGE_EXCLUDE`:\nUse this variable to prevent specific packages from being installed\nregardless of whether they are \"recommended-only\" or not. You need to\nrealize that the build process could fail with an error when you\nprevent the installation of a package whose presence is required by\nan installed package.\nIncrementing a Package Version\n==============================\nThis section provides some background on how binary package versioning\nis accomplished and presents some of the services, variables, and\nterminology involved.\nIn order to understand binary package versioning, you need to consider\nthe following:\n-  Binary Package: The binary package that is eventually built and\ninstalled into an image.\n-  Binary Package Version: The binary package version is composed of two\ncomponents --- a version and a revision.\n.. note::\nTechnically, a third component, the \"epoch\" (i.e. :term:`PE`) is involved\nbut this discussion for the most part ignores :term:`PE`.\nThe version and revision are taken from the\n:term:`PV` and\n:term:`PR` variables, respectively.\n-  :term:`PV`: The recipe version. :term:`PV` represents the version of the\nsoftware being packaged. Do not confuse :term:`PV` with the binary\npackage version.\n-  :term:`PR`: The recipe revision.\n-  :term:`SRCPV`: The OpenEmbedded\nbuild system uses this string to help define the value of :term:`PV` when\nthe source code revision needs to be included in it.\n-  :yocto_wiki:`PR Service </PR_Service>`: A\nnetwork-based service that helps automate keeping package feeds\ncompatible with existing package manager applications such as RPM,\nAPT, and OPKG.\nWhenever the binary package content changes, the binary package version\nmust change. Changing the binary package version is accomplished by\nchanging or \"bumping\" the :term:`PR` and/or :term:`PV` values. Increasing these\nvalues occurs one of two ways:\n-  Automatically using a Package Revision Service (PR Service).\n-  Manually incrementing the :term:`PR` and/or :term:`PV` variables.\nGiven a primary challenge of any build system and its users is how to\nmaintain a package feed that is compatible with existing package manager\napplications such as RPM, APT, and OPKG, using an automated system is\nmuch preferred over a manual system. In either system, the main\nrequirement is that binary package version numbering increases in a\nlinear fashion and that there is a number of version components that\nsupport that linear progression. For information on how to ensure\npackage revisioning remains linear, see the\n\":ref:`dev-manual/packages:automatically incrementing a package version number`\"\nsection.\nThe following three sections provide related information on the PR\nService, the manual method for \"bumping\" :term:`PR` and/or :term:`PV`, and on\nhow to ensure binary package revisioning remains linear.\nWorking With a PR Service\n-------------------------\nAs mentioned, attempting to maintain revision numbers in the\n:term:`Metadata` is error prone, inaccurate,\nand causes problems for people submitting recipes. Conversely, the PR\nService automatically generates increasing numbers, particularly the\nrevision field, which removes the human element.\n.. note::\nFor additional information on using a PR Service, you can see the\n:yocto_wiki:`PR Service </PR_Service>` wiki page.\nThe Yocto Project uses variables in order of decreasing priority to\nfacilitate revision numbering (i.e.\n:term:`PE`,\n:term:`PV`, and\n:term:`PR` for epoch, version, and"}
{"text": "\nrevision, respectively). The values are highly dependent on the policies\nand procedures of a given distribution and package feed.\nBecause the OpenEmbedded build system uses\n\":ref:`signatures <overview-manual/concepts:checksums (signatures)>`\", which are\nunique to a given build, the build system knows when to rebuild\npackages. All the inputs into a given task are represented by a\nsignature, which can trigger a rebuild when different. Thus, the build\nsystem itself does not rely on the :term:`PR`, :term:`PV`, and :term:`PE` numbers to\ntrigger a rebuild. The signatures, however, can be used to generate\nthese values.\nThe PR Service works with both ``OEBasic`` and ``OEBasicHash``\ngenerators. The value of :term:`PR` bumps when the checksum changes and the\ndifferent generator mechanisms change signatures under different\ncircumstances.\nAs implemented, the build system includes values from the PR Service\ninto the :term:`PR` field as an addition using the form \"``.x``\" so ``r0``\nbecomes ``r0.1``, ``r0.2`` and so forth. This scheme allows existing\n:term:`PR` values to be used for whatever reasons, which include manual\n:term:`PR` bumps, should it be necessary.\nBy default, the PR Service is not enabled or running. Thus, the packages\ngenerated are just \"self consistent\". The build system adds and removes\npackages and there are no guarantees about upgrade paths but images will\nbe consistent and correct with the latest changes.\nThe simplest form for a PR Service is for a single host development system\nthat builds the package feed (building system). For this scenario, you can\nenable a local PR Service by setting :term:`PRSERV_HOST` in your\n``local.conf`` file in the :term:`Build Directory`::\nPRSERV_HOST = \"localhost:0\"\nOnce the service is started, packages will automatically\nget increasing :term:`PR` values and BitBake takes care of starting and\nstopping the server.\nIf you have a more complex setup where multiple host development systems\nwork against a common, shared package feed, you have a single PR Service\nrunning and it is connected to each building system. For this scenario,\nyou need to start the PR Service using the ``bitbake-prserv`` command::\nbitbake-prserv --host ip --port port --start\nIn addition to\nhand-starting the service, you need to update the ``local.conf`` file of\neach building system as described earlier so each system points to the\nserver and port.\nIt is also recommended you use build history, which adds some sanity\nchecks to binary package versions, in conjunction with the server that\nis running the PR Service. To enable build history, add the following to\neach building system's ``local.conf`` file::\n# It is recommended to activate \"buildhistory\" for testing the PR service\nINHERIT += \"buildhistory\"\nBUILDHISTORY_COMMIT = \"1\"\nFor information on build\nhistory, see the\n\":ref:`dev-manual/build-quality:maintaining build output quality`\" section.\n.. note::\nThe OpenEmbedded build system does not maintain :term:`PR` information as\npart of the shared state (sstate) packages. If you maintain an sstate\nfeed, it's expected that either all your building systems that\ncontribute to the sstate feed use a shared PR Service, or you do not\nrun a PR Service on any of your building systems. Having some systems\nuse a PR Service while others do not leads to obvious problems.\nFor more information on shared state, see the\n\":ref:`overview-manual/concepts:shared state cache`\"\nsection in the Yocto Project Overview and Concepts Manual.\nManually Bumping PR\n-------------------\nThe alternative to setting up a PR Service is to manually \"bump\" the\n:term:`PR` variable.\nIf a committed change results in changing the package output, then the\nvalue of the :term:`PR` variable needs to be increased (or \"bumped\") as part of\nthat commit. For new recipes you should add the :term:`PR` variable and set\nits initial value equal to \"r0\", which is the default. Even though the\ndefault value is \"r0\", the practice of adding it to a new recipe makes\nit harder to forget to bump the variable when you make changes to the\nrecipe in future.\nUsually, version increases occur only to binary packages. However, if\nfor some reason :term:`PV` changes but does not increase, you can increase\nthe :term:`PE` variable (Package Epoch). The :term:`PE` variable defaults to\n\"0\".\nBinary package version numbering strives to follow the `Debian Version\nField Policy\nGuidelines <https://www.debian.org/doc/debian-policy/ch-controlfields.html>`__.\nThese guidelines define how versions are compared and what \"increasing\"\na version means.\nAutomatically Incrementing a Package Version Number\n---------------------------------------------------\nWhen fetching a repository, BitBake uses the\n:term:`SRCREV` variable to determine\nthe specific source code revision from which to build. You set the\n:term:`SRCREV` variable to\n:term:`AUTOREV` to cause the\nOpenEmbedded build system to automatically use the latest revision of\nthe software::\nSRCREV = \"${AUTOREV}\"\nFurthermore, you need to reference :term:`SRCPV` in :term:`PV` in order to\nautomatically update the version whenever the revision of the source\ncode changes. Here is an example::\nPV = \"1.0+git${SRCPV}\"\nThe OpenEmbedded build system substitutes :term:`SRCPV` with the following:\n.. code-block:: none\nAUTOINC+source_code_revision\nThe build system replaces the ``AUTOINC``\nwith a number. The number used depends on the state of the PR Service:\n-  If PR Service is enabled, the build system increments the number,"}
{"text": "\nwhich is similar to the behavior of\n:term:`PR`. This behavior results in\nlinearly increasing package versions, which is desirable. Here is an\nexample:\n.. code-block:: none\nhello-world-git_0.0+git0+b6558dd387-r0.0_armv7a-neon.ipk\nhello-world-git_0.0+git1+dd2f5c3565-r0.0_armv7a-neon.ipk\n-  If PR Service is not enabled, the build system replaces the\n``AUTOINC`` placeholder with zero (i.e. \"0\"). This results in\nchanging the package version since the source revision is included.\nHowever, package versions are not increased linearly. Here is an\nexample:\n.. code-block:: none\nhello-world-git_0.0+git0+b6558dd387-r0.0_armv7a-neon.ipk\nhello-world-git_0.0+git0+dd2f5c3565-r0.0_armv7a-neon.ipk\nIn summary, the OpenEmbedded build system does not track the history of\nbinary package versions for this purpose. ``AUTOINC``, in this case, is\ncomparable to :term:`PR`. If PR server is not enabled, ``AUTOINC`` in the\npackage version is simply replaced by \"0\". If PR server is enabled, the\nbuild system keeps track of the package versions and bumps the number\nwhen the package revision changes.\nHandling Optional Module Packaging\n==================================\nMany pieces of software split functionality into optional modules (or\nplugins) and the plugins that are built might depend on configuration\noptions. To avoid having to duplicate the logic that determines what\nmodules are available in your recipe or to avoid having to package each\nmodule by hand, the OpenEmbedded build system provides functionality to\nhandle module packaging dynamically.\nTo handle optional module packaging, you need to do two things:\n-  Ensure the module packaging is actually done.\n-  Ensure that any dependencies on optional modules from other recipes\nare satisfied by your recipe.\nMaking Sure the Packaging is Done\n---------------------------------\nTo ensure the module packaging actually gets done, you use the\n``do_split_packages`` function within the ``populate_packages`` Python\nfunction in your recipe. The ``do_split_packages`` function searches for\na pattern of files or directories under a specified path and creates a\npackage for each one it finds by appending to the\n:term:`PACKAGES` variable and\nsetting the appropriate values for ``FILES:packagename``,\n``RDEPENDS:packagename``, ``DESCRIPTION:packagename``, and so forth.\nHere is an example from the ``lighttpd`` recipe::\npython populate_packages:prepend () {\nlighttpd_libdir = d.expand('${libdir}')\ndo_split_packages(d, lighttpd_libdir, '^mod_(.*).so$',\n'lighttpd-module-%s', 'Lighttpd module for %s',\nextra_depends='')\n}\nThe previous example specifies a number of things in the call to\n``do_split_packages``.\n-  A directory within the files installed by your recipe through\n:ref:`ref-tasks-install` in which to search.\n-  A regular expression used to match module files in that directory. In\nthe example, note the parentheses () that mark the part of the\nexpression from which the module name should be derived.\n-  A pattern to use for the package names.\n-  A description for each package.\n-  An empty string for ``extra_depends``, which disables the default\ndependency on the main ``lighttpd`` package. Thus, if a file in\n``${libdir}`` called ``mod_alias.so`` is found, a package called\n``lighttpd-module-alias`` is created for it and the\n:term:`DESCRIPTION` is set to\n\"Lighttpd module for alias\".\nOften, packaging modules is as simple as the previous example. However,\nthere are more advanced options that you can use within\n``do_split_packages`` to modify its behavior. And, if you need to, you\ncan add more logic by specifying a hook function that is called for each\npackage. It is also perfectly acceptable to call ``do_split_packages``\nmultiple times if you have more than one set of modules to package.\nFor more examples that show how to use ``do_split_packages``, see the\n``connman.inc`` file in the ``meta/recipes-connectivity/connman/``\ndirectory of the ``poky`` :ref:`source repository <overview-manual/development-environment:yocto project source repositories>`. You can\nalso find examples in ``meta/classes-recipe/kernel.bbclass``.\nFollowing is a reference that shows ``do_split_packages`` mandatory and\noptional arguments::\nMandatory arguments\nroot\nThe path in which to search\nfile_regex\nRegular expression to match searched files.\nUse parentheses () to mark the part of this\nexpression that should be used to derive the\nmodule name (to be substituted where %s is\nused in other function arguments as noted below)\noutput_pattern\nPattern to use for the package names. Must\ninclude %s.\ndescription\nDescription to set for each package. Must\ninclude %s.\nOptional arguments\npostinst\nPostinstall script to use for all packages\n(as a string)\nrecursive\nTrue to perform a recursive search --- default\nFalse\nhook"}
{"text": "\nA hook function to be called for every match.\nThe function will be called with the following\narguments (in the order listed):\nf\nFull path to the file/directory match\npkg\nThe package name\nfile_regex\nAs above\noutput_pattern\nAs above\nmodulename\nThe module name derived using file_regex\nextra_depends\nExtra runtime dependencies (RDEPENDS) to be\nset for all packages. The default value of None\ncauses a dependency on the main package\n(${PN}) --- if you do not want this, pass empty\nstring '' for this parameter.\naux_files_pattern\nExtra item(s) to be added to FILES for each\npackage. Can be a single string item or a list\nof strings for multiple items. Must include %s.\npostrm\npostrm script to use for all packages (as a\nstring)\nallow_dirs\nTrue to allow directories to be matched -\ndefault False\nprepend\nIf True, prepend created packages to PACKAGES\ninstead of the default False which appends them\nmatch_path\nmatch file_regex on the whole relative path to\nthe root rather than just the filename\naux_files_pattern_verbatim\nExtra item(s) to be added to FILES for each\npackage, using the actual derived module name\nrather than converting it to something legal\nfor a package name. Can be a single string item\nor a list of strings for multiple items. Must\ninclude %s.\nallow_links\nTrue to allow symlinks to be matched --- default\nFalse\nsummary\nSummary to set for each package. Must include %s;\ndefaults to description if not set.\nSatisfying Dependencies\n-----------------------\nThe second part for handling optional module packaging is to ensure that\nany dependencies on optional modules from other recipes are satisfied by\nyour recipe. You can be sure these dependencies are satisfied by using\nthe :term:`PACKAGES_DYNAMIC`\nvariable. Here is an example that continues with the ``lighttpd`` recipe\nshown earlier::\nPACKAGES_DYNAMIC = \"lighttpd-module-.*\"\nThe name\nspecified in the regular expression can of course be anything. In this\nexample, it is ``lighttpd-module-`` and is specified as the prefix to\nensure that any :term:`RDEPENDS` and\n:term:`RRECOMMENDS` on a package\nname starting with the prefix are satisfied during build time. If you\nare using ``do_split_packages`` as described in the previous section,\nthe value you put in :term:`PACKAGES_DYNAMIC` should correspond to the name\npattern specified in the call to ``do_split_packages``.\nUsing Runtime Package Management\n================================\nDuring a build, BitBake always transforms a recipe into one or more\npackages. For example, BitBake takes the ``bash`` recipe and produces a\nnumber of packages (e.g. ``bash``, ``bash-bashbug``,\n``bash-completion``, ``bash-completion-dbg``, ``bash-completion-dev``,\n``bash-completion-extra``, ``bash-dbg``, and so forth). Not all\ngenerated packages are included in an image.\nIn several situations, you might need to update, add, remove, or query\nthe packages on a target device at runtime (i.e. without having to\ngenerate a new image). Examples of such situations include:\n-  You want to provide in-the-field updates to deployed devices (e.g.\nsecurity updates).\n-  You want to have a fast turn-around development cycle for one or more\napplications that run on your device.\n-  You want to temporarily install the \"debug\" packages of various\napplications on your device so that debugging can be greatly improved\nby allowing access to symbols and source debugging.\n-  You want to deploy a more minimal package selection of your device\nbut allow in-the-field updates to add a larger selection for\ncustomization.\nIn all these situations, you have something similar to a more\ntraditional Linux distribution in that in-field devices are able to\nreceive pre-compiled packages from a server for installation or update.\nBeing able to install these packages on a running, in-field device is\nwhat is termed \"runtime package management\".\nIn order to use runtime package management, you need a host or server\nmachine that serves up the pre-compiled packages plus the required\nmetadata. You also need package manipulation tools on the target. The\nbuild machine is a likely candidate to act as the server. However, that\nmachine does not necessarily have to be the package server. The build\nmachine could push its artifacts to another machine that acts as the\nserver (e.g. Internet-facing). In fact, doing so is advantageous for a\nproduction environment as getting the packages away from the development"}
{"text": "\nsystem's :term:`Build Directory` prevents accidental overwrites.\nA simple build that targets just one device produces more than one\npackage database. In other words, the packages produced by a build are\nseparated out into a couple of different package groupings based on\ncriteria such as the target's CPU architecture, the target board, or the\nC library used on the target. For example, a build targeting the\n``qemux86`` device produces the following three package databases:\n``noarch``, ``i586``, and ``qemux86``. If you wanted your ``qemux86``\ndevice to be aware of all the packages that were available to it, you\nwould need to point it to each of these databases individually. In a\nsimilar way, a traditional Linux distribution usually is configured to\nbe aware of a number of software repositories from which it retrieves\npackages.\nUsing runtime package management is completely optional and not required\nfor a successful build or deployment in any way. But if you want to make\nuse of runtime package management, you need to do a couple things above\nand beyond the basics. The remainder of this section describes what you\nneed to do.\nBuild Considerations\n--------------------\nThis section describes build considerations of which you need to be\naware in order to provide support for runtime package management.\nWhen BitBake generates packages, it needs to know what format or formats\nto use. In your configuration, you use the\n:term:`PACKAGE_CLASSES`\nvariable to specify the format:\n#. Open the ``local.conf`` file inside your :term:`Build Directory` (e.g.\n``poky/build/conf/local.conf``).\n#. Select the desired package format as follows::\nPACKAGE_CLASSES ?= \"package_packageformat\"\nwhere packageformat can be \"ipk\", \"rpm\",\n\"deb\", or \"tar\" which are the supported package formats.\n.. note::\nBecause the Yocto Project supports four different package formats,\nyou can set the variable with more than one argument. However, the\nOpenEmbedded build system only uses the first argument when\ncreating an image or Software Development Kit (SDK).\nIf you would like your image to start off with a basic package database\ncontaining the packages in your current build as well as to have the\nrelevant tools available on the target for runtime package management,\nyou can include \"package-management\" in the\n:term:`IMAGE_FEATURES`\nvariable. Including \"package-management\" in this configuration variable\nensures that when the image is assembled for your target, the image\nincludes the currently-known package databases as well as the\ntarget-specific tools required for runtime package management to be\nperformed on the target. However, this is not strictly necessary. You\ncould start your image off without any databases but only include the\nrequired on-target package tool(s). As an example, you could include\n\"opkg\" in your\n:term:`IMAGE_INSTALL` variable\nif you are using the IPK package format. You can then initialize your\ntarget's package database(s) later once your image is up and running.\nWhenever you perform any sort of build step that can potentially\ngenerate a package or modify existing package, it is always a good idea\nto re-generate the package index after the build by using the following\ncommand::\n$ bitbake package-index\nIt might be tempting to build the\npackage and the package index at the same time with a command such as\nthe following::\n$ bitbake some-package package-index\nDo not do this as\nBitBake does not schedule the package index for after the completion of\nthe package you are building. Consequently, you cannot be sure of the\npackage index including information for the package you just built.\nThus, be sure to run the package update step separately after building\nany packages.\nYou can use the\n:term:`PACKAGE_FEED_ARCHS`,\n:term:`PACKAGE_FEED_BASE_PATHS`,\nand\n:term:`PACKAGE_FEED_URIS`\nvariables to pre-configure target images to use a package feed. If you\ndo not define these variables, then manual steps as described in the\nsubsequent sections are necessary to configure the target. You should\nset these variables before building the image in order to produce a\ncorrectly configured image.\nWhen your build is complete, your packages reside in the\n``${TMPDIR}/deploy/packageformat`` directory. For example, if\n``${``\\ :term:`TMPDIR`\\ ``}`` is\n``tmp`` and your selected package type is RPM, then your RPM packages\nare available in ``tmp/deploy/rpm``.\nHost or Server Machine Setup\n----------------------------\nAlthough other protocols are possible, a server using HTTP typically\nserves packages. If you want to use HTTP, then set up and configure a\nweb server such as Apache 2, lighttpd, or Python web server on the\nmachine serving the packages.\nTo keep things simple, this section describes how to set up a\nPython web server to share package feeds from the developer's\nmachine. Although this server might not be the best for a production\nenvironment, the setup is simple and straight forward. Should you want\nto use a different server more suited for production (e.g. Apache 2,\nLighttpd, or Nginx), take the appropriate steps to do so.\nFrom within the :term:`Build Directory` where you have built an image based on\nyour packaging choice (i.e. the :term:`PACKAGE_CLASSES` setting), simply start\nthe server. The following example assumes a :term:`Build Directory` of ``poky/build``\nand a :term:`PACKAGE_CLASSES` setting of \":ref:`ref-classes-package_rpm`\"::\n$ cd poky/build/tmp/deploy/rpm"}
{"text": "\n$ python3 -m http.server\nTarget Setup\n------------\nSetting up the target differs depending on the package management\nsystem. This section provides information for RPM, IPK, and DEB.\nUsing RPM\n~~~~~~~~~\nThe :wikipedia:`Dandified Packaging <DNF_(software)>` (DNF) performs\nruntime package management of RPM packages. In order to use DNF for\nruntime package management, you must perform an initial setup on the\ntarget machine for cases where the ``PACKAGE_FEED_*`` variables were not\nset as part of the image that is running on the target. This means if\nyou built your image and did not use these variables as part of the\nbuild and your image is now running on the target, you need to perform\nthe steps in this section if you want to use runtime package management.\n.. note::\nFor information on the ``PACKAGE_FEED_*`` variables, see\n:term:`PACKAGE_FEED_ARCHS`, :term:`PACKAGE_FEED_BASE_PATHS`, and\n:term:`PACKAGE_FEED_URIS` in the Yocto Project Reference Manual variables\nglossary.\nOn the target, you must inform DNF that package databases are available.\nYou do this by creating a file named\n``/etc/yum.repos.d/oe-packages.repo`` and defining the ``oe-packages``.\nAs an example, assume the target is able to use the following package\ndatabases: ``all``, ``i586``, and ``qemux86`` from a server named\n``my.server``. The specifics for setting up the web server are up to\nyou. The critical requirement is that the URIs in the target repository\nconfiguration point to the correct remote location for the feeds.\n.. note::\nFor development purposes, you can point the web server to the build\nsystem's ``deploy`` directory. However, for production use, it is better to\ncopy the package directories to a location outside of the build area and use\nthat location. Doing so avoids situations where the build system\noverwrites or changes the ``deploy`` directory.\nWhen telling DNF where to look for the package databases, you must\ndeclare individual locations per architecture or a single location used\nfor all architectures. You cannot do both:\n-  *Create an Explicit List of Architectures:* Define individual base\nURLs to identify where each package database is located:\n.. code-block:: none\n[oe-packages]\nbaseurl=http://my.server/rpm/i586  http://my.server/rpm/qemux86 http://my.server/rpm/all\nThis example\ninforms DNF about individual package databases for all three\narchitectures.\n-  *Create a Single (Full) Package Index:* Define a single base URL that\nidentifies where a full package database is located::\n[oe-packages]\nbaseurl=http://my.server/rpm\nThis example informs DNF about a single\npackage database that contains all the package index information for\nall supported architectures.\nOnce you have informed DNF where to find the package databases, you need\nto fetch them:\n.. code-block:: none\n# dnf makecache\nDNF is now able to find, install, and\nupgrade packages from the specified repository or repositories.\n.. note::\nSee the `DNF documentation <https://dnf.readthedocs.io/en/latest/>`__ for\nadditional information.\nUsing IPK\n~~~~~~~~~\nThe ``opkg`` application performs runtime package management of IPK\npackages. You must perform an initial setup for ``opkg`` on the target\nmachine if the\n:term:`PACKAGE_FEED_ARCHS`,\n:term:`PACKAGE_FEED_BASE_PATHS`,\nand\n:term:`PACKAGE_FEED_URIS`\nvariables have not been set or the target image was built before the\nvariables were set.\nThe ``opkg`` application uses configuration files to find available\npackage databases. Thus, you need to create a configuration file inside\nthe ``/etc/opkg/`` directory, which informs ``opkg`` of any repository\nyou want to use.\nAs an example, suppose you are serving packages from a ``ipk/``\ndirectory containing the ``i586``, ``all``, and ``qemux86`` databases\nthrough an HTTP server named ``my.server``. On the target, create a\nconfiguration file (e.g. ``my_repo.conf``) inside the ``/etc/opkg/``\ndirectory containing the following:\n.. code-block:: none\nsrc/gz all http://my.server/ipk/all\nsrc/gz i586 http://my.server/ipk/i586\nsrc/gz qemux86 http://my.server/ipk/qemux86\nNext, instruct ``opkg`` to fetch the\nrepository information:\n.. code-block:: none\n# opkg update\nThe ``opkg`` application is now able to find, install, and upgrade packages\nfrom the specified repository.\nUsing DEB\n~~~~~~~~~\nThe ``apt`` application performs runtime package management of DEB\npackages. This application uses a source list file to find available\npackage databases. You must perform an initial setup for ``apt`` on the\ntarget machine if the\n:term:`PACKAGE_FEED_ARCHS`,\n:term:`PACKAGE_FEED_BASE_PATHS`,\nand"}
{"text": "\n:term:`PACKAGE_FEED_URIS`\nvariables have not been set or the target image was built before the\nvariables were set.\nTo inform ``apt`` of the repository you want to use, you might create a\nlist file (e.g. ``my_repo.list``) inside the\n``/etc/apt/sources.list.d/`` directory. As an example, suppose you are\nserving packages from a ``deb/`` directory containing the ``i586``,\n``all``, and ``qemux86`` databases through an HTTP server named\n``my.server``. The list file should contain:\n.. code-block:: none\ndeb http://my.server/deb/all ./\ndeb http://my.server/deb/i586 ./\ndeb http://my.server/deb/qemux86 ./\nNext, instruct the ``apt`` application\nto fetch the repository information:\n.. code-block:: none\n$ sudo apt update\nAfter this step,\n``apt`` is able to find, install, and upgrade packages from the\nspecified repository.\nGenerating and Using Signed Packages\n====================================\nIn order to add security to RPM packages used during a build, you can\ntake steps to securely sign them. Once a signature is verified, the\nOpenEmbedded build system can use the package in the build. If security\nfails for a signed package, the build system stops the build.\nThis section describes how to sign RPM packages during a build and how\nto use signed package feeds (repositories) when doing a build.\nSigning RPM Packages\n--------------------\nTo enable signing RPM packages, you must set up the following\nconfigurations in either your ``local.config`` or ``distro.config``\nfile::\n# Inherit sign_rpm.bbclass to enable signing functionality\nINHERIT += \" sign_rpm\"\n# Define the GPG key that will be used for signing.\nRPM_GPG_NAME = \"key_name\"\n# Provide passphrase for the key\nRPM_GPG_PASSPHRASE = \"passphrase\"\n.. note::\nBe sure to supply appropriate values for both `key_name` and\n`passphrase`.\nAside from the ``RPM_GPG_NAME`` and ``RPM_GPG_PASSPHRASE`` variables in\nthe previous example, two optional variables related to signing are available:\n-  *GPG_BIN:* Specifies a ``gpg`` binary/wrapper that is executed\nwhen the package is signed.\n-  *GPG_PATH:* Specifies the ``gpg`` home directory used when the\npackage is signed.\nProcessing Package Feeds\n------------------------\nIn addition to being able to sign RPM packages, you can also enable\nsigned package feeds for IPK and RPM packages.\nThe steps you need to take to enable signed package feed use are similar\nto the steps used to sign RPM packages. You must define the following in\nyour ``local.config`` or ``distro.config`` file::\nINHERIT += \"sign_package_feed\"\nPACKAGE_FEED_GPG_NAME = \"key_name\"\nPACKAGE_FEED_GPG_PASSPHRASE_FILE = \"path_to_file_containing_passphrase\"\nFor signed package feeds, the passphrase must be specified in a separate file,\nwhich is pointed to by the ``PACKAGE_FEED_GPG_PASSPHRASE_FILE``\nvariable. Regarding security, keeping a plain text passphrase out of the\nconfiguration is more secure.\nAside from the ``PACKAGE_FEED_GPG_NAME`` and\n``PACKAGE_FEED_GPG_PASSPHRASE_FILE`` variables, three optional variables\nrelated to signed package feeds are available:\n-  *GPG_BIN* Specifies a ``gpg`` binary/wrapper that is executed\nwhen the package is signed.\n-  *GPG_PATH:* Specifies the ``gpg`` home directory used when the\npackage is signed.\n-  *PACKAGE_FEED_GPG_SIGNATURE_TYPE:* Specifies the type of ``gpg``\nsignature. This variable applies only to RPM and IPK package feeds.\nAllowable values for the ``PACKAGE_FEED_GPG_SIGNATURE_TYPE`` are\n\"ASC\", which is the default and specifies ascii armored, and \"BIN\",\nwhich specifies binary.\nTesting Packages With ptest\n===========================\nA Package Test (ptest) runs tests against packages built by the\nOpenEmbedded build system on the target machine. A ptest contains at\nleast two items: the actual test, and a shell script (``run-ptest``)\nthat starts the test. The shell script that starts the test must not\ncontain the actual test --- the script only starts the test. On the other\nhand, the test can be anything from a simple shell script that runs a\nbinary and checks the output to an elaborate system of test binaries and\ndata files.\nThe test generates output in the format used by Automake::\nresult: testname\nwhere the result can be ``PASS``, ``FAIL``, or ``SKIP``, and\nthe testname can be any identifying string.\nFor a list of Yocto Project recipes that are already enabled with ptest,\nsee the :yocto_wiki:`Ptest </Ptest>` wiki page.\n.. note::\nA recipe is \"ptest-enabled\" if it inherits the :ref:`ref-classes-ptest`\nclass.\nAdding ptest to Your Build\n--------------------------\nTo add package testing to your build, add the :term:`DISTRO_FEATURES` and\n:term:`EXTRA_IMAGE_FEATURES` variables to your ``local.conf`` file, which\nis found in the :term:`Build Directory`::\nDISTRO_FEATURES:append = \" ptest\"\nEXTRA_IMAGE_FEATURES += \"ptest-pkgs\""}
{"text": "\nOnce your build is complete, the ptest files are installed into the\n``/usr/lib/package/ptest`` directory within the image, where ``package``\nis the name of the package.\nRunning ptest\n-------------\nThe ``ptest-runner`` package installs a shell script that loops through\nall installed ptest test suites and runs them in sequence. Consequently,\nyou might want to add this package to your image.\nGetting Your Package Ready\n--------------------------\nIn order to enable a recipe to run installed ptests on target hardware,\nyou need to prepare the recipes that build the packages you want to\ntest. Here is what you have to do for each recipe:\n-  *Be sure the recipe inherits the* :ref:`ref-classes-ptest` *class:*\nInclude the following line in each recipe::\ninherit ptest\n-  *Create run-ptest:* This script starts your test. Locate the\nscript where you will refer to it using\n:term:`SRC_URI`. Here is an\nexample that starts a test for ``dbus``::\n#!/bin/sh\ncd test\nmake -k runtest-TESTS\n-  *Ensure dependencies are met:* If the test adds build or runtime\ndependencies that normally do not exist for the package (such as\nrequiring \"make\" to run the test suite), use the\n:term:`DEPENDS` and\n:term:`RDEPENDS` variables in\nyour recipe in order for the package to meet the dependencies. Here\nis an example where the package has a runtime dependency on \"make\"::\nRDEPENDS:${PN}-ptest += \"make\"\n-  *Add a function to build the test suite:* Not many packages support\ncross-compilation of their test suites. Consequently, you usually\nneed to add a cross-compilation function to the package.\nMany packages based on Automake compile and run the test suite by\nusing a single command such as ``make check``. However, the host\n``make check`` builds and runs on the same computer, while\ncross-compiling requires that the package is built on the host but\nexecuted for the target architecture (though often, as in the case\nfor ptest, the execution occurs on the host). The built version of\nAutomake that ships with the Yocto Project includes a patch that\nseparates building and execution. Consequently, packages that use the\nunaltered, patched version of ``make check`` automatically\ncross-compiles.\nRegardless, you still must add a ``do_compile_ptest`` function to\nbuild the test suite. Add a function similar to the following to your\nrecipe::\ndo_compile_ptest() {\noe_runmake buildtest-TESTS\n}\n-  *Ensure special configurations are set:* If the package requires\nspecial configurations prior to compiling the test code, you must\ninsert a ``do_configure_ptest`` function into the recipe.\n-  *Install the test suite:* The :ref:`ref-classes-ptest` class\nautomatically copies the file ``run-ptest`` to the target and then runs make\n``install-ptest`` to run the tests. If this is not enough, you need\nto create a ``do_install_ptest`` function and make sure it gets\ncalled after the \"make install-ptest\" completes.\nCreating Node Package Manager (NPM) Packages\n============================================\n:wikipedia:`NPM <Npm_(software)>` is a package manager for the JavaScript\nprogramming language. The Yocto Project supports the NPM\n:ref:`fetcher <bitbake-user-manual/bitbake-user-manual-fetching:fetchers>`.\nYou can use this fetcher in combination with\n:doc:`devtool </ref-manual/devtool-reference>` to create recipes that produce\nNPM packages.\nThere are two workflows that allow you to create NPM packages using\n``devtool``: the NPM registry modules method and the NPM project code\nmethod.\n.. note::\nWhile it is possible to create NPM recipes manually, using\n``devtool`` is far simpler.\nAdditionally, some requirements and caveats exist.\nRequirements and Caveats\n------------------------\nYou need to be aware of the following before using ``devtool`` to create\nNPM packages:\n-  Of the two methods that you can use ``devtool`` to create NPM\npackages, the registry approach is slightly simpler. However, you\nmight consider the project approach because you do not have to\npublish your module in the `NPM registry <https://docs.npmjs.com/misc/registry>`__,\nwhich is NPM's public registry.\n-  Be familiar with\n:doc:`devtool </ref-manual/devtool-reference>`.\n-  The NPM host tools need the native ``nodejs-npm`` package, which is\npart of the OpenEmbedded environment. You need to get the package by\ncloning the :oe_git:`meta-openembedded </meta-openembedded>`\nrepository. Be sure to add the path to your local copy\nto your ``bblayers.conf`` file.\n-  ``devtool`` cannot detect native libraries in module dependencies.\nConsequently, you must manually add packages to your recipe.\n-  While deploying NPM packages, ``devtool`` cannot determine which\ndependent packages are missing on the target (e.g. the node runtime\n``nodejs``). Consequently, you need to find out what files are\nmissing and be sure they are on the target.\n-  Although you might not need NPM to run your node package, it is\nuseful to have NPM on your target. The NPM package name is\n``nodejs-npm``.\nUsing the Registry Modules Method\n---------------------------------"}
{"text": "\nThis section presents an example that uses the ``cute-files`` module,\nwhich is a file browser web application.\n.. note::\nYou must know the ``cute-files`` module version.\nThe first thing you need to do is use ``devtool`` and the NPM fetcher to\ncreate the recipe::\n$ devtool add \"npm://registry.npmjs.org;package=cute-files;version=1.0.2\"\nThe\n``devtool add`` command runs ``recipetool create`` and uses the same\nfetch URI to download each dependency and capture license details where\npossible. The result is a generated recipe.\nAfter running for quite a long time, in particular building the\n``nodejs-native`` package, the command should end as follows::\nINFO: Recipe /home/.../build/workspace/recipes/cute-files/cute-files_1.0.2.bb has been automatically created; further editing may be required to make it fully functional\nThe recipe file is fairly simple and contains every license that\n``recipetool`` finds and includes the licenses in the recipe's\n:term:`LIC_FILES_CHKSUM`\nvariables. You need to examine the variables and look for those with\n\"unknown\" in the :term:`LICENSE`\nfield. You need to track down the license information for \"unknown\"\nmodules and manually add the information to the recipe.\n``recipetool`` creates a \"shrinkwrap\" file for your recipe. Shrinkwrap\nfiles capture the version of all dependent modules. Many packages do not\nprovide shrinkwrap files but ``recipetool`` will create a shrinkwrap file as it\nruns.\n.. note::\nA package is created for each sub-module. This policy is the only\npractical way to have the licenses for all of the dependencies\nrepresented in the license manifest of the image.\nThe ``devtool edit-recipe`` command lets you take a look at the recipe::\n$ devtool edit-recipe cute-files\n# Recipe created by recipetool\n# This is the basis of a recipe and may need further editing in order to be fully functional.\n# (Feel free to remove these comments when editing.)\nSUMMARY = \"Turn any folder on your computer into a cute file browser, available on the local network.\"\n# WARNING: the following LICENSE and LIC_FILES_CHKSUM values are best guesses - it is\n# your responsibility to verify that the values are complete and correct.\n#\n# NOTE: multiple licenses have been detected; they have been separated with &\n# in the LICENSE value for now since it is a reasonable assumption that all\n# of the licenses apply. If instead there is a choice between the multiple\n# licenses then you should change the value to separate the licenses with |\n# instead of &. If there is any doubt, check the accompanying documentation\n# to determine which situation is applicable.\nSUMMARY = \"Turn any folder on your computer into a cute file browser, available on the local network.\"\nLICENSE = \"BSD-3-Clause & ISC & MIT\"\nLIC_FILES_CHKSUM = \"file://LICENSE;md5=71d98c0a1db42956787b1909c74a86ca \\\nfile://node_modules/accepts/LICENSE;md5=bf1f9ad1e2e1d507aef4883fff7103de \\\nfile://node_modules/array-flatten/LICENSE;md5=44088ba57cb871a58add36ce51b8de08 \\\n...\nfile://node_modules/cookie-signature/Readme.md;md5=57ae8b42de3dd0c1f22d5f4cf191e15a\"\nSRC_URI = \" \\\nnpm://registry.npmjs.org/;package=cute-files;version=${PV} \\\nnpmsw://${THISDIR}/${BPN}/npm-shrinkwrap.json \\\n\"\nS = \"${WORKDIR}/npm\"\ninherit npm\nLICENSE:${PN} = \"MIT\"\nLICENSE:${PN}-accepts = \"MIT\"\nLICENSE:${PN}-array-flatten = \"MIT\"\n...\nLICENSE:${PN}-vary = \"MIT\"\nHere are three key points in the previous example:\n-  :term:`SRC_URI` uses the NPM\nscheme so that the NPM fetcher is used.\n-  ``recipetool`` collects all the license information. If a\nsub-module's license is unavailable, the sub-module's name appears in\nthe comments.\n-  The ``inherit npm`` statement causes the :ref:`ref-classes-npm` class to\npackage up all the modules.\nYou can run the following command to build the ``cute-files`` package::\n$ devtool build cute-files\nRemember that ``nodejs`` must be installed on\nthe target before your package.\nAssuming 192.168.7.2 for the target's IP address, use the following\ncommand to deploy your package::\n$ devtool deploy-target -s cute-files root@192.168.7.2\nOnce the package is installed on the target, you can\ntest the application to show the contents of any directory::\n$ cd /usr/lib/node_modules/cute-files\n$ cute-files\nOn a browser,\ngo to ``http://192.168.7.2:3000`` and you see the following:\n.. image:: figures/cute-files-npm-example.png\n:width: 100%\nYou can find the recipe in ``workspace/recipes/cute-files``. You can use\nthe recipe in any layer you choose.\nUsing the NPM Projects Code Method\n----------------------------------\nAlthough it is useful to package modules already in the NPM registry,\nadding ``node.js`` projects under development is a more common developer\nuse case.\nThis section covers the NPM projects code method, which is very similar\nto the \"registry\" approach described in the previous section. In the NPM\nprojects method, you provide ``devtool`` with an URL that points to the\nsource files.\nReplicating the same example, (i.e. ``cute-files``) use the following\ncommand::\n$ devtool add https://github.com/martinaglv/cute-files.git\nThe recipe this command generates is very similar to the recipe created in"}
{"text": "\nthe previous section. However, the :term:`SRC_URI` looks like the following::\nSRC_URI = \" \\\ngit://github.com/martinaglv/cute-files.git;protocol=https;branch=master \\\nnpmsw://${THISDIR}/${BPN}/npm-shrinkwrap.json \\\n\"\nIn this example,\nthe main module is taken from the Git repository and dependencies are\ntaken from the NPM registry. Other than those differences, the recipe is\nbasically the same between the two methods. You can build and deploy the\npackage exactly as described in the previous section that uses the\nregistry modules method.\nAdding custom metadata to packages\n==================================\nThe variable\n:term:`PACKAGE_ADD_METADATA`\ncan be used to add additional metadata to packages. This is reflected in\nthe package control/spec file. To take the ipk format for example, the\nCONTROL file stored inside would contain the additional metadata as\nadditional lines.\nThe variable can be used in multiple ways, including using suffixes to\nset it for a specific package type and/or package. Note that the order\nof precedence is the same as this list:\n-  ``PACKAGE_ADD_METADATA_<PKGTYPE>:<PN>``\n-  ``PACKAGE_ADD_METADATA_<PKGTYPE>``\n-  ``PACKAGE_ADD_METADATA:<PN>``\n-  :term:`PACKAGE_ADD_METADATA`\n`<PKGTYPE>` is a parameter and expected to be a distinct name of specific\npackage type:\n-  IPK for .ipk packages\n-  DEB for .deb packages\n-  RPM for .rpm packages\n`<PN>` is a parameter and expected to be a package name.\nThe variable can contain multiple [one-line] metadata fields separated\nby the literal sequence '\\\\n'. The separator can be redefined using the\nvariable flag ``separator``.\nHere is an example that adds two custom fields for ipk\npackages::\nPACKAGE_ADD_METADATA_IPK = \"Vendor: CustomIpk\\nGroup:Applications/Spreadsheets\""}
{"text": "\nBuilding\n********\nThis section describes various build procedures, such as the steps\nneeded for a simple build, building a target for multiple configurations,\ngenerating an image for more than one machine, and so forth.\nBuilding a Simple Image\n=======================\nIn the development environment, you need to build an image whenever you\nchange hardware support, add or change system libraries, or add or\nchange services that have dependencies. There are several methods that allow\nyou to build an image within the Yocto Project. This section presents\nthe basic steps you need to build a simple image using BitBake from a\nbuild host running Linux.\n.. note::\n-  For information on how to build an image using\n:term:`Toaster`, see the\n:doc:`/toaster-manual/index`.\n-  For information on how to use ``devtool`` to build images, see the\n\":ref:`sdk-manual/extensible:using \\`\\`devtool\\`\\` in your sdk workflow`\"\nsection in the Yocto Project Application Development and the\nExtensible Software Development Kit (eSDK) manual.\n-  For a quick example on how to build an image using the\nOpenEmbedded build system, see the\n:doc:`/brief-yoctoprojectqs/index` document.\n-  You can also use the `Yocto Project BitBake\n<https://marketplace.visualstudio.com/items?itemName=yocto-project.yocto-bitbake>`__\nextension for Visual Studio Code to build images.\nThe build process creates an entire Linux distribution from source and\nplaces it in your :term:`Build Directory` under ``tmp/deploy/images``. For\ndetailed information on the build process using BitBake, see the\n\":ref:`overview-manual/concepts:images`\" section in the Yocto Project Overview\nand Concepts Manual.\nThe following figure and list overviews the build process:\n.. image:: figures/bitbake-build-flow.png\n:width: 100%\n#. *Set up Your Host Development System to Support Development Using the\nYocto Project*: See the \":doc:`start`\" section for options on how to get a\nbuild host ready to use the Yocto Project.\n#. *Initialize the Build Environment:* Initialize the build environment\nby sourcing the build environment script (i.e.\n:ref:`structure-core-script`)::\n$ source oe-init-build-env [build_dir]\nWhen you use the initialization script, the OpenEmbedded build system\nuses ``build`` as the default :term:`Build Directory` in your current work\ndirectory. You can use a `build_dir` argument with the script to\nspecify a different :term:`Build Directory`.\n.. note::\nA common practice is to use a different :term:`Build Directory` for\ndifferent targets; for example, ``~/build/x86`` for a ``qemux86``\ntarget, and ``~/build/arm`` for a ``qemuarm`` target. In any\nevent, it's typically cleaner to locate the :term:`Build Directory`\nsomewhere outside of your source directory.\n#. *Make Sure Your* ``local.conf`` *File is Correct*: Ensure the\n``conf/local.conf`` configuration file, which is found in the\n:term:`Build Directory`, is set up how you want it. This file defines many\naspects of the build environment including the target machine architecture\nthrough the :term:`MACHINE` variable, the packaging format used during\nthe build (:term:`PACKAGE_CLASSES`), and a centralized tarball download\ndirectory through the :term:`DL_DIR` variable.\n#. *Build the Image:* Build the image using the ``bitbake`` command::\n$ bitbake target\n.. note::\nFor information on BitBake, see the :doc:`bitbake:index`.\nThe target is the name of the recipe you want to build. Common\ntargets are the images in ``meta/recipes-core/images``,\n``meta/recipes-sato/images``, and so forth all found in the\n:term:`Source Directory`. Alternatively, the target\ncan be the name of a recipe for a specific piece of software such as\nBusyBox. For more details about the images the OpenEmbedded build\nsystem supports, see the\n\":ref:`ref-manual/images:Images`\" chapter in the Yocto\nProject Reference Manual.\nAs an example, the following command builds the\n``core-image-minimal`` image::\n$ bitbake core-image-minimal\nOnce an\nimage has been built, it often needs to be installed. The images and\nkernels built by the OpenEmbedded build system are placed in the\n:term:`Build Directory` in ``tmp/deploy/images``. For information on how to\nrun pre-built images such as ``qemux86`` and ``qemuarm``, see the\n:doc:`/sdk-manual/index` manual. For\ninformation about how to install these images, see the documentation\nfor your particular board or machine.\nBuilding Images for Multiple Targets Using Multiple Configurations\n==================================================================\nYou can use a single ``bitbake`` command to build multiple images or\npackages for different targets where each image or package requires a\ndifferent configuration (multiple configuration builds). The builds, in\nthis scenario, are sometimes referred to as \"multiconfigs\", and this\nsection uses that term throughout.\nThis section describes how to set up for multiple configuration builds\nand how to account for cross-build dependencies between the\nmulticonfigs.\nSetting Up and Running a Multiple Configuration Build\n-----------------------------------------------------\nTo accomplish a multiple configuration build, you must define each\ntarget's configuration separately using a parallel configuration file in\nthe :term:`Build Directory` or configuration directory within a layer, and you\nmust follow a required file hierarchy. Additionally, you must enable the"}
{"text": "\nmultiple configuration builds in your ``local.conf`` file.\nFollow these steps to set up and execute multiple configuration builds:\n-  *Create Separate Configuration Files*: You need to create a single\nconfiguration file for each build target (each multiconfig).\nThe configuration definitions are implementation dependent but often\neach configuration file will define the machine and the\ntemporary directory BitBake uses for the build. Whether the same\ntemporary directory (:term:`TMPDIR`) can be shared will depend on what is\nsimilar and what is different between the configurations. Multiple MACHINE\ntargets can share the same (:term:`TMPDIR`) as long as the rest of the\nconfiguration is the same, multiple :term:`DISTRO` settings would need separate\n(:term:`TMPDIR`) directories.\nFor example, consider a scenario with two different multiconfigs for the same\n:term:`MACHINE`: \"qemux86\" built\nfor two distributions such as \"poky\" and \"poky-lsb\". In this case,\nyou would need to use the different :term:`TMPDIR`.\nHere is an example showing the minimal statements needed in a\nconfiguration file for a \"qemux86\" target whose temporary build\ndirectory is ``tmpmultix86``::\nMACHINE = \"qemux86\"\nTMPDIR = \"${TOPDIR}/tmpmultix86\"\nThe location for these multiconfig configuration files is specific.\nThey must reside in the current :term:`Build Directory` in a sub-directory of\n``conf`` named ``multiconfig`` or within a layer's ``conf`` directory\nunder a directory named ``multiconfig``. Following is an example that defines\ntwo configuration files for the \"x86\" and \"arm\" multiconfigs:\n.. image:: figures/multiconfig_files.png\n:align: center\n:width: 50%\nThe usual :term:`BBPATH` search path is used to locate multiconfig files in\na similar way to other conf files.\n-  *Add the BitBake Multi-configuration Variable to the Local\nConfiguration File*: Use the\n:term:`BBMULTICONFIG`\nvariable in your ``conf/local.conf`` configuration file to specify\neach multiconfig. Continuing with the example from the previous\nfigure, the :term:`BBMULTICONFIG` variable needs to enable two\nmulticonfigs: \"x86\" and \"arm\" by specifying each configuration file::\nBBMULTICONFIG = \"x86 arm\"\n.. note::\nA \"default\" configuration already exists by definition. This\nconfiguration is named: \"\" (i.e. empty string) and is defined by\nthe variables coming from your ``local.conf``\nfile. Consequently, the previous example actually adds two\nadditional configurations to your build: \"arm\" and \"x86\" along\nwith \"\".\n-  *Launch BitBake*: Use the following BitBake command form to launch\nthe multiple configuration build::\n$ bitbake [mc:multiconfigname:]target [[[mc:multiconfigname:]target] ... ]\nFor the example in this section, the following command applies::\n$ bitbake mc:x86:core-image-minimal mc:arm:core-image-sato mc::core-image-base\nThe previous BitBake command builds a ``core-image-minimal`` image\nthat is configured through the ``x86.conf`` configuration file, a\n``core-image-sato`` image that is configured through the ``arm.conf``\nconfiguration file and a ``core-image-base`` that is configured\nthrough your ``local.conf`` configuration file.\n.. note::\nSupport for multiple configuration builds in the Yocto Project &DISTRO;\n(&DISTRO_NAME;) Release does not include Shared State (sstate)\noptimizations. Consequently, if a build uses the same object twice\nin, for example, two different :term:`TMPDIR`\ndirectories, the build either loads from an existing sstate cache for\nthat build at the start or builds the object fresh.\nEnabling Multiple Configuration Build Dependencies\n--------------------------------------------------\nSometimes dependencies can exist between targets (multiconfigs) in a\nmultiple configuration build. For example, suppose that in order to\nbuild a ``core-image-sato`` image for an \"x86\" multiconfig, the root\nfilesystem of an \"arm\" multiconfig must exist. This dependency is\nessentially that the\n:ref:`ref-tasks-image` task in the\n``core-image-sato`` recipe depends on the completion of the\n:ref:`ref-tasks-rootfs` task of the\n``core-image-minimal`` recipe.\nTo enable dependencies in a multiple configuration build, you must\ndeclare the dependencies in the recipe using the following statement\nform::\ntask_or_package[mcdepends] = \"mc:from_multiconfig:to_multiconfig:recipe_name:task_on_which_to_depend\"\nTo better show how to use this statement, consider the example scenario\nfrom the first paragraph of this section. The following statement needs\nto be added to the recipe that builds the ``core-image-sato`` image::\ndo_image[mcdepends] = \"mc:x86:arm:core-image-minimal:do_rootfs\"\nIn this example, the `from_multiconfig` is \"x86\". The `to_multiconfig` is \"arm\". The\ntask on which the :ref:`ref-tasks-image` task in the recipe depends is the\n:ref:`ref-tasks-rootfs` task from the ``core-image-minimal`` recipe associated\nwith the \"arm\" multiconfig.\nOnce you set up this dependency, you can build the \"x86\" multiconfig\nusing a BitBake command as follows::\n$ bitbake mc:x86:core-image-sato\nThis command executes all the tasks needed to create the\n``core-image-sato`` image for the \"x86\" multiconfig. Because of the\ndependency, BitBake also executes through the :ref:`ref-tasks-rootfs` task for the\n\"arm\" multiconfig build.\nHaving a recipe depend on the root filesystem of another build might not\nseem that useful. Consider this change to the statement in the\n``core-image-sato`` recipe::\ndo_image[mcdepends] = \"mc:x86:arm:core-image-minimal:do_image\"\nIn this case, BitBake must\ncreate the ``core-image-minimal`` image for the \"arm\" build since the\n\"x86\" build depends on it."}
{"text": "\nBecause \"x86\" and \"arm\" are enabled for multiple configuration builds\nand have separate configuration files, BitBake places the artifacts for\neach build in the respective temporary build directories (i.e.\n:term:`TMPDIR`).\nBuilding an Initial RAM Filesystem (Initramfs) Image\n====================================================\nAn initial RAM filesystem (:term:`Initramfs`) image provides a temporary root\nfilesystem used for early system initialization, typically providing tools and\nloading modules needed to locate and mount the final root filesystem.\nFollow these steps to create an :term:`Initramfs` image:\n#. *Create the Initramfs Image Recipe:* You can reference the\n``core-image-minimal-initramfs.bb`` recipe found in the\n``meta/recipes-core`` directory of the :term:`Source Directory`\nas an example from which to work.\n#. *Decide if You Need to Bundle the Initramfs Image Into the Kernel\nImage:* If you want the :term:`Initramfs` image that is built to be bundled\nin with the kernel image, set the :term:`INITRAMFS_IMAGE_BUNDLE`\nvariable to ``\"1\"`` in your ``local.conf`` configuration file and set the\n:term:`INITRAMFS_IMAGE` variable in the recipe that builds the kernel image.\nSetting the :term:`INITRAMFS_IMAGE_BUNDLE` flag causes the :term:`Initramfs`\nimage to be unpacked into the ``${B}/usr/`` directory. The unpacked\n:term:`Initramfs` image is then passed to the kernel's ``Makefile`` using the\n:term:`CONFIG_INITRAMFS_SOURCE` variable, allowing the :term:`Initramfs`\nimage to be built into the kernel normally.\n#. *Optionally Add Items to the Initramfs Image Through the Initramfs\nImage Recipe:* If you add items to the :term:`Initramfs` image by way of its\nrecipe, you should use :term:`PACKAGE_INSTALL` rather than\n:term:`IMAGE_INSTALL`. :term:`PACKAGE_INSTALL` gives more direct control of\nwhat is added to the image as compared to the defaults you might not\nnecessarily want that are set by the :ref:`ref-classes-image`\nor :ref:`ref-classes-core-image` classes.\n#. *Build the Kernel Image and the Initramfs Image:* Build your kernel\nimage using BitBake. Because the :term:`Initramfs` image recipe is a\ndependency of the kernel image, the :term:`Initramfs` image is built as well\nand bundled with the kernel image if you used the\n:term:`INITRAMFS_IMAGE_BUNDLE` variable described earlier.\nBundling an Initramfs Image From a Separate Multiconfig\n-------------------------------------------------------\nThere may be a case where we want to build an :term:`Initramfs` image which does not\ninherit the same distro policy as our main image, for example, we may want\nour main image to use ``TCLIBC=\"glibc\"``, but to use ``TCLIBC=\"musl\"`` in our :term:`Initramfs`\nimage to keep a smaller footprint. However, by performing the steps mentioned\nabove the :term:`Initramfs` image will inherit ``TCLIBC=\"glibc\"`` without allowing us\nto override it.\nTo achieve this, you need to perform some additional steps:\n#. *Create a multiconfig for your Initramfs image:* You can perform the steps\non \":ref:`dev-manual/building:building images for multiple targets using multiple configurations`\" to create a separate multiconfig.\nFor the sake of simplicity let's assume such multiconfig is called: ``initramfscfg.conf`` and\ncontains the variables::\nTMPDIR=\"${TOPDIR}/tmp-initramfscfg\"\nTCLIBC=\"musl\"\n#. *Set additional Initramfs variables on your main configuration:*\nAdditionally, on your main configuration (``local.conf``) you need to set the\nvariables::\nINITRAMFS_MULTICONFIG = \"initramfscfg\"\nINITRAMFS_DEPLOY_DIR_IMAGE = \"${TOPDIR}/tmp-initramfscfg/deploy/images/${MACHINE}\"\nThe variables :term:`INITRAMFS_MULTICONFIG` and :term:`INITRAMFS_DEPLOY_DIR_IMAGE`\nare used to create a multiconfig dependency from the kernel to the :term:`INITRAMFS_IMAGE`\nto be built coming from the ``initramfscfg`` multiconfig, and to let the\nbuildsystem know where the :term:`INITRAMFS_IMAGE` will be located.\nBuilding a system with such configuration will build the kernel using the\nmain configuration but the :ref:`ref-tasks-bundle_initramfs` task will grab the\nselected :term:`INITRAMFS_IMAGE` from :term:`INITRAMFS_DEPLOY_DIR_IMAGE`\ninstead, resulting in a musl based :term:`Initramfs` image bundled in the kernel\nbut a glibc based main image.\nThe same is applicable to avoid inheriting :term:`DISTRO_FEATURES` on :term:`INITRAMFS_IMAGE`\nor to build a different :term:`DISTRO` for it such as ``poky-tiny``.\nBuilding a Tiny System\n======================\nVery small distributions have some significant advantages such as\nrequiring less on-die or in-package memory (cheaper), better performance\nthrough efficient cache usage, lower power requirements due to less\nmemory, faster boot times, and reduced development overhead. Some\nreal-world examples where a very small distribution gives you distinct\nadvantages are digital cameras, medical devices, and small headless\nsystems.\nThis section presents information that shows you how you can trim your\ndistribution to even smaller sizes than the ``poky-tiny`` distribution,\nwhich is around 5 Mbytes, that can be built out-of-the-box using the\nYocto Project.\nTiny System Overview\n--------------------\nThe following list presents the overall steps you need to consider and\nperform to create distributions with smaller root filesystems, achieve\nfaster boot times, maintain your critical functionality, and avoid\ninitial RAM disks:\n-  :ref:`Determine your goals and guiding principles\n<dev-manual/building:goals and guiding principles>`\n-  :ref:`dev-manual/building:understand what contributes to your image size`\n-  :ref:`Reduce the size of the root filesystem\n<dev-manual/building:trim the root filesystem>`\n-  :ref:`Reduce the size of the kernel <dev-manual/building:trim the kernel>`\n-  :ref:`dev-manual/building:remove package management requirements`\n-  :ref:`dev-manual/building:look for other ways to minimize size`\n-  :ref:`dev-manual/building:iterate on the process`\nGoals and Guiding Principles\n----------------------------\nBefore you can reach your destination, you need to know where you are\ngoing. Here is an example list that you can use as a guide when creating\nvery small distributions:"}
{"text": "\n-  Determine how much space you need (e.g. a kernel that is 1 Mbyte or\nless and a root filesystem that is 3 Mbytes or less).\n-  Find the areas that are currently taking 90% of the space and\nconcentrate on reducing those areas.\n-  Do not create any difficult \"hacks\" to achieve your goals.\n-  Leverage the device-specific options.\n-  Work in a separate layer so that you keep changes isolated. For\ninformation on how to create layers, see the\n\":ref:`dev-manual/layers:understanding and creating layers`\" section.\nUnderstand What Contributes to Your Image Size\n----------------------------------------------\nIt is easiest to have something to start with when creating your own\ndistribution. You can use the Yocto Project out-of-the-box to create the\n``poky-tiny`` distribution. Ultimately, you will want to make changes in\nyour own distribution that are likely modeled after ``poky-tiny``.\n.. note::\nTo use ``poky-tiny`` in your build, set the :term:`DISTRO` variable in your\n``local.conf`` file to \"poky-tiny\" as described in the\n\":ref:`dev-manual/custom-distribution:creating your own distribution`\"\nsection.\nUnderstanding some memory concepts will help you reduce the system size.\nMemory consists of static, dynamic, and temporary memory. Static memory\nis the TEXT (code), DATA (initialized data in the code), and BSS\n(uninitialized data) sections. Dynamic memory represents memory that is\nallocated at runtime: stacks, hash tables, and so forth. Temporary\nmemory is recovered after the boot process. This memory consists of\nmemory used for decompressing the kernel and for the ``__init__``\nfunctions.\nTo help you see where you currently are with kernel and root filesystem\nsizes, you can use two tools found in the :term:`Source Directory`\nin the\n``scripts/tiny/`` directory:\n-  ``ksize.py``: Reports component sizes for the kernel build objects.\n-  ``dirsize.py``: Reports component sizes for the root filesystem.\nThis next tool and command help you organize configuration fragments and\nview file dependencies in a human-readable form:\n-  ``merge_config.sh``: Helps you manage configuration files and\nfragments within the kernel. With this tool, you can merge individual\nconfiguration fragments together. The tool allows you to make\noverrides and warns you of any missing configuration options. The\ntool is ideal for allowing you to iterate on configurations, create\nminimal configurations, and create configuration files for different\nmachines without having to duplicate your process.\nThe ``merge_config.sh`` script is part of the Linux Yocto kernel Git\nrepositories (i.e. ``linux-yocto-3.14``, ``linux-yocto-3.10``,\n``linux-yocto-3.8``, and so forth) in the ``scripts/kconfig``\ndirectory.\nFor more information on configuration fragments, see the\n\":ref:`kernel-dev/common:creating configuration fragments`\"\nsection in the Yocto Project Linux Kernel Development Manual.\n-  ``bitbake -u taskexp -g bitbake_target``: Using the BitBake command\nwith these options brings up a Dependency Explorer from which you can\nview file dependencies. Understanding these dependencies allows you\nto make informed decisions when cutting out various pieces of the\nkernel and root filesystem.\nTrim the Root Filesystem\n------------------------\nThe root filesystem is made up of packages for booting, libraries, and\napplications. To change things, you can configure how the packaging\nhappens, which changes the way you build them. You can also modify the\nfilesystem itself or select a different filesystem.\nFirst, find out what is hogging your root filesystem by running the\n``dirsize.py`` script from your root directory::\n$ cd root-directory-of-image\n$ dirsize.py 100000 > dirsize-100k.log\n$ cat dirsize-100k.log\nYou can apply a filter to the script to ignore files\nunder a certain size. The previous example filters out any files below\n100 Kbytes. The sizes reported by the tool are uncompressed, and thus\nwill be smaller by a relatively constant factor in a compressed root\nfilesystem. When you examine your log file, you can focus on areas of\nthe root filesystem that take up large amounts of memory.\nYou need to be sure that what you eliminate does not cripple the\nfunctionality you need. One way to see how packages relate to each other\nis by using the Dependency Explorer UI with the BitBake command::\n$ cd image-directory\n$ bitbake -u taskexp -g image\nUse the interface to\nselect potential packages you wish to eliminate and see their dependency\nrelationships.\nWhen deciding how to reduce the size, get rid of packages that result in\nminimal impact on the feature set. For example, you might not need a VGA\ndisplay. Or, you might be able to get by with ``devtmpfs`` and ``mdev``\ninstead of ``udev``.\nUse your ``local.conf`` file to make changes. For example, to eliminate\n``udev`` and ``glib``, set the following in the local configuration\nfile::\nVIRTUAL-RUNTIME_dev_manager = \"\"\nFinally, you should consider exactly the type of root filesystem you\nneed to meet your needs while also reducing its size. For example,\nconsider ``cramfs``, ``squashfs``, ``ubifs``, ``ext2``, or an\n:term:`Initramfs` using ``initramfs``. Be aware that ``ext3`` requires a 1\nMbyte journal. If you are okay with running read-only, you do not need\nthis journal.\n.. note::\nAfter each round of elimination, you need to rebuild your system and\nthen use the tools to see the effects of your reductions.\nTrim the Kernel\n---------------\nThe kernel is built by including policies for hardware-independent"}
{"text": "\naspects. What subsystems do you enable? For what architecture are you\nbuilding? Which drivers do you build by default?\n.. note::\nYou can modify the kernel source if you want to help with boot time.\nRun the ``ksize.py`` script from the top-level Linux build directory to\nget an idea of what is making up the kernel::\n$ cd top-level-linux-build-directory\n$ ksize.py > ksize.log\n$ cat ksize.log\nWhen you examine the log, you will see how much space is taken up with\nthe built-in ``.o`` files for drivers, networking, core kernel files,\nfilesystem, sound, and so forth. The sizes reported by the tool are\nuncompressed, and thus will be smaller by a relatively constant factor\nin a compressed kernel image. Look to reduce the areas that are large\nand taking up around the \"90% rule.\"\nTo examine, or drill down, into any particular area, use the ``-d``\noption with the script::\n$ ksize.py -d > ksize.log\nUsing this option\nbreaks out the individual file information for each area of the kernel\n(e.g. drivers, networking, and so forth).\nUse your log file to see what you can eliminate from the kernel based on\nfeatures you can let go. For example, if you are not going to need\nsound, you do not need any drivers that support sound.\nAfter figuring out what to eliminate, you need to reconfigure the kernel\nto reflect those changes during the next build. You could run\n``menuconfig`` and make all your changes at once. However, that makes it\ndifficult to see the effects of your individual eliminations and also\nmakes it difficult to replicate the changes for perhaps another target\ndevice. A better method is to start with no configurations using\n``allnoconfig``, create configuration fragments for individual changes,\nand then manage the fragments into a single configuration file using\n``merge_config.sh``. The tool makes it easy for you to iterate using the\nconfiguration change and build cycle.\nEach time you make configuration changes, you need to rebuild the kernel\nand check to see what impact your changes had on the overall size.\nRemove Package Management Requirements\n--------------------------------------\nPackaging requirements add size to the image. One way to reduce the size\nof the image is to remove all the packaging requirements from the image.\nThis reduction includes both removing the package manager and its unique\ndependencies as well as removing the package management data itself.\nTo eliminate all the packaging requirements for an image, be sure that\n\"package-management\" is not part of your\n:term:`IMAGE_FEATURES`\nstatement for the image. When you remove this feature, you are removing\nthe package manager as well as its dependencies from the root\nfilesystem.\nLook for Other Ways to Minimize Size\n------------------------------------\nDepending on your particular circumstances, other areas that you can\ntrim likely exist. The key to finding these areas is through tools and\nmethods described here combined with experimentation and iteration. Here\nare a couple of areas to experiment with:\n-  ``glibc``: In general, follow this process:\n#. Remove ``glibc`` features from\n:term:`DISTRO_FEATURES`\nthat you think you do not need.\n#. Build your distribution.\n#. If the build fails due to missing symbols in a package, determine\nif you can reconfigure the package to not need those features. For\nexample, change the configuration to not support wide character\nsupport as is done for ``ncurses``. Or, if support for those\ncharacters is needed, determine what ``glibc`` features provide\nthe support and restore the configuration.\n4. Rebuild and repeat the process.\n-  ``busybox``: For BusyBox, use a process similar as described for\n``glibc``. A difference is you will need to boot the resulting system\nto see if you are able to do everything you expect from the running\nsystem. You need to be sure to integrate configuration fragments into\nBusybox because BusyBox handles its own core features and then allows\nyou to add configuration fragments on top.\nIterate on the Process\n----------------------\nIf you have not reached your goals on system size, you need to iterate\non the process. The process is the same. Use the tools and see just what\nis taking up 90% of the root filesystem and the kernel. Decide what you\ncan eliminate without limiting your device beyond what you need.\nDepending on your system, a good place to look might be Busybox, which\nprovides a stripped down version of Unix tools in a single, executable\nfile. You might be able to drop virtual terminal services or perhaps\nipv6.\nBuilding Images for More than One Machine\n=========================================\nA common scenario developers face is creating images for several\ndifferent machines that use the same software environment. In this\nsituation, it is tempting to set the tunings and optimization flags for\neach build specifically for the targeted hardware (i.e. \"maxing out\" the\ntunings). Doing so can considerably add to build times and package feed\nmaintenance collectively for the machines. For example, selecting tunes\nthat are extremely specific to a CPU core used in a system might enable\nsome micro optimizations in GCC for that particular system but would\notherwise not gain you much of a performance difference across the other\nsystems as compared to using a more general tuning across all the builds\n(e.g. setting :term:`DEFAULTTUNE`\nspecifically for each machine's build). Rather than \"max out\" each\nbuild's tunings, you can take steps that cause the OpenEmbedded build\nsystem to reuse software across the various machines where it makes\nsense.\nIf build speed and package feed maintenance are considerations, you"}
{"text": "\nshould consider the points in this section that can help you optimize\nyour tunings to best consider build times and package feed maintenance.\n-  *Share the :term:`Build Directory`:* If at all possible, share the\n:term:`TMPDIR` across builds. The Yocto Project supports switching between\ndifferent :term:`MACHINE` values in the same :term:`TMPDIR`. This practice\nis well supported and regularly used by developers when building for\nmultiple machines. When you use the same :term:`TMPDIR` for multiple\nmachine builds, the OpenEmbedded build system can reuse the existing native\nand often cross-recipes for multiple machines. Thus, build time decreases.\n.. note::\nIf :term:`DISTRO` settings change or fundamental configuration settings\nsuch as the filesystem layout, you need to work with a clean :term:`TMPDIR`.\nSharing :term:`TMPDIR` under these circumstances might work but since it is\nnot guaranteed, you should use a clean :term:`TMPDIR`.\n-  *Enable the Appropriate Package Architecture:* By default, the\nOpenEmbedded build system enables three levels of package\narchitectures: \"all\", \"tune\" or \"package\", and \"machine\". Any given\nrecipe usually selects one of these package architectures (types) for\nits output. Depending for what a given recipe creates packages,\nmaking sure you enable the appropriate package architecture can\ndirectly impact the build time.\nA recipe that just generates scripts can enable \"all\" architecture\nbecause there are no binaries to build. To specifically enable \"all\"\narchitecture, be sure your recipe inherits the\n:ref:`ref-classes-allarch` class.\nThis class is useful for \"all\" architectures because it configures\nmany variables so packages can be used across multiple architectures.\nIf your recipe needs to generate packages that are machine-specific\nor when one of the build or runtime dependencies is already\nmachine-architecture dependent, which makes your recipe also\nmachine-architecture dependent, make sure your recipe enables the\n\"machine\" package architecture through the\n:term:`MACHINE_ARCH`\nvariable::\nPACKAGE_ARCH = \"${MACHINE_ARCH}\"\nWhen you do not\nspecifically enable a package architecture through the\n:term:`PACKAGE_ARCH`, The\nOpenEmbedded build system defaults to the\n:term:`TUNE_PKGARCH` setting::\nPACKAGE_ARCH = \"${TUNE_PKGARCH}\"\n-  *Choose a Generic Tuning File if Possible:* Some tunes are more\ngeneric and can run on multiple targets (e.g. an ``armv5`` set of\npackages could run on ``armv6`` and ``armv7`` processors in most\ncases). Similarly, ``i486`` binaries could work on ``i586`` and\nhigher processors. You should realize, however, that advances on\nnewer processor versions would not be used.\nIf you select the same tune for several different machines, the\nOpenEmbedded build system reuses software previously built, thus\nspeeding up the overall build time. Realize that even though a new\nsysroot for each machine is generated, the software is not recompiled\nand only one package feed exists.\n-  *Manage Granular Level Packaging:* Sometimes there are cases where\ninjecting another level of package architecture beyond the three\nhigher levels noted earlier can be useful. For example, consider how\nNXP (formerly Freescale) allows for the easy reuse of binary packages\nin their layer\n:yocto_git:`meta-freescale </meta-freescale/>`.\nIn this example, the\n:yocto_git:`fsl-dynamic-packagearch </meta-freescale/tree/classes/fsl-dynamic-packagearch.bbclass>`\nclass shares GPU packages for i.MX53 boards because all boards share\nthe AMD GPU. The i.MX6-based boards can do the same because all\nboards share the Vivante GPU. This class inspects the BitBake\ndatastore to identify if the package provides or depends on one of\nthe sub-architecture values. If so, the class sets the\n:term:`PACKAGE_ARCH` value\nbased on the ``MACHINE_SUBARCH`` value. If the package does not\nprovide or depend on one of the sub-architecture values but it\nmatches a value in the machine-specific filter, it sets\n:term:`MACHINE_ARCH`. This\nbehavior reduces the number of packages built and saves build time by\nreusing binaries.\n-  *Use Tools to Debug Issues:* Sometimes you can run into situations\nwhere software is being rebuilt when you think it should not be. For\nexample, the OpenEmbedded build system might not be using shared\nstate between machines when you think it should be. These types of\nsituations are usually due to references to machine-specific\nvariables such as :term:`MACHINE`,\n:term:`SERIAL_CONSOLES`,\n:term:`XSERVER`,\n:term:`MACHINE_FEATURES`,\nand so forth in code that is supposed to only be tune-specific or\nwhen the recipe depends\n(:term:`DEPENDS`,\n:term:`RDEPENDS`,\n:term:`RRECOMMENDS`,\n:term:`RSUGGESTS`, and so forth)\non some other recipe that already has\n:term:`PACKAGE_ARCH` defined\nas \"${MACHINE_ARCH}\".\n.. note::\nPatches to fix any issues identified are most welcome as these\nissues occasionally do occur.\nFor such cases, you can use some tools to help you sort out the\nsituation:\n-  ``state-diff-machines.sh``*:* You can find this tool in the\n``scripts`` directory of the Source Repositories. See the comments\nin the script for information on how to use the tool.\n-  *BitBake's \"-S printdiff\" Option:* Using this option causes\nBitBake to try to establish the closest signature match it can"}
{"text": "\n(e.g. in the shared state cache) and then run ``bitbake-diffsigs``\nover the matches to determine the stamps and delta where these two\nstamp trees diverge.\nBuilding Software from an External Source\n=========================================\nBy default, the OpenEmbedded build system uses the :term:`Build Directory`\nwhen building source code. The build process involves fetching the source\nfiles, unpacking them, and then patching them if necessary before the build\ntakes place.\nThere are situations where you might want to build software from source\nfiles that are external to and thus outside of the OpenEmbedded build\nsystem. For example, suppose you have a project that includes a new BSP\nwith a heavily customized kernel. And, you want to minimize exposing the\nbuild system to the development team so that they can focus on their\nproject and maintain everyone's workflow as much as possible. In this\ncase, you want a kernel source directory on the development machine\nwhere the development occurs. You want the recipe's\n:term:`SRC_URI` variable to point to\nthe external directory and use it as is, not copy it.\nTo build from software that comes from an external source, all you need to do\nis inherit the :ref:`ref-classes-externalsrc` class and then set\nthe :term:`EXTERNALSRC` variable to point to your external source code. Here\nare the statements to put in your ``local.conf`` file::\nINHERIT += \"externalsrc\"\nEXTERNALSRC:pn-myrecipe = \"path-to-your-source-tree\"\nThis next example shows how to accomplish the same thing by setting\n:term:`EXTERNALSRC` in the recipe itself or in the recipe's append file::\nEXTERNALSRC = \"path\"\nEXTERNALSRC_BUILD = \"path\"\n.. note::\nIn order for these settings to take effect, you must globally or\nlocally inherit the :ref:`ref-classes-externalsrc` class.\nBy default, :ref:`ref-classes-externalsrc` builds the source code in a\ndirectory separate from the external source directory as specified by\n:term:`EXTERNALSRC`. If you need\nto have the source built in the same directory in which it resides, or\nsome other nominated directory, you can set\n:term:`EXTERNALSRC_BUILD`\nto point to that directory::\nEXTERNALSRC_BUILD:pn-myrecipe = \"path-to-your-source-tree\"\nReplicating a Build Offline\n===========================\nIt can be useful to take a \"snapshot\" of upstream sources used in a\nbuild and then use that \"snapshot\" later to replicate the build offline.\nTo do so, you need to first prepare and populate your downloads\ndirectory your \"snapshot\" of files. Once your downloads directory is\nready, you can use it at any time and from any machine to replicate your\nbuild.\nFollow these steps to populate your Downloads directory:\n#. *Create a Clean Downloads Directory:* Start with an empty downloads\ndirectory (:term:`DL_DIR`). You\nstart with an empty downloads directory by either removing the files\nin the existing directory or by setting :term:`DL_DIR` to point to either\nan empty location or one that does not yet exist.\n#. *Generate Tarballs of the Source Git Repositories:* Edit your\n``local.conf`` configuration file as follows::\nDL_DIR = \"/home/your-download-dir/\"\nBB_GENERATE_MIRROR_TARBALLS = \"1\"\nDuring\nthe fetch process in the next step, BitBake gathers the source files\nand creates tarballs in the directory pointed to by :term:`DL_DIR`. See\nthe\n:term:`BB_GENERATE_MIRROR_TARBALLS`\nvariable for more information.\n#. *Populate Your Downloads Directory Without Building:* Use BitBake to\nfetch your sources but inhibit the build::\n$ bitbake target --runonly=fetch\nThe downloads directory (i.e. ``${DL_DIR}``) now has\na \"snapshot\" of the source files in the form of tarballs, which can\nbe used for the build.\n#. *Optionally Remove Any Git or other SCM Subdirectories From the\nDownloads Directory:* If you want, you can clean up your downloads\ndirectory by removing any Git or other Source Control Management\n(SCM) subdirectories such as ``${DL_DIR}/git2/*``. The tarballs\nalready contain these subdirectories.\nOnce your downloads directory has everything it needs regarding source\nfiles, you can create your \"own-mirror\" and build your target.\nUnderstand that you can use the files to build the target offline from\nany machine and at any time.\nFollow these steps to build your target using the files in the downloads\ndirectory:\n#. *Using Local Files Only:* Inside your ``local.conf`` file, add the\n:term:`SOURCE_MIRROR_URL` variable, inherit the\n:ref:`ref-classes-own-mirrors` class, and use the\n:term:`BB_NO_NETWORK` variable to your ``local.conf``::\nSOURCE_MIRROR_URL ?= \"file:///home/your-download-dir/\"\nINHERIT += \"own-mirrors\"\nBB_NO_NETWORK = \"1\"\nThe :term:`SOURCE_MIRROR_URL` and :ref:`ref-classes-own-mirrors`\nclass set up the system to use the downloads directory as your \"own\nmirror\". Using the :term:`BB_NO_NETWORK` variable makes sure that\nBitBake's fetching process in step 3 stays local, which means files\nfrom your \"own-mirror\" are used.\n#. *Start With a Clean Build:* You can start with a clean build by\nremoving the ``${``\\ :term:`TMPDIR`\\ ``}`` directory or using a new\n:term:`Build Directory`.\n#. *Build Your Target:* Use BitBake to build your target::\n$ bitbake target\nThe build completes using the known local \"snapshot\" of source\nfiles from your mirror. The resulting tarballs for your \"snapshot\" of"}
{"text": "Building\nsource files are in the downloads directory.\n.. note::\nThe offline build does not work if recipes attempt to find the\nlatest version of software by setting\n:term:`SRCREV` to\n``${``\\ :term:`AUTOREV`\\ ``}``::\nSRCREV = \"${AUTOREV}\"\nWhen a recipe sets :term:`SRCREV` to\n``${``\\ :term:`AUTOREV`\\ ``}``, the build system accesses the network in an\nattempt to determine the latest version of software from the SCM.\nTypically, recipes that use :term:`AUTOREV` are custom or modified\nrecipes. Recipes that reside in public repositories usually do not\nuse :term:`AUTOREV`.\nIf you do have recipes that use :term:`AUTOREV`, you can take steps to\nstill use the recipes in an offline build. Do the following:\n#. Use a configuration generated by enabling :ref:`build\nhistory <dev-manual/build-quality:maintaining build output quality>`.\n#. Use the ``buildhistory-collect-srcrevs`` command to collect the\nstored :term:`SRCREV` values from the build's history. For more\ninformation on collecting these values, see the\n\":ref:`dev-manual/build-quality:build history package information`\"\nsection.\n#. Once you have the correct source revisions, you can modify\nthose recipes to set :term:`SRCREV` to specific versions of the\nsoftware."}
{"text": "\nChecking for Vulnerabilities\n****************************\nVulnerabilities in Poky and OE-Core\n===================================\nThe Yocto Project has an infrastructure to track and address unfixed\nknown security vulnerabilities, as tracked by the public\n:wikipedia:`Common Vulnerabilities and Exposures (CVE) <Common_Vulnerabilities_and_Exposures>`\ndatabase.\nThe Yocto Project maintains a `list of known vulnerabilities\n<https://autobuilder.yocto.io/pub/non-release/patchmetrics/>`__\nfor packages in Poky and OE-Core, tracking the evolution of the number of\nunpatched CVEs and the status of patches. Such information is available for\nthe current development version and for each supported release.\nSecurity is a process, not a product, and thus at any time, a number of security\nissues may be impacting Poky and OE-Core. It is up to the maintainers, users,\ncontributors and anyone interested in the issues to investigate and possibly fix them by\nupdating software components to newer versions or by applying patches to address them.\nIt is recommended to work with Poky and OE-Core upstream maintainers and submit\npatches to fix them, see \":doc:`../contributor-guide/submit-changes`\" for details.\nVulnerability check at build time\n=================================\nTo enable a check for CVE security vulnerabilities using\n:ref:`ref-classes-cve-check` in the specific image or target you are building,\nadd the following setting to your configuration::\nINHERIT += \"cve-check\"\nThe CVE database contains some old incomplete entries which have been\ndeemed not to impact Poky or OE-Core. These CVE entries can be excluded from the\ncheck using build configuration::\ninclude conf/distro/include/cve-extra-exclusions.inc\nWith this CVE check enabled, BitBake build will try to map each compiled software component\nrecipe name and version information to the CVE database and generate recipe and\nimage specific reports. These reports will contain:\n-  metadata about the software component like names and versions\n-  metadata about the CVE issue such as description and NVD link\n-  for each software component, a list of CVEs which are possibly impacting this version\n-  status of each CVE: ``Patched``, ``Unpatched`` or ``Ignored``\nThe status ``Patched`` means that a patch file to address the security issue has been\napplied. ``Unpatched`` status means that no patches to address the issue have been\napplied and that the issue needs to be investigated. ``Ignored`` means that after\nanalysis, it has been deemed to ignore the issue as it for example affects\nthe software component on a different operating system platform.\nAfter a build with CVE check enabled, reports for each compiled source recipe will be\nfound in ``build/tmp/deploy/cve``.\nFor example the CVE check report for the ``flex-native`` recipe looks like::\n$ cat poky/build/tmp/deploy/cve/flex-native\nLAYER: meta\nPACKAGE NAME: flex-native\nPACKAGE VERSION: 2.6.4\nCVE: CVE-2016-6354\nCVE STATUS: Patched\nCVE SUMMARY: Heap-based buffer overflow in the yy_get_next_buffer function in Flex before 2.6.1 might allow context-dependent attackers to cause a denial of service or possibly execute arbitrary code via vectors involving num_to_read.\nCVSS v2 BASE SCORE: 7.5\nCVSS v3 BASE SCORE: 9.8\nVECTOR: NETWORK\nMORE INFORMATION: https://nvd.nist.gov/vuln/detail/CVE-2016-6354\nLAYER: meta\nPACKAGE NAME: flex-native\nPACKAGE VERSION: 2.6.4\nCVE: CVE-2019-6293\nCVE STATUS: Ignored\nCVE SUMMARY: An issue was discovered in the function mark_beginning_as_normal in nfa.c in flex 2.6.4. There is a stack exhaustion problem caused by the mark_beginning_as_normal function making recursive calls to itself in certain scenarios involving lots of '*' characters. Remote attackers could leverage this vulnerability to cause a denial-of-service.\nCVSS v2 BASE SCORE: 4.3\nCVSS v3 BASE SCORE: 5.5\nVECTOR: NETWORK\nMORE INFORMATION: https://nvd.nist.gov/vuln/detail/CVE-2019-6293\nFor images, a summary of all recipes included in the image and their CVEs is also\ngenerated in textual and JSON formats. These ``.cve`` and ``.json`` reports can be found\nin the ``tmp/deploy/images`` directory for each compiled image.\nAt build time CVE check will also throw warnings about ``Unpatched`` CVEs::\nWARNING: flex-2.6.4-r0 do_cve_check: Found unpatched CVE (CVE-2019-6293), for more information check /poky/build/tmp/work/core2-64-poky-linux/flex/2.6.4-r0/temp/cve.log\nWARNING: libarchive-3.5.1-r0 do_cve_check: Found unpatched CVE (CVE-2021-36976), for more information check /poky/build/tmp/work/core2-64-poky-linux/libarchive/3.5.1-r0/temp/cve.log\nIt is also possible to check the CVE status of individual packages as follows::\nbitbake -c cve_check flex libarchive\nFixing CVE product name and version mappings\n============================================\nBy default, :ref:`ref-classes-cve-check` uses the recipe name :term:`BPN` as CVE\nproduct name when querying the CVE database. If this mapping contains false positives, e.g.\nsome reported CVEs are not for the software component in question, or false negatives like\nsome CVEs are not found to impact the recipe when they should, then the problems can be\nin the recipe name to CVE product mapping. These mapping issues can be fixed by setting\nthe :term:`CVE_PRODUCT` variable inside the recipe. This defines the name of the software component in the\nupstream `NIST CVE database <https://nvd.nist.gov/>`__.\nThe variable supports using vendor and product names like this::\nCVE_PRODUCT = \"flex_project:flex\"\nIn this example the vendor name used in the CVE database is ``flex_project`` and the\nproduct is ``flex``. With this setting the ``flex`` recipe only maps to this specific\nproduct and not products from other vendors with same name ``flex``.\nSimilarly, when the recipe version :term:`PV` is not compatible with software versions used by\nthe upstream software component releases and the CVE database, these can be fixed using\nthe :term:`CVE_VERSION` variable.\nNote that if the CVE entries in the NVD database contain bugs or have missing or incomplete\ninformation, it is recommended to fix the information there directly instead of working\naround the issues possibly for a long time in Poky and OE-Core side recipes. Feedback to\nNVD about CVE entries can be provided through the `NVD contact form <https://nvd.nist.gov/info/contact-form>`__.\nFixing vulnerabilities in recipes\n=================================\nSuppose a CVE security issue impacts a software component. In that case, it can\nbe fixed by updating to a newer version, by applying a patch, or by marking it\nas patched via :term:`CVE_STATUS` variable flag. For Poky and OE-Core master"}
{"text": "\nbranches, updating to a more recent software component release with fixes is\nthe best option, but patches can be applied if releases are not yet available.\nFor stable branches, we want to avoid API (Application Programming Interface)\nor ABI (Application Binary Interface) breakages. When submitting an update,\na minor version update of a component is preferred if the version is\nbackward-compatible. Many software components have backward-compatible stable\nversions, with a notable example of the Linux kernel. However, if the new\nversion does or likely might introduce incompatibilities, extracting and\nbackporting patches is preferred.\nHere is an example of fixing CVE security issues with patch files,\nan example from the :oe_layerindex:`ffmpeg recipe for dunfell </layerindex/recipe/122174>`::\nSRC_URI = \"https://www.ffmpeg.org/releases/${BP}.tar.xz \\\nfile://mips64_cpu_detection.patch \\\nfile://CVE-2020-12284.patch \\\nfile://0001-libavutil-include-assembly-with-full-path-from-sourc.patch \\\nfile://CVE-2021-3566.patch \\\nfile://CVE-2021-38291.patch \\\nfile://CVE-2022-1475.patch \\\nfile://CVE-2022-3109.patch \\\nfile://CVE-2022-3341.patch \\\nfile://CVE-2022-48434.patch \\\n\"\nThe recipe has both generic and security-related fixes. The CVE patch files are named\naccording to the CVE they fix.\nWhen preparing the patch file, take the original patch from the upstream repository.\nDo not use patches from different distributions, except if it is the only available source.\nModify the patch adding OE-related metadata. We will follow the example of the\n``CVE-2022-3341.patch``.\nThe original `commit message <https://github.com/FFmpeg/FFmpeg/commit/9cf652cef49d74afe3d454f27d49eb1a1394951e.patch/>`__\nis::\nFrom 9cf652cef49d74afe3d454f27d49eb1a1394951e Mon Sep 17 00:00:00 2001\nFrom: Jiasheng Jiang <jiasheng@iscas.ac.cn>\nDate: Wed, 23 Feb 2022 10:31:59 +0800\nSubject: [PATCH] avformat/nutdec: Add check for avformat_new_stream\nCheck for failure of avformat_new_stream() and propagate\nthe error code.\nSigned-off-by: Michael Niedermayer <michael@niedermayer.cc>\n---\nlibavformat/nutdec.c | 16 ++++++++++++----\n1 file changed, 12 insertions(+), 4 deletions(-)\nFor the correct operations of the ``cve-check``, it requires the CVE\nidentification in a ``CVE:`` tag of the patch file commit message using\nthe format::\nCVE: CVE-2022-3341\nIt is also recommended to add the ``Upstream-Status:`` tag with a link\nto the original patch and sign-off by people working on the backport.\nIf there are any modifications to the original patch, note them in\nthe ``Comments:`` tag.\nWith the additional information, the header of the patch file in OE-core becomes::\nFrom 9cf652cef49d74afe3d454f27d49eb1a1394951e Mon Sep 17 00:00:00 2001\nFrom: Jiasheng Jiang <jiasheng@iscas.ac.cn>\nDate: Wed, 23 Feb 2022 10:31:59 +0800\nSubject: [PATCH] avformat/nutdec: Add check for avformat_new_stream\nCheck for failure of avformat_new_stream() and propagate\nthe error code.\nSigned-off-by: Michael Niedermayer <michael@niedermayer.cc>\nCVE: CVE-2022-3341\nUpstream-Status: Backport [https://github.com/FFmpeg/FFmpeg/commit/9cf652cef49d74afe3d454f27d49eb1a1394951e]\nComments: Refreshed Hunk\nSigned-off-by: Narpat Mali <narpat.mali@windriver.com>\nSigned-off-by: Bhabu Bindu <bhabu.bindu@kpit.com>\n---\nlibavformat/nutdec.c | 16 ++++++++++++----\n1 file changed, 12 insertions(+), 4 deletions(-)\nA good practice is to include the CVE identifier in the patch file name, the patch file\ncommit message and optionally in the recipe commit message.\nCVE checker will then capture this information and change the CVE status to ``Patched``\nin the generated reports.\nIf analysis shows that the CVE issue does not impact the recipe due to configuration, platform,\nversion or other reasons, the CVE can be marked as ``Ignored`` by using\nthe :term:`CVE_STATUS` variable flag with appropriate reason which is mapped to ``Ignored``.\nThe entry should have the format like::\nCVE_STATUS[CVE-2016-10642] = \"cpe-incorrect: This is specific to the npm package that installs cmake, so isn't relevant to OpenEmbedded\"\nAs mentioned previously, if data in the CVE database is wrong, it is recommended\nto fix those issues in the CVE database (NVD in the case of OE-core and Poky)\ndirectly.\nNote that if there are many CVEs with the same status and reason, those can be\nshared by using the :term:`CVE_STATUS_GROUPS` variable.\nRecipes can be completely skipped by CVE check by including the recipe name in\nthe :term:`CVE_CHECK_SKIP_RECIPE` variable.\nImplementation details\n======================\nHere's what the :ref:`ref-classes-cve-check` class does to find unpatched CVE IDs.\nFirst the code goes through each patch file provided by a recipe. If a valid CVE ID\nis found in the name of the file, the corresponding CVE is considered as patched.\nDon't forget that if multiple CVE IDs are found in the filename, only the last\none is considered. Then, the code looks for ``CVE: CVE-ID`` lines in the patch\nfile. The found CVE IDs are also considered as patched.\nAdditionally ``CVE_STATUS`` variable flags are parsed for reasons mapped to ``Patched``\nand these are also considered as patched.\nThen, the code looks up all the CVE IDs in the NIST database for all the\nproducts defined in :term:`CVE_PRODUCT`. Then, for each found CVE:\n-  If the package name (:term:`PN`) is part of\n:term:`CVE_CHECK_SKIP_RECIPE`, it is considered as ``Patched``.\n-  If the CVE ID has status ``CVE_STATUS[<CVE ID>] = \"ignored\"`` or if it's set to\nany reason which is mapped to status ``Ignored`` via ``CVE_CHECK_STATUSMAP``,\nit is  set as ``Ignored``.\n-  If the CVE ID is part of the patched CVE for the recipe, it is\nalready considered as ``Patched``.\n-  Otherwise, the code checks whether the recipe version (:term:`PV`)"}
{"text": "Checking for Vulnerabilities\nis within the range of versions impacted by the CVE. If so, the CVE\nis considered as ``Unpatched``.\nThe CVE database is stored in :term:`DL_DIR` and can be inspected using\n``sqlite3`` command as follows::\nsqlite3 downloads/CVE_CHECK/nvdcve_1.1.db .dump | grep CVE-2021-37462\nWhen analyzing CVEs, it is recommended to:\n-  study the latest information in `CVE database <https://nvd.nist.gov/vuln/search>`__.\n-  check how upstream developers of the software component addressed the issue, e.g.\nwhat patch was applied, which upstream release contains the fix.\n-  check what other Linux distributions like `Debian <https://security-tracker.debian.org/tracker/>`__\ndid to analyze and address the issue.\n-  follow security notices from other Linux distributions.\n-  follow public `open source security mailing lists <https://oss-security.openwall.org/wiki/mailing-lists>`__ for\ndiscussions and advance notifications of CVE bugs and software releases with fixes."}
{"text": "\nUsing Wayland and Weston\n************************\n:wikipedia:`Wayland <Wayland_(display_server_protocol)>`\nis a computer display server protocol that provides a method for\ncompositing window managers to communicate directly with applications\nand video hardware and expects them to communicate with input hardware\nusing other libraries. Using Wayland with supporting targets can result\nin better control over graphics frame rendering than an application\nmight otherwise achieve.\nThe Yocto Project provides the Wayland protocol libraries and the\nreference :wikipedia:`Weston <Wayland_(display_server_protocol)#Weston>`\ncompositor as part of its release. You can find the integrated packages\nin the ``meta`` layer of the :term:`Source Directory`.\nSpecifically, you\ncan find the recipes that build both Wayland and Weston at\n``meta/recipes-graphics/wayland``.\nYou can build both the Wayland and Weston packages for use only with targets\nthat accept the :wikipedia:`Mesa 3D and Direct Rendering Infrastructure\n<Mesa_(computer_graphics)>`, which is also known as Mesa DRI. This implies that\nyou cannot build and use the packages if your target uses, for example, the\nIntel Embedded Media and Graphics Driver (Intel EMGD) that overrides Mesa DRI.\n.. note::\nDue to lack of EGL support, Weston 1.0.3 will not run directly on the\nemulated QEMU hardware. However, this version of Weston will run\nunder X emulation without issues.\nThis section describes what you need to do to implement Wayland and use\nthe Weston compositor when building an image for a supporting target.\nEnabling Wayland in an Image\n============================\nTo enable Wayland, you need to enable it to be built and enable it to be\nincluded (installed) in the image.\nBuilding Wayland\n----------------\nTo cause Mesa to build the ``wayland-egl`` platform and Weston to build\nWayland with Kernel Mode Setting\n(`KMS <https://wiki.archlinux.org/index.php/Kernel_Mode_Setting>`__)\nsupport, include the \"wayland\" flag in the\n:term:`DISTRO_FEATURES`\nstatement in your ``local.conf`` file::\nDISTRO_FEATURES:append = \" wayland\"\n.. note::\nIf X11 has been enabled elsewhere, Weston will build Wayland with X11\nsupport\nInstalling Wayland and Weston\n-----------------------------\nTo install the Wayland feature into an image, you must include the\nfollowing\n:term:`CORE_IMAGE_EXTRA_INSTALL`\nstatement in your ``local.conf`` file::\nCORE_IMAGE_EXTRA_INSTALL += \"wayland weston\"\nRunning Weston\n==============\nTo run Weston inside X11, enabling it as described earlier and building\na Sato image is sufficient. If you are running your image under Sato, a\nWeston Launcher appears in the \"Utility\" category.\nAlternatively, you can run Weston through the command-line interpretor\n(CLI), which is better suited for development work. To run Weston under\nthe CLI, you need to do the following after your image is built:\n#. Run these commands to export ``XDG_RUNTIME_DIR``::\nmkdir -p /tmp/$USER-weston\nchmod 0700 /tmp/$USER-weston\nexport XDG_RUNTIME_DIR=/tmp/$USER-weston\n#. Launch Weston in the shell::\nweston"}
{"text": "\nCreating Your Own Distribution\n******************************\nWhen you build an image using the Yocto Project and do not alter any\ndistribution :term:`Metadata`, you are\ncreating a Poky distribution. If you wish to gain more control over\npackage alternative selections, compile-time options, and other\nlow-level configurations, you can create your own distribution.\nTo create your own distribution, the basic steps consist of creating\nyour own distribution layer, creating your own distribution\nconfiguration file, and then adding any needed code and Metadata to the\nlayer. The following steps provide some more detail:\n-  *Create a layer for your new distro:* Create your distribution layer\nso that you can keep your Metadata and code for the distribution\nseparate. It is strongly recommended that you create and use your own\nlayer for configuration and code. Using your own layer as compared to\njust placing configurations in a ``local.conf`` configuration file\nmakes it easier to reproduce the same build configuration when using\nmultiple build machines. See the\n\":ref:`dev-manual/layers:creating a general layer using the \\`\\`bitbake-layers\\`\\` script`\"\nsection for information on how to quickly set up a layer.\n-  *Create the distribution configuration file:* The distribution\nconfiguration file needs to be created in the ``conf/distro``\ndirectory of your layer. You need to name it using your distribution\nname (e.g. ``mydistro.conf``).\n.. note::\nThe :term:`DISTRO` variable in your ``local.conf`` file determines the\nname of your distribution.\nYou can split out parts of your configuration file into include files\nand then \"require\" them from within your distribution configuration\nfile. Be sure to place the include files in the\n``conf/distro/include`` directory of your layer. A common example\nusage of include files would be to separate out the selection of\ndesired version and revisions for individual recipes.\nYour configuration file needs to set the following required\nvariables:\n- :term:`DISTRO_NAME`\n- :term:`DISTRO_VERSION`\nThese following variables are optional and you typically set them\nfrom the distribution configuration file:\n- :term:`DISTRO_FEATURES`\n- :term:`DISTRO_EXTRA_RDEPENDS`\n- :term:`DISTRO_EXTRA_RRECOMMENDS`\n- :term:`TCLIBC`\n.. tip::\nIf you want to base your distribution configuration file on the\nvery basic configuration from OE-Core, you can use\n``conf/distro/defaultsetup.conf`` as a reference and just include\nvariables that differ as compared to ``defaultsetup.conf``.\nAlternatively, you can create a distribution configuration file\nfrom scratch using the ``defaultsetup.conf`` file or configuration files\nfrom another distribution such as Poky as a reference.\n-  *Provide miscellaneous variables:* Be sure to define any other\nvariables for which you want to create a default or enforce as part\nof the distribution configuration. You can include nearly any\nvariable from the ``local.conf`` file. The variables you use are not\nlimited to the list in the previous bulleted item.\n-  *Point to Your distribution configuration file:* In your ``local.conf``\nfile in the :term:`Build Directory`, set your :term:`DISTRO` variable to\npoint to your distribution's configuration file. For example, if your\ndistribution's configuration file is named ``mydistro.conf``, then\nyou point to it as follows::\nDISTRO = \"mydistro\"\n-  *Add more to the layer if necessary:* Use your layer to hold other\ninformation needed for the distribution:\n-  Add recipes for installing distro-specific configuration files\nthat are not already installed by another recipe. If you have\ndistro-specific configuration files that are included by an\nexisting recipe, you should add an append file (``.bbappend``) for\nthose. For general information and recommendations on how to add\nrecipes to your layer, see the\n\":ref:`dev-manual/layers:creating your own layer`\" and\n\":ref:`dev-manual/layers:following best practices when creating layers`\"\nsections.\n-  Add any image recipes that are specific to your distribution.\n-  Add a ``psplash`` append file for a branded splash screen, using\nthe :term:`SPLASH_IMAGES` variable.\n-  Add any other append files to make custom changes that are\nspecific to individual recipes.\nFor information on append files, see the\n\":ref:`dev-manual/layers:appending other layers metadata with your layer`\"\nsection."}
{"text": "\nEnabling GObject Introspection Support\n**************************************\n`GObject introspection <https://gi.readthedocs.io/en/latest/>`__\nis the standard mechanism for accessing GObject-based software from\nruntime environments. GObject is a feature of the GLib library that\nprovides an object framework for the GNOME desktop and related software.\nGObject Introspection adds information to GObject that allows objects\ncreated within it to be represented across different programming\nlanguages. If you want to construct GStreamer pipelines using Python, or\ncontrol UPnP infrastructure using Javascript and GUPnP, GObject\nintrospection is the only way to do it.\nThis section describes the Yocto Project support for generating and\npackaging GObject introspection data. GObject introspection data is a\ndescription of the API provided by libraries built on top of the GLib\nframework, and, in particular, that framework's GObject mechanism.\nGObject Introspection Repository (GIR) files go to ``-dev`` packages,\n``typelib`` files go to main packages as they are packaged together with\nlibraries that are introspected.\nThe data is generated when building such a library, by linking the\nlibrary with a small executable binary that asks the library to describe\nitself, and then executing the binary and processing its output.\nGenerating this data in a cross-compilation environment is difficult\nbecause the library is produced for the target architecture, but its\ncode needs to be executed on the build host. This problem is solved with\nthe OpenEmbedded build system by running the code through QEMU, which\nallows precisely that. Unfortunately, QEMU does not always work\nperfectly as mentioned in the \":ref:`dev-manual/gobject-introspection:known issues`\"\nsection.\nEnabling the Generation of Introspection Data\n=============================================\nEnabling the generation of introspection data (GIR files) in your\nlibrary package involves the following:\n#. Inherit the :ref:`ref-classes-gobject-introspection` class.\n#. Make sure introspection is not disabled anywhere in the recipe or\nfrom anything the recipe includes. Also, make sure that\n\"gobject-introspection-data\" is not in\n:term:`DISTRO_FEATURES_BACKFILL_CONSIDERED`\nand that \"qemu-usermode\" is not in\n:term:`MACHINE_FEATURES_BACKFILL_CONSIDERED`.\nIn either of these conditions, nothing will happen.\n#. Try to build the recipe. If you encounter build errors that look like\nsomething is unable to find ``.so`` libraries, check where these\nlibraries are located in the source tree and add the following to the\nrecipe::\nGIR_EXTRA_LIBS_PATH = \"${B}/something/.libs\"\n.. note::\nSee recipes in the ``oe-core`` repository that use that\n:term:`GIR_EXTRA_LIBS_PATH` variable as an example.\n#. Look for any other errors, which probably mean that introspection\nsupport in a package is not entirely standard, and thus breaks down\nin a cross-compilation environment. For such cases, custom-made fixes\nare needed. A good place to ask and receive help in these cases is\nthe :ref:`Yocto Project mailing\nlists <resources-mailinglist>`.\n.. note::\nUsing a library that no longer builds against the latest Yocto\nProject release and prints introspection related errors is a good\ncandidate for the previous procedure.\nDisabling the Generation of Introspection Data\n==============================================\nYou might find that you do not want to generate introspection data. Or,\nperhaps QEMU does not work on your build host and target architecture\ncombination. If so, you can use either of the following methods to\ndisable GIR file generations:\n-  Add the following to your distro configuration::\nDISTRO_FEATURES_BACKFILL_CONSIDERED = \"gobject-introspection-data\"\nAdding this statement disables generating introspection data using\nQEMU but will still enable building introspection tools and libraries\n(i.e. building them does not require the use of QEMU).\n-  Add the following to your machine configuration::\nMACHINE_FEATURES_BACKFILL_CONSIDERED = \"qemu-usermode\"\nAdding this statement disables the use of QEMU when building packages for your\nmachine. Currently, this feature is used only by introspection\nrecipes and has the same effect as the previously described option.\n.. note::\nFuture releases of the Yocto Project might have other features\naffected by this option.\nIf you disable introspection data, you can still obtain it through other\nmeans such as copying the data from a suitable sysroot, or by generating\nit on the target hardware. The OpenEmbedded build system does not\ncurrently provide specific support for these techniques.\nTesting that Introspection Works in an Image\n============================================\nUse the following procedure to test if generating introspection data is\nworking in an image:\n#. Make sure that \"gobject-introspection-data\" is not in\n:term:`DISTRO_FEATURES_BACKFILL_CONSIDERED`\nand that \"qemu-usermode\" is not in\n:term:`MACHINE_FEATURES_BACKFILL_CONSIDERED`.\n#. Build ``core-image-sato``.\n#. Launch a Terminal and then start Python in the terminal.\n#. Enter the following in the terminal::\n>>> from gi.repository import GLib\n>>> GLib.get_host_name()\n#. For something a little more advanced, enter the following see:\nhttps://python-gtk-3-tutorial.readthedocs.io/en/latest/introduction.html\nKnown Issues\n============\nHere are know issues in GObject Introspection Support:"}
{"text": "Enabling GObject Introspection Support\n-  ``qemu-ppc64`` immediately crashes. Consequently, you cannot build\nintrospection data on that architecture.\n-  x32 is not supported by QEMU. Consequently, introspection data is\ndisabled.\n-  musl causes transient GLib binaries to crash on assertion failures.\nConsequently, generating introspection data is disabled.\n-  Because QEMU is not able to run the binaries correctly, introspection\nis disabled for some specific packages under specific architectures\n(e.g. ``gcr``, ``libsecret``, and ``webkit``).\n-  QEMU usermode might not work properly when running 64-bit binaries\nunder 32-bit host machines. In particular, \"qemumips64\" is known to\nnot work under i686."}
{"text": "\n***********************************\nSetting Up to Use the Yocto Project\n***********************************\nThis chapter provides guidance on how to prepare to use the Yocto\nProject. You can learn about creating a team environment to develop\nusing the Yocto Project, how to set up a :ref:`build\nhost <dev-manual/start:preparing the build host>`, how to locate\nYocto Project source repositories, and how to create local Git\nrepositories.\nCreating a Team Development Environment\n=======================================\nIt might not be immediately clear how you can use the Yocto Project in a\nteam development environment, or how to scale it for a large team of\ndevelopers. You can adapt the Yocto Project to many different use cases\nand scenarios; however, this flexibility could cause difficulties if you\nare trying to create a working setup that scales effectively.\nTo help you understand how to set up this type of environment, this\nsection presents a procedure that gives you information that can help\nyou get the results you want. The procedure is high-level and presents\nsome of the project's most successful experiences, practices, solutions,\nand available technologies that have proved to work well in the past;\nhowever, keep in mind, the procedure here is simply a starting point.\nYou can build off these steps and customize the procedure to fit any\nparticular working environment and set of practices.\n#.  *Determine Who is Going to be Developing:* You first need to\nunderstand who is going to be doing anything related to the Yocto\nProject and determine their roles. Making this determination is\nessential to completing subsequent steps, which are to get your\nequipment together and set up your development environment's\nhardware topology.\nHere are possible roles:\n-  *Application Developer:* This type of developer does application\nlevel work on top of an existing software stack.\n-  *Core System Developer:* This type of developer works on the\ncontents of the operating system image itself.\n-  *Build Engineer:* This type of developer manages Autobuilders and\nreleases. Depending on the specifics of the environment, not all\nsituations might need a Build Engineer.\n-  *Test Engineer:* This type of developer creates and manages\nautomated tests that are used to ensure all application and core\nsystem development meets desired quality standards.\n#.  *Gather the Hardware:* Based on the size and make-up of the team,\nget the hardware together. Ideally, any development, build, or test\nengineer uses a system that runs a supported Linux distribution.\nThese systems, in general, should be high performance (e.g. dual,\nsix-core Xeons with 24 Gbytes of RAM and plenty of disk space). You\ncan help ensure efficiency by having any machines used for testing\nor that run Autobuilders be as high performance as possible.\n.. note::\nGiven sufficient processing power, you might also consider\nbuilding Yocto Project development containers to be run under\nDocker, which is described later.\n#.  *Understand the Hardware Topology of the Environment:* Once you\nunderstand the hardware involved and the make-up of the team, you\ncan understand the hardware topology of the development environment.\nYou can get a visual idea of the machines and their roles across the\ndevelopment environment.\n#.  *Use Git as Your Source Control Manager (SCM):* Keeping your\n:term:`Metadata` (i.e. recipes,\nconfiguration files, classes, and so forth) and any software you are\ndeveloping under the control of an SCM system that is compatible\nwith the OpenEmbedded build system is advisable. Of all of the SCMs\nsupported by BitBake, the Yocto Project team strongly recommends using\n:ref:`overview-manual/development-environment:git`.\nGit is a distributed system\nthat is easy to back up, allows you to work remotely, and then\nconnects back to the infrastructure.\n.. note::\nFor information about BitBake, see the\n:doc:`bitbake:index`.\nIt is relatively easy to set up Git services and create infrastructure like\n:yocto_git:`/`, which is based on server software called\n`Gitolite <https://gitolite.com>`__\nwith `cgit <https://git.zx2c4.com/cgit/about/>`__ being used to\ngenerate the web interface that lets you view the repositories.\n``gitolite`` identifies users using SSH keys and allows\nbranch-based access controls to repositories that you can control as\nlittle or as much as necessary.\n#.  *Set up the Application Development Machines:* As mentioned earlier,\napplication developers are creating applications on top of existing\nsoftware stacks. Following are some best practices for setting up\nmachines used for application development:\n-  Use a pre-built toolchain that contains the software stack\nitself. Then, develop the application code on top of the stack.\nThis method works well for small numbers of relatively isolated\napplications.\n-  Keep your cross-development toolchains updated. You can do this\nthrough provisioning either as new toolchain downloads or as\nupdates through a package update mechanism using ``opkg`` to\nprovide updates to an existing toolchain. The exact mechanics of\nhow and when to do this depend on local policy.\n-  Use multiple toolchains installed locally into different\nlocations to allow development across versions.\n#.  *Set up the Core Development Machines:* As mentioned earlier, core\ndevelopers work on the contents of the operating system itself.\nFollowing are some best practices for setting up machines used for\ndeveloping images:\n-  Have the :term:`OpenEmbedded Build System` available on\nthe developer workstations so developers can run their own builds"}
{"text": "\nand directly rebuild the software stack.\n-  Keep the core system unchanged as much as possible and do your\nwork in layers on top of the core system. Doing so gives you a\ngreater level of portability when upgrading to new versions of\nthe core system or Board Support Packages (BSPs).\n-  Share layers amongst the developers of a particular project and\ncontain the policy configuration that defines the project.\n#.  *Set up an Autobuilder:* Autobuilders are often the core of the\ndevelopment environment. It is here that changes from individual\ndevelopers are brought together and centrally tested. Based on this\nautomated build and test environment, subsequent decisions about\nreleases can be made. Autobuilders also allow for \"continuous\nintegration\" style testing of software components and regression\nidentification and tracking.\nSee \":yocto_ab:`Yocto Project Autobuilder <>`\" for more\ninformation and links to buildbot. The Yocto Project team has found\nthis implementation works well in this role. A public example of\nthis is the Yocto Project Autobuilders, which the Yocto Project team\nuses to test the overall health of the project.\nThe features of this system are:\n-  Highlights when commits break the build.\n-  Populates an :ref:`sstate\ncache <overview-manual/concepts:shared state cache>` from which\ndevelopers can pull rather than requiring local builds.\n-  Allows commit hook triggers, which trigger builds when commits\nare made.\n-  Allows triggering of automated image booting and testing under\nthe QuickEMUlator (QEMU).\n-  Supports incremental build testing and from-scratch builds.\n-  Shares output that allows developer testing and historical\nregression investigation.\n-  Creates output that can be used for releases.\n-  Allows scheduling of builds so that resources can be used\nefficiently.\n#.  *Set up Test Machines:* Use a small number of shared, high\nperformance systems for testing purposes. Developers can use these\nsystems for wider, more extensive testing while they continue to\ndevelop locally using their primary development system.\n#.  *Document Policies and Change Flow:* The Yocto Project uses a\nhierarchical structure and a pull model. There are scripts to create and\nsend pull requests (i.e. ``create-pull-request`` and\n``send-pull-request``). This model is in line with other open source\nprojects where maintainers are responsible for specific areas of the\nproject and a single maintainer handles the final \"top-of-tree\"\nmerges.\n.. note::\nYou can also use a more collective push model. The ``gitolite``\nsoftware supports both the push and pull models quite easily.\nAs with any development environment, it is important to document the\npolicy used as well as any main project guidelines so they are\nunderstood by everyone. It is also a good idea to have\nwell-structured commit messages, which are usually a part of a\nproject's guidelines. Good commit messages are essential when\nlooking back in time and trying to understand why changes were made.\nIf you discover that changes are needed to the core layer of the\nproject, it is worth sharing those with the community as soon as\npossible. Chances are if you have discovered the need for changes,\nsomeone else in the community needs them also.\n#.  *Development Environment Summary:* Aside from the previous steps,\nhere are best practices within the Yocto Project development\nenvironment:\n-  Use :ref:`overview-manual/development-environment:git` as the source control\nsystem.\n-  Maintain your Metadata in layers that make sense for your\nsituation. See the \":ref:`overview-manual/yp-intro:the yocto project layer model`\"\nsection in the Yocto Project Overview and Concepts Manual and the\n\":ref:`dev-manual/layers:understanding and creating layers`\"\nsection for more information on layers.\n-  Separate the project's Metadata and code by using separate Git\nrepositories. See the \":ref:`overview-manual/development-environment:yocto project source repositories`\"\nsection in the Yocto Project Overview and Concepts Manual for\ninformation on these repositories. See the\n\":ref:`dev-manual/start:locating yocto project source files`\"\nsection for information on how to set up local Git repositories\nfor related upstream Yocto Project Git repositories.\n-  Set up the directory for the shared state cache\n(:term:`SSTATE_DIR`) where\nit makes sense. For example, set up the sstate cache on a system\nused by developers in the same organization and share the same\nsource directories on their machines.\n-  Set up an Autobuilder and have it populate the sstate cache and\nsource directories.\n-  The Yocto Project community encourages you to send patches to the\nproject to fix bugs or add features. If you do submit patches,\nfollow the project commit guidelines for writing good commit\nmessages. See the \":doc:`../contributor-guide/submit-changes`\"\nsection in the Yocto Project and OpenEmbedded Contributor Guide.\n-  Send changes to the core sooner than later as others are likely\nto run into the same issues. For some guidance on mailing lists\nto use, see the lists in the\n\":ref:`contributor-guide/submit-changes:finding a suitable mailing list`\"\nsection. For a description\nof the available mailing lists, see the \":ref:`resources-mailinglist`\" section in\nthe Yocto Project Reference Manual.\nPreparing the Build Host\n========================\nThis section provides procedures to set up a system to be used as your\n:term:`Build Host` for\ndevelopment using the Yocto Project. Your build host can be a native\nLinux machine (recommended), it can be a machine (Linux, Mac, or"}
{"text": "\nWindows) that uses `CROPS <https://github.com/crops/poky-container>`__,\nwhich leverages `Docker Containers <https://www.docker.com/>`__ or it\ncan be a Windows machine capable of running version 2 of Windows Subsystem\nFor Linux (WSL 2).\n.. note::\nThe Yocto Project is not compatible with version 1 of\n:wikipedia:`Windows Subsystem for Linux <Windows_Subsystem_for_Linux>`.\nIt is compatible but neither officially supported nor validated with\nWSL 2. If you still decide to use WSL please upgrade to\n`WSL 2 <https://learn.microsoft.com/en-us/windows/wsl/install>`__.\nOnce your build host is set up to use the Yocto Project, further steps\nare necessary depending on what you want to accomplish. See the\nfollowing references for information on how to prepare for Board Support\nPackage (BSP) development and kernel development:\n-  *BSP Development:* See the \":ref:`bsp-guide/bsp:preparing your build host to work with bsp layers`\"\nsection in the Yocto Project Board Support Package (BSP) Developer's\nGuide.\n-  *Kernel Development:* See the \":ref:`kernel-dev/common:preparing the build host to work on the kernel`\"\nsection in the Yocto Project Linux Kernel Development Manual.\nSetting Up a Native Linux Host\n------------------------------\nFollow these steps to prepare a native Linux machine as your Yocto\nProject Build Host:\n#. *Use a Supported Linux Distribution:* You should have a reasonably\ncurrent Linux-based host system. You will have the best results with\na recent release of Fedora, openSUSE, Debian, Ubuntu, RHEL or CentOS\nas these releases are frequently tested against the Yocto Project and\nofficially supported. For a list of the distributions under\nvalidation and their status, see the \":ref:`Supported Linux\nDistributions <system-requirements-supported-distros>`\"\nsection in the Yocto Project Reference Manual and the wiki page at\n:yocto_wiki:`Distribution Support </Distribution_Support>`.\n#. *Have Enough Free Memory:* Your system should have at least 50 Gbytes\nof free disk space for building images.\n#. *Meet Minimal Version Requirements:* The OpenEmbedded build system\nshould be able to run on any modern distribution that has the\nfollowing versions for Git, tar, Python, gcc and make.\n-  Git &MIN_GIT_VERSION; or greater\n-  tar &MIN_TAR_VERSION; or greater\n-  Python &MIN_PYTHON_VERSION; or greater.\n-  gcc &MIN_GCC_VERSION; or greater.\n-  GNU make &MIN_MAKE_VERSION; or greater\nIf your build host does not meet any of these listed version\nrequirements, you can take steps to prepare the system so that you\ncan still use the Yocto Project. See the\n\":ref:`ref-manual/system-requirements:required git, tar, python, make and gcc versions`\"\nsection in the Yocto Project Reference Manual for information.\n#. *Install Development Host Packages:* Required development host\npackages vary depending on your build host and what you want to do\nwith the Yocto Project. Collectively, the number of required packages\nis large if you want to be able to cover all cases.\nFor lists of required packages for all scenarios, see the\n\":ref:`ref-manual/system-requirements:required packages for the build host`\"\nsection in the Yocto Project Reference Manual.\nOnce you have completed the previous steps, you are ready to continue\nusing a given development path on your native Linux machine. If you are\ngoing to use BitBake, see the\n\":ref:`dev-manual/start:cloning the \\`\\`poky\\`\\` repository`\"\nsection. If you are going\nto use the Extensible SDK, see the \":doc:`/sdk-manual/extensible`\" Chapter in the Yocto\nProject Application Development and the Extensible Software Development\nKit (eSDK) manual. If you want to work on the kernel, see the :doc:`/kernel-dev/index`. If you are going to use\nToaster, see the \":doc:`/toaster-manual/setup-and-use`\"\nsection in the Toaster User Manual. If you are a VSCode user, you can configure\nthe `Yocto Project BitBake\n<https://marketplace.visualstudio.com/items?itemName=yocto-project.yocto-bitbake>`__\nextension accordingly.\nSetting Up to Use CROss PlatformS (CROPS)\n-----------------------------------------\nWith `CROPS <https://github.com/crops/poky-container>`__, which\nleverages `Docker Containers <https://www.docker.com/>`__, you can\ncreate a Yocto Project development environment that is operating system\nagnostic. You can set up a container in which you can develop using the\nYocto Project on a Windows, Mac, or Linux machine.\nFollow these general steps to prepare a Windows, Mac, or Linux machine\nas your Yocto Project build host:\n#. *Determine What Your Build Host Needs:*\n`Docker <https://www.docker.com/what-docker>`__ is a software\ncontainer platform that you need to install on the build host.\nDepending on your build host, you might have to install different\nsoftware to support Docker containers. Go to the Docker installation\npage and read about the platform requirements in \"`Supported\nPlatforms <https://docs.docker.com/engine/install/#supported-platforms>`__\"\nyour build host needs to run containers.\n#. *Choose What To Install:* Depending on whether or not your build host\nmeets system requirements, you need to install \"Docker CE Stable\" or\nthe \"Docker Toolbox\". Most situations call for Docker CE. However, if\nyou have a build host that does not meet requirements (e.g.\nPre-Windows 10 or Windows 10 \"Home\" version), you must install Docker\nToolbox instead.\n#. *Go to the Install Site for Your Platform:* Click the link for the\nDocker edition associated with your build host's native software. For\nexample, if your build host is running Microsoft Windows Version 10\nand you want the Docker CE Stable edition, click that link under\n\"Supported Platforms\".\n#. *Install the Software:* Once you have understood all the\npre-requisites, you can download and install the appropriate\nsoftware. Follow the instructions for your specific machine and the\ntype of the software you need to install:\n-  Install `Docker Desktop on"}
{"text": "\nWindows <https://docs.docker.com/docker-for-windows/install/#install-docker-desktop-on-windows>`__\nfor Windows build hosts that meet requirements.\n-  Install `Docker Desktop on\nMacOs <https://docs.docker.com/docker-for-mac/install/#install-and-run-docker-desktop-on-mac>`__\nfor Mac build hosts that meet requirements.\n-  Install `Docker Engine on\nCentOS <https://docs.docker.com/engine/install/centos/>`__\nfor Linux build hosts running the CentOS distribution.\n-  Install `Docker Engine on\nDebian <https://docs.docker.com/engine/install/debian/>`__\nfor Linux build hosts running the Debian distribution.\n-  Install `Docker Engine for\nFedora <https://docs.docker.com/engine/install/fedora/>`__\nfor Linux build hosts running the Fedora distribution.\n-  Install `Docker Engine for\nUbuntu <https://docs.docker.com/engine/install/ubuntu/>`__\nfor Linux build hosts running the Ubuntu distribution.\n#. *Optionally Orient Yourself With Docker:* If you are unfamiliar with\nDocker and the container concept, you can learn more here -\nhttps://docs.docker.com/get-started/.\n#. *Launch Docker or Docker Toolbox:* You should be able to launch\nDocker or the Docker Toolbox and have a terminal shell on your\ndevelopment host.\n#. *Set Up the Containers to Use the Yocto Project:* Go to\nhttps://github.com/crops/docker-win-mac-docs/wiki and follow\nthe directions for your particular build host (i.e. Linux, Mac, or\nWindows).\nOnce you complete the setup instructions for your machine, you have\nthe Poky, Extensible SDK, and Toaster containers available. You can\nclick those links from the page and learn more about using each of\nthose containers.\nOnce you have a container set up, everything is in place to develop just\nas if you were running on a native Linux machine. If you are going to\nuse the Poky container, see the\n\":ref:`dev-manual/start:cloning the \\`\\`poky\\`\\` repository`\"\nsection. If you are going to use the Extensible SDK container, see the\n\":doc:`/sdk-manual/extensible`\" Chapter in the Yocto\nProject Application Development and the Extensible Software Development\nKit (eSDK) manual. If you are going to use the Toaster container, see\nthe \":doc:`/toaster-manual/setup-and-use`\"\nsection in the Toaster User Manual. If you are a VSCode user, you can configure\nthe `Yocto Project BitBake\n<https://marketplace.visualstudio.com/items?itemName=yocto-project.yocto-bitbake>`__\nextension accordingly.\nSetting Up to Use Windows Subsystem For Linux (WSL 2)\n-----------------------------------------------------\nWith `Windows Subsystem for Linux (WSL 2)\n<https://learn.microsoft.com/en-us/windows/wsl/>`__,\nyou can create a Yocto Project development environment that allows you\nto build on Windows. You can set up a Linux distribution inside Windows\nin which you can develop using the Yocto Project.\nFollow these general steps to prepare a Windows machine using WSL 2 as\nyour Yocto Project build host:\n#. *Make sure your Windows machine is capable of running WSL 2:*\nWhile all Windows 11 and Windows Server 2022 builds support WSL 2,\nthe first versions of Windows 10 and Windows Server 2019 didn't.\nCheck the minimum build numbers for `Windows 10\n<https://learn.microsoft.com/en-us/windows/wsl/install-manual#step-2---check-requirements-for-running-wsl-2>`__\nand for `Windows Server 2019\n<https://learn.microsoft.com/en-us/windows/wsl/install-on-server>`__.\nTo check which build version you are running, you may open a command\nprompt on Windows and execute the command \"ver\"::\nC:\\Users\\myuser> ver\nMicrosoft Windows [Version 10.0.19041.153]\n#. *Install the Linux distribution of your choice inside WSL 2:*\nOnce you know your version of Windows supports WSL 2, you can\ninstall the distribution of your choice from the Microsoft Store.\nOpen the Microsoft Store and search for Linux. While there are\nseveral Linux distributions available, the assumption is that your\npick will be one of the distributions supported by the Yocto Project\nas stated on the instructions for using a native Linux host. After\nmaking your selection, simply click \"Get\" to download and install the\ndistribution.\n#. *Check which Linux distribution WSL 2 is using:* Open a Windows\nPowerShell and run::\nC:\\WINDOWS\\system32> wsl -l -v\nNAME    STATE   VERSION\n*Ubuntu Running 2\nNote that WSL 2 supports running as many different Linux distributions\nas you want to install.\n#. *Optionally Get Familiar with WSL:* You can learn more on\nhttps://docs.microsoft.com/en-us/windows/wsl/wsl2-about.\n#. *Launch your WSL Distibution:* From the Windows start menu simply\nlaunch your WSL distribution just like any other application.\n#. *Optimize your WSL 2 storage often:* Due to the way storage is\nhandled on WSL 2, the storage space used by the underlying Linux\ndistribution is not reflected immediately, and since BitBake heavily\nuses storage, after several builds, you may be unaware you are\nrunning out of space. As WSL 2 uses a VHDX file for storage, this issue\ncan be easily avoided by regularly optimizing this file in a manual way:\n1. *Find the location of your VHDX file:*\nFirst you need to find the distro app package directory, to achieve this\nopen a Windows Powershell as Administrator and run::\nC:\\WINDOWS\\system32> Get-AppxPackage -Name \"*Ubuntu*\" | Select PackageFamilyName\nPackageFamilyName\n-----------------\nCanonicalGroupLimited.UbuntuonWindows_79abcdefgh\nYou should now\nreplace the PackageFamilyName and your user on the following path\nto find your VHDX file::"}
{"text": "\nls C:\\Users\\myuser\\AppData\\Local\\Packages\\CanonicalGroupLimited.UbuntuonWindows_79abcdefgh\\LocalState\\\nMode                 LastWriteTime         Length Name\n-a----         3/14/2020   9:52 PM    57418973184 ext4.vhdx\nYour VHDX file path is:\n``C:\\Users\\myuser\\AppData\\Local\\Packages\\CanonicalGroupLimited.UbuntuonWindows_79abcdefgh\\LocalState\\ext4.vhdx``\n2a. *Optimize your VHDX file using Windows Powershell:*\nTo use the ``optimize-vhd`` cmdlet below, first install the Hyper-V\noption on Windows. Then, open a Windows Powershell as Administrator to\noptimize your VHDX file, shutting down WSL first::\nC:\\WINDOWS\\system32> wsl --shutdown\nC:\\WINDOWS\\system32> optimize-vhd -Path C:\\Users\\myuser\\AppData\\Local\\Packages\\CanonicalGroupLimited.UbuntuonWindows_79abcdefgh\\LocalState\\ext4.vhdx -Mode full\nA progress bar should be shown while optimizing the\nVHDX file, and storage should now be reflected correctly on the\nWindows Explorer.\n2b. *Optimize your VHDX file using DiskPart:*\nThe ``optimize-vhd`` cmdlet noted in step 2a above is provided by\nHyper-V. Not all SKUs of Windows can install Hyper-V. As an alternative,\nuse the DiskPart tool. To start, open a Windows command prompt as\nAdministrator to optimize your VHDX file, shutting down WSL first::\nC:\\WINDOWS\\system32> wsl --shutdown\nC:\\WINDOWS\\system32> diskpart\nDISKPART> select vdisk file=\"<path_to_VHDX_file>\"\nDISKPART> attach vdisk readonly\nDISKPART> compact vdisk\nDISKPART> exit\n.. note::\nThe current implementation of WSL 2 does not have out-of-the-box\naccess to external devices such as those connected through a USB\nport, but it automatically mounts your ``C:`` drive on ``/mnt/c/``\n(and others), which you can use to share deploy artifacts to be later\nflashed on hardware through Windows, but your :term:`Build Directory`\nshould not reside inside this mountpoint.\nOnce you have WSL 2 set up, everything is in place to develop just as if\nyou were running on a native Linux machine. If you are going to use the\nExtensible SDK container, see the \":doc:`/sdk-manual/extensible`\" Chapter in the Yocto\nProject Application Development and the Extensible Software Development\nKit (eSDK) manual. If you are going to use the Toaster container, see\nthe \":doc:`/toaster-manual/setup-and-use`\"\nsection in the Toaster User Manual. If you are a VSCode user, you can configure\nthe `Yocto Project BitBake\n<https://marketplace.visualstudio.com/items?itemName=yocto-project.yocto-bitbake>`__\nextension accordingly.\nLocating Yocto Project Source Files\n===================================\nThis section shows you how to locate, fetch and configure the source\nfiles you'll need to work with the Yocto Project.\n.. note::\n-  For concepts and introductory information about Git as it is used\nin the Yocto Project, see the \":ref:`overview-manual/development-environment:git`\"\nsection in the Yocto Project Overview and Concepts Manual.\n-  For concepts on Yocto Project source repositories, see the\n\":ref:`overview-manual/development-environment:yocto project source repositories`\"\nsection in the Yocto Project Overview and Concepts Manual.\"\nAccessing Source Repositories\n-----------------------------\nWorking from a copy of the upstream :ref:`dev-manual/start:accessing source repositories` is the\npreferred method for obtaining and using a Yocto Project release. You\ncan view the Yocto Project Source Repositories at\n:yocto_git:`/`. In particular, you can find the ``poky``\nrepository at :yocto_git:`/poky`.\nUse the following procedure to locate the latest upstream copy of the\n``poky`` Git repository:\n#. *Access Repositories:* Open a browser and go to\n:yocto_git:`/` to access the GUI-based interface into the\nYocto Project source repositories.\n#. *Select the Repository:* Click on the repository in which you are\ninterested (e.g. ``poky``).\n#. *Find the URL Used to Clone the Repository:* At the bottom of the\npage, note the URL used to clone that repository\n(e.g. :yocto_git:`/poky`).\n.. note::\nFor information on cloning a repository, see the\n\":ref:`dev-manual/start:cloning the \\`\\`poky\\`\\` repository`\" section.\nAccessing Source Archives\n-------------------------\nThe Yocto Project also provides source archives of its releases, which\nare available on :yocto_dl:`/releases/yocto/`. Then, choose the subdirectory\ncontaining the release you wish to use, for example\n:yocto_dl:`yocto-&DISTRO; </releases/yocto/yocto-&DISTRO;/>`.\nYou will find there source archives of individual components (if you wish\nto use them individually), and of the corresponding Poky release bundling\na selection of these components.\n.. note::\nThe recommended method for accessing Yocto Project components is to\nuse Git to clone the upstream repository and work from within that\nlocally cloned repository.\nUsing the Downloads Page\n------------------------\nThe :yocto_home:`Yocto Project Website <>` uses a \"RELEASES\" page\nfrom which you can locate and download tarballs of any Yocto Project\nrelease. Rather than Git repositories, these files represent snapshot\ntarballs similar to the tarballs located in the Index of Releases\ndescribed in the \":ref:`dev-manual/start:accessing source archives`\" section.\n#. *Go to the Yocto Project Website:* Open The\n:yocto_home:`Yocto Project Website <>` in your browser.\n#. *Get to the Downloads Area:* Select the \"RELEASES\" item from the\npull-down \"DEVELOPMENT\" tab menu near the top of the page.\n#. *Select a Yocto Project Release:* On the top of the \"RELEASE\" page currently\nsupported releases are displayed, further down past supported Yocto Project\nreleases are visible. The \"Download\" links in the rows of the table there"}
{"text": "\nwill lead to the download tarballs for the release.\n(e.g. &DISTRO_NAME_NO_CAP;, &DISTRO_NAME_NO_CAP_MINUS_ONE;, and so forth).\n.. note::\nFor a \"map\" of Yocto Project releases to version numbers, see the\n:yocto_wiki:`Releases </Releases>` wiki page.\nYou can use the \"RELEASE ARCHIVE\" link to reveal a menu of all Yocto\nProject releases.\n#. *Download Tools or Board Support Packages (BSPs):* Next to the tarballs you\nwill find download tools or BSPs as well. Just select a Yocto Project\nrelease and look for what you need.\nCloning and Checking Out Branches\n=================================\nTo use the Yocto Project for development, you need a release locally\ninstalled on your development system. This locally installed set of\nfiles is referred to as the :term:`Source Directory`\nin the Yocto Project documentation.\nThe preferred method of creating your Source Directory is by using\n:ref:`overview-manual/development-environment:git` to clone a local copy of the upstream\n``poky`` repository. Working from a cloned copy of the upstream\nrepository allows you to contribute back into the Yocto Project or to\nsimply work with the latest software on a development branch. Because\nGit maintains and creates an upstream repository with a complete history\nof changes and you are working with a local clone of that repository,\nyou have access to all the Yocto Project development branches and tag\nnames used in the upstream repository.\nCloning the ``poky`` Repository\n-------------------------------\nFollow these steps to create a local version of the upstream\n:term:`Poky` Git repository.\n#. *Set Your Directory:* Change your working directory to where you want\nto create your local copy of ``poky``.\n#. *Clone the Repository:* The following example command clones the\n``poky`` repository and uses the default name \"poky\" for your local\nrepository::\n$ git clone git://git.yoctoproject.org/poky\nCloning into 'poky'...\nremote: Counting objects: 432160, done.\nremote: Compressing objects: 100% (102056/102056), done.\nremote: Total 432160 (delta 323116), reused 432037 (delta 323000)\nReceiving objects: 100% (432160/432160), 153.81 MiB | 8.54 MiB/s, done.\nResolving deltas: 100% (323116/323116), done.\nChecking connectivity... done.\nUnless you\nspecify a specific development branch or tag name, Git clones the\n\"master\" branch, which results in a snapshot of the latest\ndevelopment changes for \"master\". For information on how to check out\na specific development branch or on how to check out a local branch\nbased on a tag name, see the\n\":ref:`dev-manual/start:checking out by branch in poky`\" and\n\":ref:`dev-manual/start:checking out by tag in poky`\" sections, respectively.\nOnce the local repository is created, you can change to that\ndirectory and check its status. The ``master`` branch is checked out\nby default::\n$ cd poky\n$ git status\nOn branch master\nYour branch is up-to-date with 'origin/master'.\nnothing to commit, working directory clean\n$ git branch\n* master\nYour local repository of poky is identical to the\nupstream poky repository at the time from which it was cloned. As you\nwork with the local branch, you can periodically use the\n``git pull --rebase`` command to be sure you are up-to-date\nwith the upstream branch.\nChecking Out by Branch in Poky\n------------------------------\nWhen you clone the upstream poky repository, you have access to all its\ndevelopment branches. Each development branch in a repository is unique\nas it forks off the \"master\" branch. To see and use the files of a\nparticular development branch locally, you need to know the branch name\nand then specifically check out that development branch.\n.. note::\nChecking out an active development branch by branch name gives you a\nsnapshot of that particular branch at the time you check it out.\nFurther development on top of the branch that occurs after check it\nout can occur.\n#. *Switch to the Poky Directory:* If you have a local poky Git\nrepository, switch to that directory. If you do not have the local\ncopy of poky, see the\n\":ref:`dev-manual/start:cloning the \\`\\`poky\\`\\` repository`\"\nsection.\n#. *Determine Existing Branch Names:*\n::\n$ git branch -a\n* master\nremotes/origin/1.1_M1\nremotes/origin/1.1_M2\nremotes/origin/1.1_M3\nremotes/origin/1.1_M4\nremotes/origin/1.2_M1\nremotes/origin/1.2_M2\nremotes/origin/1.2_M3\n. . .\nremotes/origin/thud\nremotes/origin/thud-next\nremotes/origin/warrior\nremotes/origin/warrior-next\nremotes/origin/zeus\nremotes/origin/zeus-next"}
{"text": "\n... and so on ...\n#. *Check out the Branch:* Check out the development branch in which you\nwant to work. For example, to access the files for the Yocto Project\n&DISTRO; Release (&DISTRO_NAME;), use the following command::\n$ git checkout -b &DISTRO_NAME_NO_CAP; origin/&DISTRO_NAME_NO_CAP;\nBranch &DISTRO_NAME_NO_CAP; set up to track remote branch &DISTRO_NAME_NO_CAP; from origin.\nSwitched to a new branch '&DISTRO_NAME_NO_CAP;'\nThe previous command checks out the \"&DISTRO_NAME_NO_CAP;\" development\nbranch and reports that the branch is tracking the upstream\n\"origin/&DISTRO_NAME_NO_CAP;\" branch.\nThe following command displays the branches that are now part of your\nlocal poky repository. The asterisk character indicates the branch\nthat is currently checked out for work::\n$ git branch\nmaster\n* &DISTRO_NAME_NO_CAP;\nChecking Out by Tag in Poky\n---------------------------\nSimilar to branches, the upstream repository uses tags to mark specific\ncommits associated with significant points in a development branch (i.e.\na release point or stage of a release). You might want to set up a local\nbranch based on one of those points in the repository. The process is\nsimilar to checking out by branch name except you use tag names.\n.. note::\nChecking out a branch based on a tag gives you a stable set of files\nnot affected by development on the branch above the tag.\n#. *Switch to the Poky Directory:* If you have a local poky Git\nrepository, switch to that directory. If you do not have the local\ncopy of poky, see the\n\":ref:`dev-manual/start:cloning the \\`\\`poky\\`\\` repository`\"\nsection.\n#. *Fetch the Tag Names:* To checkout the branch based on a tag name,\nyou need to fetch the upstream tags into your local repository::\n$ git fetch --tags\n$\n#. *List the Tag Names:* You can list the tag names now::\n$ git tag\n1.1_M1.final\n1.1_M1.rc1\n1.1_M1.rc2\n1.1_M2.final\n1.1_M2.rc1\n.\n.\n.\nyocto-2.5\nyocto-2.5.1\nyocto-2.5.2\nyocto-2.5.3\nyocto-2.6\nyocto-2.6.1\nyocto-2.6.2\nyocto-2.7\nyocto_1.5_M5.rc8\n#. *Check out the Branch:*\n::\n$ git checkout tags/yocto-&DISTRO; -b my_yocto_&DISTRO;\nSwitched to a new branch 'my_yocto_&DISTRO;'\n$ git branch\nmaster\n* my_yocto_&DISTRO;\nThe previous command creates and\nchecks out a local branch named \"my_yocto_&DISTRO;\", which is based on\nthe commit in the upstream poky repository that has the same tag. In\nthis example, the files you have available locally as a result of the\n``checkout`` command are a snapshot of the \"&DISTRO_NAME_NO_CAP;\"\ndevelopment branch at the point where Yocto Project &DISTRO; was\nreleased."}
{"text": "\n.. _device-manager:\nSelecting a Device Manager\n**************************\nThe Yocto Project provides multiple ways to manage the device manager\n(``/dev``):\n-  Persistent and Pre-Populated ``/dev``: For this case, the ``/dev``\ndirectory is persistent and the required device nodes are created\nduring the build.\n-  Use ``devtmpfs`` with a Device Manager: For this case, the ``/dev``\ndirectory is provided by the kernel as an in-memory file system and\nis automatically populated by the kernel at runtime. Additional\nconfiguration of device nodes is done in user space by a device\nmanager like ``udev`` or ``busybox-mdev``.\nUsing Persistent and Pre-Populated ``/dev``\n===========================================\nTo use the static method for device population, you need to set the\n:term:`USE_DEVFS` variable to \"0\"\nas follows::\nUSE_DEVFS = \"0\"\nThe content of the resulting ``/dev`` directory is defined in a Device\nTable file. The\n:term:`IMAGE_DEVICE_TABLES`\nvariable defines the Device Table to use and should be set in the\nmachine or distro configuration file. Alternatively, you can set this\nvariable in your ``local.conf`` configuration file.\nIf you do not define the :term:`IMAGE_DEVICE_TABLES` variable, the default\n``device_table-minimal.txt`` is used::\nIMAGE_DEVICE_TABLES = \"device_table-mymachine.txt\"\nThe population is handled by the ``makedevs`` utility during image\ncreation:\nUsing ``devtmpfs`` and a Device Manager\n=======================================\nTo use the dynamic method for device population, you need to use (or be\nsure to set) the :term:`USE_DEVFS`\nvariable to \"1\", which is the default::\nUSE_DEVFS = \"1\"\nWith this\nsetting, the resulting ``/dev`` directory is populated by the kernel\nusing ``devtmpfs``. Make sure the corresponding kernel configuration\nvariable ``CONFIG_DEVTMPFS`` is set when building you build a Linux\nkernel.\nAll devices created by ``devtmpfs`` will be owned by ``root`` and have\npermissions ``0600``.\nTo have more control over the device nodes, you can use a device manager\nlike ``udev`` or ``busybox-mdev``. You choose the device manager by\ndefining the ``VIRTUAL-RUNTIME_dev_manager`` variable in your machine or\ndistro configuration file. Alternatively, you can set this variable in\nyour ``local.conf`` configuration file::\nVIRTUAL-RUNTIME_dev_manager = \"udev\"\n# Some alternative values\n# VIRTUAL-RUNTIME_dev_manager = \"busybox-mdev\"\n# VIRTUAL-RUNTIME_dev_manager = \"systemd\""}
{"text": "\nDealing with Vulnerability Reports\n**********************************\nThe Yocto Project and OpenEmbedded are open-source, community-based projects\nused in numerous products. They assemble multiple other open-source projects,\nand need to handle security issues and practices both internal (in the code\nmaintained by both projects), and external (maintained by other projects and\norganizations).\nThis manual assembles security-related information concerning the whole\necosystem. It includes information on reporting a potential security issue,\nthe operation of the YP Security team and how to contribute in the\nrelated code. It is written to be useful for both security researchers and\nYP developers.\nHow to report a potential security vulnerability?\n=================================================\nIf you would like to report a public issue (for example, one with a released\nCVE number), please report it using the\n:yocto_bugs:`Security Bugzilla </enter_bug.cgi?product=Security>`.\nIf you are dealing with a not-yet-released issue, or an urgent one, please send\na message to security AT yoctoproject DOT org, including as many details as\npossible: the layer or software module affected, the recipe and its version,\nand any example code, if available. This mailing list is monitored by the\nYocto Project Security team.\nFor each layer, you might also look for specific instructions (if any) for\nreporting potential security issues in the specific ``SECURITY.md`` file at the\nroot of the repository. Instructions on how and where submit a patch are\nusually available in ``README.md``. If this is your first patch to the\nYocto Project/OpenEmbedded, you might want to have a look into the\nContributor's Manual section\n\":ref:`contributor-guide/submit-changes:preparing changes for submission`\".\nBranches maintained with security fixes\n---------------------------------------\nSee the\n:ref:`Release process <ref-manual/release-process:Stable Release Process>`\ndocumentation for details regarding the policies and maintenance of stable\nbranches.\nThe :yocto_wiki:`Releases page </Releases>` contains a list\nof all releases of the Yocto Project. Versions in gray are no longer actively\nmaintained with security patches, but well-tested patches may still be accepted\nfor them for significant issues.\nSecurity-related discussions at the Yocto Project\n-------------------------------------------------\nWe have set up two security-related mailing lists:\n-  Public List: yocto [dash] security [at] yoctoproject[dot] org\nThis is a public mailing list for anyone to subscribe to. This list is an\nopen list to discuss public security issues/patches and security-related\ninitiatives. For more information, including subscription information,\nplease see the  :yocto_lists:`yocto-security mailing list info page </g/yocto-security>`.\n- Private List: security [at] yoctoproject [dot] org\nThis is a private mailing list for reporting non-published potential\nvulnerabilities. The list is monitored by the Yocto Project Security team.\nWhat you should do if you find a security vulnerability\n-------------------------------------------------------\nIf you find a security flaw: a crash, an information leakage, or anything that\ncan have a security impact if exploited in any Open Source software built or\nused by the Yocto Project, please report this to the Yocto Project Security\nTeam. If you prefer to contact the upstream project directly, please send a\ncopy to the security team at the Yocto Project as well. If you believe this is\nhighly sensitive information, please report the vulnerability in a secure way,\ni.e. encrypt the email and send it to the private list. This ensures that\nthe exploit is not leaked and exploited before a response/fix has been generated.\nSecurity team\n=============\nThe Yocto Project/OpenEmbedded security team coordinates the work on security\nsubjects in the project. All general discussion takes place publicly. The\nSecurity Team only uses confidential communication tools to deal with private\nvulnerability reports before they are released.\nSecurity team appointment\n-------------------------\nThe Yocto Project Security Team consists of at least three members. When new\nmembers are needed, the Yocto Project Technical Steering Committee (YP TSC)\nasks for nominations by public channels including a nomination deadline.\nSelf-nominations are possible. When the limit time is\nreached, the YP TSC posts the list of candidates for the comments of project\nparticipants and developers. Comments may be sent publicly or privately to the\nYP and OE TSCs. The candidates are approved by both YP TSC and OpenEmbedded\nTechnical Steering Committee (OE TSC) and the final list of the team members\nis announced publicly. The aim is to have people representing technical\nleadership, security knowledge and infrastructure present with enough people\nto provide backup/coverage but keep the notification list small enough to\nminimize information risk and maintain trust.\nYP Security Team members may resign at any time.\nSecurity Team Operations\n------------------------\nThe work of the Security Team might require high confidentiality. Team members\nare individuals selected by merit and do not represent the companies they work\nfor. They do not share information about confidential issues outside of the team\nand do not hint about ongoing embargoes.\nTeam members can bring in domain experts as needed. Those people should be\nadded to individual issues only and adhere to the same standards as the YP\nSecurity Team.\nThe YP security team organizes its meetings and communication as needed.\nWhen the YP Security team receives a report about a potential security\nvulnerability, they quickly analyze and notify the reporter of the result.\nThey might also request more information.\nIf the issue is confirmed and affects the code maintained by the YP, they\nconfidentially notify maintainers of that code and work with them to prepare\na fix.\nIf the issue is confirmed and affects an upstream project, the YP security team\nnotifies the project. Usually, the upstream project analyzes the problem again."}
{"text": "\nIf they deem it a real security problem in their software, they develop and\nrelease a fix following their security policy. They may want to include the\noriginal reporter in the loop. There is also sometimes some coordination for\nhandling patches, backporting patches etc, or just understanding the problem\nor what caused it.\nWhen the fix is publicly available, the YP security team member or the\npackage maintainer sends patches against the YP code base, following usual\nprocedures, including public code review.\nWhat Yocto Security Team does when it receives a security vulnerability\n-----------------------------------------------------------------------\nThe YP Security Team team performs a quick analysis and would usually report\nthe flaw to the upstream project. Normally the upstream project analyzes the\nproblem. If they deem it a real security problem in their software, they\ndevelop and release a fix following their own security policy. They may want\nto include the original reporter in the loop. There is also sometimes some\ncoordination for handling patches, backporting patches etc, or just\nunderstanding the problem or what caused it.\nThe security policy of the upstream project might include a notification to\nLinux distributions or other important downstream projects in advance to\ndiscuss coordinated disclosure. These mailing lists are normally non-public.\nWhen the upstream project releases a version with the fix, they are responsible\nfor contacting `Mitre <https://www.cve.org/>`__ to get a CVE number assigned and\nthe CVE record published.\nIf an upstream project does not respond quickly\n-----------------------------------------------\nIf an upstream project does not fix the problem in a reasonable time,\nthe Yocto's Security Team will contact other interested parties (usually\nother distributions) in the community and together try to solve the\nvulnerability as quickly as possible.\nThe Yocto Project Security team adheres to the 90 days disclosure policy\nby default. An increase of the embargo time is possible when necessary.\nCurrent Security Team members\n-----------------------------\nFor secure communications, please send your messages encrypted using the GPG\nkeys. Remember, message headers are not encrypted so do not include sensitive\ninformation in the subject line.\n-  Ross Burton: <ross@burtonini.com> `Public key <https://keys.openpgp.org/search?q=ross%40burtonini.com>`__\n-  Michael Halstead: <mhalstead [at] linuxfoundation [dot] org>\n`Public key <https://pgp.mit.edu/pks/lookup?op=vindex&search=0x3373170601861969>`__\nor `Public key <https://keyserver.ubuntu.com/pks/lookup?op=get&search=0xd1f2407285e571ed12a407a73373170601861969>`__\n-  Richard Purdie: <richard.purdie@linuxfoundation.org> `Public key <https://keys.openpgp.org/search?q=richard.purdie%40linuxfoundation.org>`__\n-  Marta Rybczynska: <marta DOT rybczynska [at] syslinbit [dot] com> `Public key <https://keys.openpgp.org/search?q=marta.rybczynska@syslinbit.com>`__\n-  Steve Sakoman: <steve [at] sakoman [dot] com> `Public key <https://keys.openpgp.org/search?q=steve%40sakoman.com>`__"}
{"text": "\nOptionally Using an External Toolchain\n**************************************\nYou might want to use an external toolchain as part of your development.\nIf this is the case, the fundamental steps you need to accomplish are as\nfollows:\n-  Understand where the installed toolchain resides. For cases where you\nneed to build the external toolchain, you would need to take separate\nsteps to build and install the toolchain.\n-  Make sure you add the layer that contains the toolchain to your\n``bblayers.conf`` file through the\n:term:`BBLAYERS` variable.\n-  Set the :term:`EXTERNAL_TOOLCHAIN` variable in your ``local.conf`` file\nto the location in which you installed the toolchain.\nThe toolchain configuration is very flexible and customizable. It\nis primarily controlled with the :term:`TCMODE` variable. This variable\ncontrols which ``tcmode-*.inc`` file to include from the\n``meta/conf/distro/include`` directory within the :term:`Source Directory`.\nThe default value of :term:`TCMODE` is \"default\", which tells the\nOpenEmbedded build system to use its internally built toolchain (i.e.\n``tcmode-default.inc``). However, other patterns are accepted. In\nparticular, \"external-\\*\" refers to external toolchains. One example is\nthe Mentor Graphics Sourcery G++ Toolchain. Support for this toolchain resides\nin the separate ``meta-sourcery`` layer at\nhttps://github.com/MentorEmbedded/meta-sourcery/.\nSee its ``README`` file for details about how to use this layer.\nAnother example of external toolchain layer is\n:yocto_git:`meta-arm-toolchain </meta-arm/tree/meta-arm-toolchain/>`\nsupporting GNU toolchains released by ARM.\nYou can find further information by reading about the :term:`TCMODE` variable\nin the Yocto Project Reference Manual's variable glossary."}
